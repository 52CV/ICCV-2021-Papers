# ICCV2021æœ€æ–°ä¿¡æ¯åŠå·²æ¥æ”¶è®ºæ–‡/ä»£ç (æŒç»­æ›´æ–°)

<div align="center">
  <img src="image/ICCV2021.png"/>
</div>

å®˜ç½‘é“¾æ¥ï¼šhttp://iccv2021.thecvf.com/home<br>
å¼€ä¼šæ—¶é—´ï¼š2021å¹´10æœˆ11æ—¥è‡³17æ—¥<br>

# :exclamation::exclamation::exclamation::star2::star2::star2:ğŸ“—ğŸ“—ğŸ“—ICCV 2021æ”¶å½•è®ºæ–‡å·²å…¨éƒ¨å…¬å¸ƒï¼Œä¸‹è½½å¯åœ¨ã€æˆ‘çˆ±è®¡ç®—æœºè§†è§‰ã€‘åå°å›å¤â€œpaperâ€ï¼Œå³å¯æ”¶åˆ°ã€‚å…±è®¡ 1612 ç¯‡ã€‚

# :exclamation::exclamation::exclamation::star2::star2::star2:å…¨éƒ¨è®ºæ–‡å·²ç²—ç•¥åˆ†ç±»å®Œæ¯•ï¼Œè¯·æŸ¥é˜…

### :exclamation::exclamation::exclamation::star2::star2::star2:ICCV 2021æ”¶å½•è®ºæ–‡ï¼Œ10 æœˆ 27 æ—¥æ•´ç†å¦‚ä¸‹ï¼š
* åŠ¨ä½œå®šä½
  * [D2-Net: Weakly-Supervised Action Localization via Discriminative Embeddings and Denoised Activations](https://openaccess.thecvf.com/content/ICCV2021/papers/Narayan_D2-Net_Weakly-Supervised_Action_Localization_via_Discriminative_Embeddings_and_Denoised_Activations_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/naraysa/D2-Net)
* å‰ªæ
  * [Auto Graph Encoder-Decoder for Neural Network Pruning](https://openaccess.thecvf.com/content/ICCV2021/papers/Yu_Auto_Graph_Encoder-Decoder_for_Neural_Network_Pruning_ICCV_2021_paper.pdf)
* è¡¨é¢é‡å»º
  * [Adaptive Surface Reconstruction With Multiscale Convolutional Kernels](https://openaccess.thecvf.com/content/ICCV2021/papers/Ummenhofer_Adaptive_Surface_Reconstruction_With_Multiscale_Convolutional_Kernels_ICCV_2021_paper.pdf)
* åˆ†ç±»
  * [Mixture-Based Feature Space Learning for Few-Shot Image Classification](https://openaccess.thecvf.com/content/ICCV2021/papers/Afrasiyabi_Mixture-Based_Feature_Space_Learning_for_Few-Shot_Image_Classification_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/ArmanAfrasiyabi/MixtFSL-fs):house:[project](https://lvsn.github.io/MixtFSL/):tv:[video](https://www.youtube.com/embed/DxMCHCQKDCQ)
* Table Structure Recognition(è¡¨æ ¼ç»“æ„è¯†åˆ«)
  * [TGRNet: A Table Graph Reconstruction Network for Table Structure Recognition](https://arxiv.org/abs/2106.10598)<br>:star:[code](https://github.com/xuewenyuan/TGRNet)
* å›¾åƒä¿®å¤
  * [Learning a Sketch Tensor Space for Image Inpainting of Man-made Scenes](https://arxiv.org/abs/2103.15087)<br>:star:[code](https://github.com/ewrfcas/MST_inpainting):house:[project](https://ewrfcas.github.io/MST_inpainting/)
* åˆ†å‰²
  * [Specialize and Fuse: Pyramidal Output Representation for Semantic Segmentation](https://arxiv.org/abs/2108.01866)
  * [Unsupervised Semantic Segmentation by Contrasting Object Mask Proposals](https://arxiv.org/abs/2102.06191)<br>:star:[code](https://github.com/wvangansbeke/Unsupervised-Semantic-Segmentation)
  * [Graph-BAS3Net: Boundary-Aware Semi-Supervised Segmentation Network With Bilateral Graph Convolution](https://openaccess.thecvf.com/content/ICCV2021/papers/Huang_Graph-BAS3Net_Boundary-Aware_Semi-Supervised_Segmentation_Network_With_Bilateral_Graph_Convolution_ICCV_2021_paper.pdf)åŒ»å­¦å›¾åƒåˆ†å‰²
  * [Scribble-Supervised Semantic Segmentation Inference](https://openaccess.thecvf.com/content/ICCV2021/papers/Xu_Scribble-Supervised_Semantic_Segmentation_Inference_ICCV_2021_paper.pdf)
* Transformer
  * [Scalable Vision Transformers With Hierarchical Pooling](https://arxiv.org/abs/2103.10619)<br>:star:[code](https://github.com/MonashAI/HVT)
  * [Visual Transformers: Where Do Transformers Really Belong in Vision Models?](https://openaccess.thecvf.com/content/ICCV2021/papers/Wu_Visual_Transformers_Where_Do_Transformers_Really_Belong_in_Vision_Models_ICCV_2021_paper.pdf)
* è§†è§‰å¯¼èˆª
  * [Visual Graph Memory With Unsupervised Representation for Visual Navigation](https://openaccess.thecvf.com/content/ICCV2021/papers/Kwon_Visual_Graph_Memory_With_Unsupervised_Representation_for_Visual_Navigation_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/rllab-snu/Visual-Graph-Memory):house:[project](https://rllab-snu.github.io/projects/vgm/doc.html):tv:[video](https://www.youtube.com/watch?v=Uksb_kR80Hk)
* è‡ªåŠ¨é©¾é©¶
  * [MGNet: Monocular Geometric Scene Understanding for Autonomous Driving](https://openaccess.thecvf.com/content/ICCV2021/papers/Schon_MGNet_Monocular_Geometric_Scene_Understanding_for_Autonomous_Driving_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/markusschoen/MGNet):tv:[video](https://www.youtube.com/watch?v=GXdQNtVQYmY)
* GAN
  * [Multi-Class Multi-Instance Count Conditioned Adversarial Image Generation](https://arxiv.org/abs/2103.16795)<br>:star:[code](https://github.com/boschresearch/MCCGAN)
  * [Generative Adversarial Registration for Improved Conditional Deformable Templates](https://arxiv.org/abs/2105.04349)<br>:star:[code](https://github.com/neel-dey/Atlas-GAN)
  * [F-Drop&Match: GANs with a Dead Zone in the High-Frequency Domain](https://openaccess.thecvf.com/content/ICCV2021/papers/Yamaguchi_F-DropMatch_GANs_With_a_Dead_Zone_in_the_High-Frequency_Domain_ICCV_2021_paper.pdf)
* å»é˜´å½±
  * [DC-ShadowNet: Single-Image Hard and Soft Shadow Removal Using Unsupervised Domain-Classifier Guided Network](https://openaccess.thecvf.com/content/ICCV2021/papers/Jin_DC-ShadowNet_Single-Image_Hard_and_Soft_Shadow_Removal_Using_Unsupervised_Domain-Classifier_ICCV_2021_paper.pdf)
* æ¸²æŸ“
  * [EgoRenderer: Rendering Human Avatars from Egocentric Camera Images](https://openaccess.thecvf.com/content/ICCV2021/papers/Hu_EgoRenderer_Rendering_Human_Avatars_From_Egocentric_Camera_Images_ICCV_2021_paper.pdf)
* VQA
  * [Auto-Parsing Network for Image Captioning and Visual Question Answering](https://arxiv.org/abs/2108.10568)
* ä¸‰ç»´é‡å»º
  * [CryoDRGN2: Ab initio neural reconstruction of 3D protein structures from real cryo-EM images](https://openaccess.thecvf.com/content/ICCV2021/papers/Zhong_CryoDRGN2_Ab_Initio_Neural_Reconstruction_of_3D_Protein_Structures_From_ICCV_2021_paper.pdf)
* VL
  * [AESOP: Abstract Encoding of Stories, Objects, and Pictures](https://openaccess.thecvf.com/content/ICCV2021/papers/Ravi_AESOP_Abstract_Encoding_of_Stories_Objects_and_Pictures_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/Hareesh-Ravi/AESOP):tv:[video](https://www.youtube.com/watch?v=ygGzY1DSSMk)
  * [Adaptive Hierarchical Graph Reasoning With Semantic Coherence for Video-and-Language Inference](https://arxiv.org/abs/2107.12270)
* èšç±»
  * [Graph Contrastive Clustering](https://arxiv.org/abs/2104.01429)<br>:star:[code](https://github.com/mynameischaos/GCC)
* CAM
  * [LFI-CAM: Learning Feature Importance for Better Visual Explanation](https://openaccess.thecvf.com/content/ICCV2021/papers/Lee_LFI-CAM_Learning_Feature_Importance_for_Better_Visual_Explanation_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/TrustworthyAI-kr/LFI-CAM)
* visual grounding
  * [InstanceRefer: Cooperative Holistic Understanding for Visual Grounding on Point Clouds through Instance Multi-level Contextual Referring](https://arxiv.org/abs/2103.01128)<br>:star:[code](https://github.com/CurryYuan/InstanceRefer)
* å§¿æ€
  * [Encoder-Decoder With Multi-Level Attention for 3D Human Shape and Pose Estimation](https://arxiv.org/abs/2109.02303)<br>:star:[code](https://github.com/ziniuwan/maed)
  * [Motion Adaptive Pose Estimation from Compressed Videos](https://openaccess.thecvf.com/content/ICCV2021/papers/Fan_Motion_Adaptive_Pose_Estimation_From_Compressed_Videos_ICCV_2021_paper.pdf)
* ç›®æ ‡æ£€æµ‹
  * [iNAS: Integral NAS for Device-Aware Salient Object Detection](https://openaccess.thecvf.com/content/ICCV2021/papers/Gu_iNAS_Integral_NAS_for_Device-Aware_Salient_Object_Detection_ICCV_2021_paper.pdf)<br>:house:[project](https://mmcheng.net/inas/)
  * [Dual Bipartite Graph Learning: A General Approach for Domain Adaptive Object Detection](https://openaccess.thecvf.com/content/ICCV2021/papers/Chen_Dual_Bipartite_Graph_Learning_A_General_Approach_for_Domain_Adaptive_ICCV_2021_paper.pdf)
  * [WB-DETR: Transformer-Based Detector without Backbone](https://openaccess.thecvf.com/content/ICCV2021/papers/Liu_WB-DETR_Transformer-Based_Detector_Without_Backbone_ICCV_2021_paper.pdf)
  * [Geometry-Based Distance Decomposition for Monocular 3D Object Detection](https://openaccess.thecvf.com/content/ICCV2021/papers/Shi_Geometry-Based_Distance_Decomposition_for_Monocular_3D_Object_Detection_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/Rock-100/MonoDet)
* åŸŸé€‚åº”
  * [Geometry-Aware Self-Training for Unsupervised Domain Adaptation on Object Point Clouds](https://openaccess.thecvf.com/content/ICCV2021/papers/Zou_Geometry-Aware_Self-Training_for_Unsupervised_Domain_Adaptation_on_Object_Point_Clouds_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/zou-longkun/GAST)
* çŸ¥è¯†è’¸é¦
  * [Exploring Inter-Channel Correlation for Diversity-Preserved Knowledge Distillation](https://openaccess.thecvf.com/content/ICCV2021/papers/Liu_Exploring_Inter-Channel_Correlation_for_Diversity-Preserved_Knowledge_Distillation_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/ADLab-AutoDrive/ICKD)
* åŠ¨ä½œè¯†åˆ«
  * [Class-Incremental Learning for Action Recognition in Videos](https://openaccess.thecvf.com/content/ICCV2021/papers/Park_Class-Incremental_Learning_for_Action_Recognition_in_Videos_ICCV_2021_paper.pdf)
* è§†é¢‘è¡¨ç¤ºå­¦ä¹ 
  * [Space-Time Crop & Attend: Improving Cross-Modal Video Representation Learning](https://arxiv.org/abs/2103.10211)<br>:star:[code](https://github.com/facebookresearch/GDT)
* å¯¹æŠ—æ”»å‡»
  * [RDA: Robust Domain Adaptation via Fourier Adversarial Attacking](https://arxiv.org/abs/2106.02874)
* è§†å›¾åˆæˆ
  * [Worldsheet: Wrapping the World in a 3D Sheet for View Synthesis from a Single Image](https://arxiv.org/abs/2012.09854)<br>:open_mouth:oral:star:[code](https://github.com/facebookresearch/worldsheet):house:[project](https://worldsheet.github.io/):tv:[video](https://youtu.be/j5aT3zRxFlk)
* è¿åŠ¨å»æ¨¡ç³Š
  * [Perceptual Variousness Motion Deblurring With Light Global Context Refinement](https://openaccess.thecvf.com/content/ICCV2021/papers/Li_Perceptual_Variousness_Motion_Deblurring_With_Light_Global_Context_Refinement_ICCV_2021_paper.pdf)
* å›¾åƒå­—å¹•
  * [In Defense of Scene Graphs for Image Captioning](https://arxiv.org/abs/2102.04990)<br>:star:[code](https://github.com/Kien085/SG2Caps)
* è½¦é€Ÿä¼°è®¡
  * [Robust Automatic Monocular Vehicle Speed Estimation for Traffic Surveillance](https://openaccess.thecvf.com/content/ICCV2021/papers/Revaud_Robust_Automatic_Monocular_Vehicle_Speed_Estimation_for_Traffic_Surveillance_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/naver/cctv)
* äººè„¸å›¾åƒæ£€ç´¢
  * [Face Image Retrieval with Attribute Manipulation](https://openaccess.thecvf.com/content/ICCV2021/papers/Zaeemzadeh_Face_Image_Retrieval_With_Attribute_Manipulation_ICCV_2021_paper.pdf)
* è„‘è‚¿ç˜¤åˆ†å‰²
  * [RFNet: Region-Aware Fusion Network for Incomplete Multi-Modal Brain Tumor Segmentation](https://openaccess.thecvf.com/content/ICCV2021/papers/Ding_RFNet_Region-Aware_Fusion_Network_for_Incomplete_Multi-Modal_Brain_Tumor_Segmentation_ICCV_2021_paper.pdf)
* å¯¹æ¯”å­¦ä¹ 
  * [Weakly Supervised Contrastive Learning](https://arxiv.org/abs/2110.04770)
* è¿åŠ¨åˆ†å‰²
  * [SLIM: Self-Supervised LiDAR Scene Flow and Motion Segmentation](https://openaccess.thecvf.com/content/ICCV2021/papers/Baur_SLIM_Self-Supervised_LiDAR_Scene_Flow_and_Motion_Segmentation_ICCV_2021_paper.pdf)
* è½¨è¿¹é¢„æµ‹
  * [Likelihood-Based Diverse Sampling for Trajectory Forecasting](https://arxiv.org/abs/2011.15084)<br>:star:[code](https://github.com/JasonMa2016/LDS)
* åŸŸæ³›åŒ–
  * [A Simple Feature Augmentation for Domain Generalization](https://openaccess.thecvf.com/content/ICCV2021/papers/Li_A_Simple_Feature_Augmentation_for_Domain_Generalization_ICCV_2021_paper.pdf)
* ç›®æ ‡å®šä½
  * [TS-CAM: Token Semantic Coupled Attention Map for Weakly Supervised Object Localization](https://openaccess.thecvf.com/content/ICCV2021/papers/Gao_TS-CAM_Token_Semantic_Coupled_Attention_Map_for_Weakly_Supervised_Object_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/vasgaowei/TS-CAM)
* è¿åŠ¨åˆæˆ
  * [Synthesis of Compositional Animations from Textual Descriptions](https://arxiv.org/abs/2103.14675)<br>:star:[code](https://github.com/anindita127/Complextext2animation)
* è§†è§‰å¯¹è¯
  * [Unified Questioner Transformer for Descriptive Question Generation in Goal-Oriented Visual Dialogue](https://arxiv.org/abs/2106.15550)

* å…¶å®ƒ
  * [Localized Simple Multiple Kernel K-means](https://openaccess.thecvf.com/content/ICCV2021/papers/Liu_Localized_Simple_Multiple_Kernel_K-Means_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/xinwangliu/LocalizedSMKKM)
  * [SmartShadow: Artistic Shadow Drawing Tool for Line Drawings](https://openaccess.thecvf.com/content/ICCV2021/papers/Zhang_SmartShadow_Artistic_Shadow_Drawing_Tool_for_Line_Drawings_ICCV_2021_paper.pdf)
  * [PT-CapsNet: A Novel Prediction-Tuning Capsule Network Suitable for Deeper Architectures](https://openaccess.thecvf.com/content/ICCV2021/papers/Pan_PT-CapsNet_A_Novel_Prediction-Tuning_Capsule_Network_Suitable_for_Deeper_Architectures_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/Christinepan881/PT-CapsNet)
  * [Generalized Shuffled Linear Regression](https://openaccess.thecvf.com/content/ICCV2021/papers/Li_Generalized_Shuffled_Linear_Regression_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/SILI1994/Generalized-Shuffled-Linear-Regression)
  * [The Animation Transformer: Visual Correspondence via Segment Matching](https://openaccess.thecvf.com/content/ICCV2021/papers/Casey_The_Animation_Transformer_Visual_Correspondence_via_Segment_Matching_ICCV_2021_paper.pdf)
  * [Weak Adaptation Learning: Addressing Cross-Domain Data Insufficiency With Weak Annotator](https://openaccess.thecvf.com/content/ICCV2021/papers/Xu_Weak_Adaptation_Learning_Addressing_Cross-Domain_Data_Insufficiency_With_Weak_Annotator_ICCV_2021_paper.pdf)
  * [Building-GAN: Graph-Conditioned Architectural Volumetric Design Generation](https://openaccess.thecvf.com/content/ICCV2021/papers/Chang_Building-GAN_Graph-Conditioned_Architectural_Volumetric_Design_Generation_ICCV_2021_paper.pdf)
  * [Procrustean Training for Imbalanced Deep Learning](https://arxiv.org/abs/2104.01769)

ã€2ã€‘
* ç‚¹äº‘
  * [Progressive Seed Generation Auto-Encoder for Unsupervised Point Cloud Learning](https://openaccess.thecvf.com/content/ICCV2021/papers/Yang_Progressive_Seed_Generation_Auto-Encoder_for_Unsupervised_Point_Cloud_Learning_ICCV_2021_paper.pdf)
* image hiding(å›¾åƒéšè—)
  * [HiNet: Deep Image Hiding by Invertible Network](https://openaccess.thecvf.com/content/ICCV2021/papers/Jing_HiNet_Deep_Image_Hiding_by_Invertible_Network_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/TomTomTommi/HiNet)
* Transformer
  * [Anticipative Video Transformer](https://arxiv.org/abs/2106.02036)<br>:star:[code](https://github.com/facebookresearch/AVT):house:[project](https://facebookresearch.github.io/AVT/)
* å›¾åƒç¿»è¯‘
  * [Semantically Robust Unpaired Image Translation for Data with Unmatched Semantics Statistics](https://arxiv.org/abs/2012.04932)
* èšç±»
  * [End-to-End Robust Joint Unsupervised Image Alignment and Clustering](https://openaccess.thecvf.com/content/ICCV2021/papers/Zeng_End-to-End_Robust_Joint_Unsupervised_Image_Alignment_and_Clustering_ICCV_2021_paper.pdf)
* è§†é¢‘è¯†åˆ«
  * [Multi-Modal Multi-Action Video Recognition](https://openaccess.thecvf.com/content/ICCV2021/papers/Shi_Multi-Modal_Multi-Action_Video_Recognition_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/zhenglab/multi-action-video)
* ç›¸æœºæ ¡å‡†
  * [BabelCalib: A Universal Approach to Calibrating Central Cameras](https://arxiv.org/abs/2109.09704)<br>:star:[code](https://github.com/ylochman/babelcalib)
* å»å™ª
  * [Cross-Patch Graph Convolutional Network for Image Denoising](https://openaccess.thecvf.com/content/ICCV2021/papers/Li_Cross-Patch_Graph_Convolutional_Network_for_Image_Denoising_ICCV_2021_paper.pdf)
* è¡¨å¾å­¦ä¹ 
  * [Curious Representation Learning for Embodied Intelligence](https://arxiv.org/abs/2105.01060)<br>:star:[code](https://github.com/yilundu/crl):house:[project](https://yilundu.github.io/crl/)
* è¡Œäººæ£€æµ‹
  * [Body-Face Joint Detection via Embedding and Head Hook](https://openaccess.thecvf.com/content/ICCV2021/papers/Wan_Body-Face_Joint_Detection_via_Embedding_and_Head_Hook_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/AibeeDetect/BFJDet)
* åˆ†å‰²
  * [Semi-Supervised Semantic Segmentation With Pixel-Level Contrastive Learning From a Class-Wise Memory Bank](https://arxiv.org/abs/2104.13415)<br>:star:[code](https://github.com/Shathe/SemiSeg-Contrastive)
* è½¦é“çº¿æ£€æµ‹
  * [Active Learning for Lane Detection: A Knowledge Distillation Approach](https://openaccess.thecvf.com/content/ICCV2021/papers/Peng_Active_Learning_for_Lane_Detection_A_Knowledge_Distillation_Approach_ICCV_2021_paper.pdf)
* ä¸‰ç»´è§†è§‰
  * [Interpolation-Aware Padding for 3D Sparse Convolutional Neural Networks](https://arxiv.org/abs/2108.06925)
* Visual Grounding
  * [TransVG: End-to-End Visual Grounding With Transformers](https://arxiv.org/abs/2104.08541)<br>:star:[code](https://github.com/djiajunustc/TransVG)
* é‡åŒ–
  * [Once Quantization-Aware Training: High Performance Extremely Low-bit Architecture Search](https://arxiv.org/abs/2010.04354)<br>:star:[code](https://github.com/LaVieEnRoseSMZ/OQA)
* å…¶å®ƒ
  * [Rotation Averaging in a Split Second: A Primal-Dual Method and a Closed-Form for Cycle Graphs](https://arxiv.org/abs/2109.08046)<br>:star:[code](https://github.com/gabmoreira/maks)
  * [Effectively Leveraging Attributes for Visual Similarity](https://arxiv.org/abs/2105.01695)<br>:star:[code](https://github.com/samarth4149/PAN)

# ç›®å½•

|:dog:|:mouse:|:hamster:|:tiger:|
|------|------|------|------|
|[65.Optical Flow Estimation(å…‰æµä¼°è®¡)](#65)|
|[61.Metric Learning(å…ƒå­¦ä¹ )](#61)|[62.Open-Set Recognition(å¼€æ”¾é›†è¯†åˆ«)](#62)|[63.Data Augmentation(æ•°æ®å¢å¼º)](#63)|[64.Anomaly Detection(å¼‚å¸¸æ£€æµ‹)](#64)|
|[57.Image Matching(å›¾åƒåŒ¹é…)](#57)|[58.Computational Photography(å…‰å­¦ã€å‡ ä½•ã€å…‰åœºæˆåƒã€è®¡ç®—æ‘„å½±)](#58)|[59.Graph Neural Networks(å›¾ç¥ç»ç½‘ç»œ)](#59)|[60.Federated Learning(è”åˆå­¦ä¹ )](#60)
|[53.Vision Localization(è§†è§‰å®šä½)](#53)|[54.Sketch recognition(è‰å›¾)](#54)|[55.Activity Recognition(æ´»åŠ¨è¯†åˆ«)](#55)|[56.Dataset(æ•°æ®é›†)](#56)|
|[49.Human-Object Interaction(äººç‰©äº¤äº’)](#49)|[50.Continual Learning(æŒç»­å­¦ä¹ )](#50)|[51.View Synthesis(è§†å›¾åˆæˆ)](#51)|[52.Vision-and-Language(è§†è§‰è¯­è¨€)](#52)|
|[45.Image Caption(å›¾åƒå­—å¹•)](#45)|[46.Defect Detection(ç¼ºé™·æ£€æµ‹)](#46)|[47.NAS](#47)|[48.6DoF](#48)|
|[41.Out-of-Distribution Detection(OOD)](#41)|[42.Visual Representations Learning(è§†è§‰è¡¨å¾å­¦ä¹ )](#42)|[43.Dense Prediction(å¯†é›†é¢„æµ‹)](#43)|[44.Human motion prediction(äººä½“è¿åŠ¨é¢„æµ‹)](#44)|
|[37.Multitask Learning(å¤šä»»åŠ¡å­¦ä¹ )](#37)|[38.Weakly/Semi-Supervised/Self-supervised/Unsupervised Learning(è‡ª/åŠ/å¼±ç›‘ç£å­¦ä¹ )](#38)|[39.Incremental Learning(å¢é‡å­¦ä¹ )](#39)|[40.Metric Learning(åº¦é‡å­¦ä¹ )](#40)|
|[33.Remote Sensing Images(é¥æ„Ÿå½±åƒ)](#33)|[34.Image Super-Resolution(å›¾åƒè¶…åˆ†è¾¨ç‡)](#34)|[35.Quantization/Pruning/Knowledge Distillation/Model Compression(é‡åŒ–ã€å‰ªæã€è’¸é¦ã€æ¨¡å‹å‹ç¼©/æ‰©å±•ä¸ä¼˜åŒ–)](#35)|[36.SLAM/AR/VR/æœºå™¨äºº](#36)|
|[29.Image Retrieval(å›¾åƒæ£€ç´¢)](#29)|[30.Image Generation/synthesis(å›¾åƒç”Ÿæˆ/åˆæˆ)](#30)|[31.Style Transfer(é£æ ¼è¿ç§»)](#31)|[32.è¯­éŸ³](#32)|
|[25.Medical Image(åŒ»å­¦å½±åƒ)](#25)|[26.Image Processing(å›¾åƒå¤„ç†)](#26)|[27.Multi-label image recognition(å¤šæ ‡ç­¾å›¾åƒè¯†åˆ«)](#27)|[28.Contrastive Learning(å¯¹æ¯”å­¦ä¹ )](#28)]
|[21.Active Learning(ä¸»åŠ¨å­¦ä¹ )](#21)|[22.GAN](#22)|[23.Gaze Estimation(è§†çº¿ä¼°è®¡)](#23)|[24.Face(äººè„¸)](#24)|
|[17.3D(ä¸‰ç»´è§†è§‰)](#17)|[18.Transformers](#18)|[19.Self-Driving Vehicles(è‡ªåŠ¨é©¾é©¶)](#19)|[20.Adversarial Learning(å¯¹æŠ—å­¦ä¹ )](#20)|
|[13.Image Segmentation(å›¾åƒåˆ†å‰²)](#13)|[14.Object Detection(ç›®æ ‡æ£€æµ‹)](#13)|[15.Object Tracking(ç›®æ ‡è·Ÿè¸ª)](#15)|[16.Re-Identification(é‡è¯†åˆ«)](#16)|
|[9.Video](#9)|[10.OCR](10)|[11.Visual Question Answering(è§†è§‰é—®ç­”)](#11)|[12.Image/Fine-Grained Classification(å›¾åƒ/ç»†ç²’åº¦åˆ†ç±»)](#12)|
|[5.Few-Shot/Zero-Shot Learning;Domain Generalization/Adaptation(å°/é›¶æ ·æœ¬å­¦ä¹ ;åŸŸé€‚åº”/æ³›åŒ–)](#5)|[6.Point Cloud(ç‚¹äº‘)](#6)|[7.Scene Graph Generation(åœºæ™¯å›¾ç”Ÿæˆ)](#7)|[8.Human Pose Estimation(äººä½“å§¿æ€ä¼°è®¡)](#8)|
|[1.Other(å…¶å®ƒ)](#1)|[2.Sign Language(æ‰‹è¯­è¯†åˆ«)](#2)|[3.Image Clustering(å›¾åƒèšç±»)](#3)|[4.Neural rendering(ç¥ç»æ¸²æŸ“)](#4)|

<a name="65"/>

## 65.Optical Flow Estimation(å…‰æµä¼°è®¡)
* [Separable Flow: Learning Motion Cost Volumes for Optical Flow Estimation](https://openaccess.thecvf.com/content/ICCV2021/papers/Zhang_Separable_Flow_Learning_Motion_Cost_Volumes_for_Optical_Flow_Estimation_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/feihuzhang/SeparableFlow)
* [High-Resolution Optical Flow from 1D Attention and Correlation](https://arxiv.org/abs/2104.13918)<br>:open_mouth:oral:star:[code](https://github.com/haofeixu/flow1d)
* [GyroFlow: Gyroscope-Guided Unsupervised Optical Flow Learning](https://arxiv.org/abs/2103.13725)<br>:star:[code](https://github.com/megvii-research/GyroFlow)
* [Sensor-Guided Optical Flow](https://arxiv.org/abs/2109.15321)<br>:star:[code](https://github.com/mattpoggi/sensor-guided-flow)

<a name="64"/>

## 64.Anomaly Detection(å¼‚å¸¸æ£€æµ‹)
* è¡¨é¢å¼‚å¸¸æ£€æµ‹
  * [DRÃ†M â€“ A discriminatively trained reconstru](https://openaccess.thecvf.com/content/ICCV2021/papers/Zavrtanik_DRAEM_-_A_Discriminatively_Trained_Reconstruction_Embedding_for_Surface_Anomaly_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/VitjanZ/DRAEM)
* å¼‚å¸¸æ£€æµ‹
  * [Weakly Supervised Temporal Anomaly Segmentation with Dynamic Time Warping](https://arxiv.org/abs/2108.06816)
  * [Learning Unsupervised Metaformer for Anomaly Detection]](https://openaccess.thecvf.com/content/ICCV2021/papers/Wu_Learning_Unsupervised_Metaformer_for_Anomaly_Detection_ICCV_2021_paper.pdf)<br>è§£å†³å›¾åƒå¼‚å¸¸çš„åˆ†ç±»æˆ–å®šä½

<a name="63"/>

## 63.Data Augmentation(æ•°æ®å¢å¼º)
* [DivAug: Plug-In Automated Data Augmentation With Explicit Diversity Maximization](https://arxiv.org/abs/2103.14545)<br>:star:[code](https://github.com/warai-0toko/DivAug)
* [TrivialAugment: Tuning-Free Yet State-of-the-Art Data Augmentation](https://openaccess.thecvf.com/content/ICCV2021/papers/Gu_Removing_the_Bias_of_Integral_Pose_Regression_ICCV_2021_paper.pdf)<br>:open_mouth:oral:star:[code](https://github.com/automl/trivialaugment)
* [Semantic Aware Data Augmentation for Cell Nuclei Microscopical Images With Artificial Neural Networks](https://openaccess.thecvf.com/content/ICCV2021/papers/Naghizadeh_Semantic_Aware_Data_Augmentation_for_Cell_Nuclei_Microscopical_Images_With_ICCV_2021_paper.pdf)
* [A Simple Baseline for Semi-Supervised Semantic Segmentation With Strong Data Augmentation](https://arxiv.org/abs/2104.07256)

<a name="62"/>

## 62.Open-Set Recognition(å¼€æ”¾é›†è¯†åˆ«)
* [OpenGAN: Open-Set Recognition via Open Data Generation](https://arxiv.org/abs/2104.02939)<br>:trophy:Best Paper Honorable Mention
* [Conditional Variational Capsule Network for Open Set Recognition](https://arxiv.org/abs/2104.09159)<br>:star:[code](https://github.com/guglielmocamporese/cvaecaposr)

<a name="61"/>

## 61.Metric Learning(å…ƒå­¦ä¹ )
* [Do Different Deep Metric Learning Losses Lead to Similar Learned Features?](https://openaccess.thecvf.com/content/ICCV2021/papers/Kobs_Do_Different_Deep_Metric_Learning_Losses_Lead_to_Similar_Learned_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/konstantinkobs/DML-analysis)
* [Learning With Memory-Based Virtual Classes for Deep Metric Learning](https://arxiv.org/abs/2103.16940)<br>:star:[code](https://github.com/navervision/MemVir)

<a name="60"/>

## 60.Federated Learning(è”åˆå­¦ä¹ )
* [Federated Learning for Non-IID Data via Unified Feature Learning and Optimization Objective Alignment](https://openaccess.thecvf.com/content/ICCV2021/papers/Zhang_Federated_Learning_for_Non-IID_Data_via_Unified_Feature_Learning_and_ICCV_2021_paper.pdf)
* [Ensemble Attention Distillation for Privacy-Preserving Federated Learning](https://openaccess.thecvf.com/content/ICCV2021/papers/Gong_Ensemble_Attention_Distillation_for_Privacy-Preserving_Federated_Learning_ICCV_2021_paper.pdf)

<a name="59"/>

## 59.Graph Neural Networks(å›¾ç¥ç»ç½‘ç»œ)
* [Meta-Aggregator: Learning to Aggregate for 1-bit Graph Neural Networks](https://arxiv.org/abs/2109.12872) 
* [PoGO-Net: Pose Graph Optimization With Graph Neural Networks](https://openaccess.thecvf.com/content/ICCV2021/papers/Li_PoGO-Net_Pose_Graph_Optimization_With_Graph_Neural_Networks_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/xxylii/PoGO-Net)
* [Dynamic Dual Gating Neural Networks](https://openaccess.thecvf.com/content/ICCV2021/papers/Li_Dynamic_Dual_Gating_Neural_Networks_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/lfr-0531/DGNet)

<a name="58"/>

## 58.Computational Photography(å…‰å­¦ã€å‡ ä½•ã€å…‰åœºæˆåƒã€è®¡ç®—æ‘„å½±)
* [An Asynchronous Kalman Filter for Hybrid Event Cameras](https://arxiv.org/abs/2012.05590)<br>:star:[code](https://github.com/ziweiWWANG/AKF)
* [4D Cloud Scattering Tomography](https://openaccess.thecvf.com/content/ICCV2021/papers/Ronen_4D_Cloud_Scattering_Tomography_ICCV_2021_paper.pdf)
* Snapshot compressive imaging(å¿«ç…§å‹ç¼©æˆåƒ)
  * [Dense Deep Unfolding Network with 3D-CNN Prior for Snapshot Compressive Imaging](https://arxiv.org/abs/2109.06548)<br>:star:[code](https://github.com/jianzhangcs/SCI3D)
* å…‰åœº
  * [Light Field Saliency Detection with Dual Local Graph Learning andReciprocative Guidance](https://arxiv.org/abs/2110.00698)
  * [Fast Light-Field Disparity Estimation With Multi-Disparity-Scale Cost Aggregation](https://openaccess.thecvf.com/content/ICCV2021/papers/Huang_Fast_Light-Field_Disparity_Estimation_With_Multi-Disparity-Scale_Cost_Aggregation_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/zcong17huang/FastLFnet)
  * [SeLFVi: Self-supervised Light-Field Video Reconstruction from Stereo Video](https://openaccess.thecvf.com/content/ICCV2021/papers/Shedligeri_SeLFVi_Self-Supervised_Light-Field_Video_Reconstruction_From_Stereo_Video_ICCV_2021_paper.pdf)
  * [SIGNET: Efficient Neural Representation for Light Fields](https://openaccess.thecvf.com/content/ICCV2021/papers/Feng_SIGNET_Efficient_Neural_Representation_for_Light_Fields_ICCV_2021_paper.pdf)
  * å…‰åœºé‡å»º
    * [Learning Dynamic Interpolation for Extremely Sparse Light Fields With Wide Baselines](https://openaccess.thecvf.com/content/ICCV2021/papers/Guo_Learning_Dynamic_Interpolation_for_Extremely_Sparse_Light_Fields_With_Wide_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/MantangGuo/DI4SLF)
* å‹ç¼©æˆåƒ
  * [Time-Multiplexed Coded Aperture Imaging: Learned Coded Aperture and Pixel Exposures for Compressive Imaging Systems](https://arxiv.org/abs/2104.02820)
* Homography Estimation
  * [LocalTrans: A Multiscale Local Transformer Network for Cross-Resolution Homography Estimation](http://arxiv.org/abs/2106.04067)
* è®¡ç®—æˆåƒ
  * [Extreme-Quality Computational Imaging via Degradation Framework](https://openaccess.thecvf.com/content/ICCV2021/papers/Chen_Extreme-Quality_Computational_Imaging_via_Degradation_Framework_ICCV_2021_paper.pdf)
* å…‰å­¦åƒå·®çŸ«æ­£
  * [Universal and Flexible Optical Aberration Correction Using Deep-Prior Based Deconvolution](https://arxiv.org/abs/2104.03078)<br>:star:[code](https://github.com/leehsiu/UABC)

<a name="57"/>

## 57.Image Matching(å›¾åƒåŒ¹é…)
* [Matching in the Dark: A Dataset for Matching Image Pairs of Low-light Scenes](https://arxiv.org/abs/2109.03585)
* ç‰¹å¾ç‚¹åŒ¹é…
  * [P2-Net: Joint Description and Detection of Local Features for Pixel and Point Matching](https://openaccess.thecvf.com/content/ICCV2021/papers/Wang_P2-Net_Joint_Description_and_Detection_of_Local_Features_for_Pixel_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/BingCS/P2-Net)
 
<a name="56"/>

## 56.Dataset(æ•°æ®é›†)
* [Large Scale Multi-Illuminant (LSMI) Dataset for Developing White Balance Algorithm Under Mixed Illumination](https://openaccess.thecvf.com/content/ICCV2021/papers/Kim_Large_Scale_Multi-Illuminant_LSMI_Dataset_for_Developing_White_Balance_Algorithm_ICCV_2021_paper.pdf)<br>:sunflower:[dataset](https://github.com/DY112/LSMI-dataset)
* [FloW: A Dataset and Benchmark for Floating Waste Detection in Inland Waters](https://openaccess.thecvf.com/content/ICCV2021/papers/Cheng_FloW_A_Dataset_and_Benchmark_for_Floating_Waste_Detection_in_ICCV_2021_paper.pdf)<br>:sunflower:[dataset](https://github.com/ORCA-Uboat/FloW-Dataset)<br>å†…é™†æ°´åŸŸæ¼‚æµ®åºŸç‰©æ£€æµ‹æ•°æ®é›†å’ŒåŸºå‡†
* [FloorPlanCAD: A Large-Scale CAD Drawing Dataset for Panoptic Symbol Spotting](https://openaccess.thecvf.com/content/ICCV2021/papers/Fan_FloorPlanCAD_A_Large-Scale_CAD_Drawing_Dataset_for_Panoptic_Symbol_Spotting_ICCV_2021_paper.pdf)<br>:house:[project](https://floorplancad.github.io/)
* ç”Ÿç‰©åŒ»å­¦å›¾åƒ
  * [BioFors: A Large Biomedical Image Forensics Dataset](https://arxiv.org/abs/2108.12961)
* 3Dé‡å»º
  * [Common Objects in 3D: Large-Scale Learning and Evaluation of Real-life 3D Category Reconstruction](https://arxiv.org/abs/2109.00512)<br>:sunflower:[dataset](https://github.com/facebookresearch/co3d)
* èˆªç©ºå½±åƒæ•°æ®é›†
  * [Beyond Road Extraction: A Dataset for Map Update using Aerial Images](https://arxiv.org/abs/2110.04690)<br>:star:[code](https://github.com/favyen/muno21):house:[project](https://favyen.com/muno21/)<br>ç”¨äºä½¿ç”¨èˆªæ‹å›¾åƒæ›´æ–°åœ°å›¾çš„æ•°æ®é›†
* åŠ¨ä½œè¯†åˆ«
  * [HAA500: Human-Centric Atomic Action Dataset with Curated Videos](https://arxiv.org/abs/2009.05224)
* ç›®æ ‡è¯†åˆ«
  * [ORBIT: A Real-World Few-Shot Dataset for Teachable Object Recognition](https://arxiv.org/abs/2104.03841)<br>:star:[code](https://github.com/microsoft/ORBIT-Dataset):sunflower:[dataset](https://city.figshare.com/articles/dataset/ORBIT_A_real-world_few-shot_dataset_for_teachable_object_recognition_collected_from_people_who_are_blind_or_low_vision/14294597)
* è½¦é“çº¿æ£€æµ‹
  * [VIL-100: A New Dataset and a Baseline Model for Video Instance Lane Detection](https://openaccess.thecvf.com/content/ICCV2021/papers/Zhang_VIL-100_A_New_Dataset_and_a_Baseline_Model_for_Video_ICCV_2021_paper.pdf)<br>:sunflower:[dataset](https://github.com/yujun0-0/MMA-Net)
* è‡ªåŠ¨é©¾é©¶
  * [Large Scale Interactive Motion Forecasting for Autonomous Driving: The Waymo Open Motion Dataset](https://openaccess.thecvf.com/content/ICCV2021/papers/Ettinger_Large_Scale_Interactive_Motion_Forecasting_for_Autonomous_Driving_The_Waymo_ICCV_2021_paper.pdf)
* è§†è§‰è¯­è¨€æ•°æ®é›†
  * [E-ViL: A Dataset and Benchmark for Natural Language Explanations in Vision-Language Tasks](https://openaccess.thecvf.com/content/ICCV2021/papers/Kayser_E-ViL_A_Dataset_and_Benchmark_for_Natural_Language_Explanations_in_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/maximek3/e-ViL)VL
* DeepFakeæ£€æµ‹
  * [KoDF: A Large-Scale Korean DeepFake Detection Dataset](https://arxiv.org/abs/2103.10094)<br>:sunflower:[dataset](https://deepbrainai-research.github.io/kodf/)
* é«˜è´¨é‡è§†é¢‘
  * [Seeing Dynamic Scene in the Dark: A High-Quality Video Dataset With Mechatronic Alignment](https://openaccess.thecvf.com/content/ICCV2021/papers/Wang_Seeing_Dynamic_Scene_in_the_Dark_A_High-Quality_Video_Dataset_ICCV_2021_paper.pdf)<br>:sunflower:[dataset](https://github.com/dvlab-research/SDSD)è§†é¢‘

<a name="55"/>

## 55.Activity Recognition(æ´»åŠ¨è¯†åˆ«)
* [Selective Feature Compression for Efficient Activity Recognition Inference](https://arxiv.org/abs/2104.00179)
* å°ç»„æ´»åŠ¨è¯†åˆ«
  * [Spatio-Temporal Dynamic Inference Network for Group Activity Recognition](https://arxiv.org/abs/2108.11743)<br>:star:[code](https://github.com/JacobYuan7/DIN_GAR)
  * [GroupFormer: Group Activity Recognition with Clustered Spatial-Temporal Transformer](https://arxiv.org/abs/2108.12630)<br>:star:[code](https://github.com/xueyee/GroupFormer) 
 

<a name="54"/>

## 54.Sketch recognition(è‰å›¾)
* [SketchLattice: Latticed Representation for Sketch Manipulation](https://arxiv.org/abs/2108.11636)
* [SketchAA: Abstract Representation for Abstract Sketches](https://openaccess.thecvf.com/content/ICCV2021/papers/Yang_SketchAA_Abstract_Representation_for_Abstract_Sketches_ICCV_2021_paper.pdf)
* [SketchLattice: Latticed Representation for Sketch Manipulation](https://arxiv.org/abs/2108.11636)

<a name="53"/>

## 53.Vision Localization(è§†è§‰å®šä½)
* [Continual Learning for Image-Based Camera Localization](https://arxiv.org/abs/2108.09112)<br>:star:[code](https://github.com/AaltoVision/CL_HSCNet)
* [CrowdDriven: A New Challenging Dataset for Outdoor Visual Localization](https://arxiv.org/abs/2109.04527)<br>:sunflower:[dataset](http://mapillary.com/)
* [Pose Correction for Highly Accurate Visual Localization in Large-Scale Indoor Spaces](https://openaccess.thecvf.com/content/ICCV2021/papers/Hyeon_Pose_Correction_for_Highly_Accurate_Visual_Localization_in_Large-Scale_Indoor_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/JanghunHyeon/PCLoc)
* [Cross-Descriptor Visual Localization and Mapping](https://openaccess.thecvf.com/content/ICCV2021/papers/Dusmanu_Cross-Descriptor_Visual_Localization_and_Mapping_ICCV_2021_paper.pdf)

<a name="52"/>

## 52.Vision-and-Language(è§†è§‰è¯­è¨€)
* [YouRefIt: Embodied Reference Understanding with Language and Gesture](https://arxiv.org/abs/2109.03413)<br>:open_mouth:oral:house:[project](https://yixchen.github.io/YouRefIt/)
* [VLGrammar: Grounded Grammar Induction of Vision and Language](https://arxiv.org/abs/2103.12975)<br>:star:[code](https://github.com/evelinehong/VLGrammar)
* [COOKIE: Contrastive Cross-Modal Knowledge Sharing Pre-Training for Vision-Language Representation](https://openaccess.thecvf.com/content/ICCV2021/papers/Wen_COOKIE_Contrastive_Cross-Modal_Knowledge_Sharing_Pre-Training_for_Vision-Language_Representation_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/kywen1119/COOKIE)
* [Panoptic Narrative Grounding](https://openaccess.thecvf.com/content/ICCV2021/papers/Gonzalez_Panoptic_Narrative_Grounding_ICCV_2021_paper.pdf)<br>:open_mouth:oral:star:[code](https://github.com/BCV-Uniandes/PNG)
* [AESOP: Abstract Encoding of Stories, Objects, and Pictures](https://openaccess.thecvf.com/content/ICCV2021/papers/Ravi_AESOP_Abstract_Encoding_of_Stories_Objects_and_Pictures_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/Hareesh-Ravi/AESOP):tv:[video](https://www.youtube.com/watch?v=ygGzY1DSSMk)
* [Adaptive Hierarchical Graph Reasoning With Semantic Coherence for Video-and-Language Inference](https://arxiv.org/abs/2107.12270)
* è§†è§‰æ¨ç†
  * [Interpretable Visual Reasoning via Induced Symbolic Space](https://arxiv.org/abs/2011.11603)
* è¯­ä¹‰å¯¼èˆª
  * [THDA: Treasure Hunt Data Augmentation for Semantic Navigation](https://openaccess.thecvf.com/content/ICCV2021/papers/Maksymets_THDA_Treasure_Hunt_Data_Augmentation_for_Semantic_Navigation_ICCV_2021_paper.pdf)
* è§†è§‰è¯­è¨€å¯¼èˆª
  * [Airbert: In-domain Pretraining for Vision-and-Language Navigation](https://arxiv.org/abs/2108.09105)<br>:house:[project](https://airbert-vln.github.io/)
  * [Waypoint Models for Instruction-guided Navigation in Continuous Environments](https://arxiv.org/abs/2110.02207)<br>:open_mouth:oral:star:[code](https://github.com/jacobkrantz/VLN-CE):house:[project](https://jacobkrantz.github.io/waypoint-vlnce/):tv:[video](https://youtu.be/hrHj9-1xoio)
  * [The Road To Know-Where: An Object-and-Room Informed Sequential BERT for Indoor Vision-Language Navigation](https://openaccess.thecvf.com/content/ICCV2021/papers/Qi_The_Road_To_Know-Where_An_Object-and-Room_Informed_Sequential_BERT_for_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/YuankaiQi/ORIST)
  * [Vision-Language Navigation With Random Environmental Mixup](https://arxiv.org/abs/2106.07876)
* è§†è§‰å¯¹è¯å¯¼èˆª
  * [Self-Motivated Communication Agent for Real-World Vision-Dialog Navigation](https://openaccess.thecvf.com/content/ICCV2021/papers/Zhu_Self-Motivated_Communication_Agent_for_Real-World_Vision-Dialog_Navigation_ICCV_2021_paper.pdf)
* è§†è§‰å¯¼èˆª
  * [Pose Invariant Topological Memory for Visual Navigation](https://openaccess.thecvf.com/content/ICCV2021/papers/Taniguchi_Pose_Invariant_Topological_Memory_for_Visual_Navigation_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/jonkhler/s2cnn)
  * [Visual Graph Memory With Unsupervised Representation for Visual Navigation](https://openaccess.thecvf.com/content/ICCV2021/papers/Kwon_Visual_Graph_Memory_With_Unsupervised_Representation_for_Visual_Navigation_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/rllab-snu/Visual-Graph-Memory):house:[project](https://rllab-snu.github.io/projects/vgm/doc.html):tv:[video](https://www.youtube.com/watch?v=Uksb_kR80Hk)
* visual grounding
  * [InstanceRefer: Cooperative Holistic Understanding for Visual Grounding on Point Clouds through Instance Multi-level Contextual Referring](https://arxiv.org/abs/2103.01128)<br>:star:[code](https://github.com/CurryYuan/InstanceRefer)
  * [TransVG: End-to-End Visual Grounding With Transformers](https://arxiv.org/abs/2104.08541)<br>:star:[code](https://github.com/djiajunustc/TransVG)
* è§†è§‰å¯¹è¯
  * [Unified Questioner Transformer for Descriptive Question Generation in Goal-Oriented Visual Dialogue](https://arxiv.org/abs/2106.15550)

<a name="51"/>

## 51.View Synthesis(è§†å›¾åˆæˆ)
* [Out-of-boundary View Synthesis Towards Full-Frame Video Stabilization](https://arxiv.org/abs/2108.09041)<br>:star:[code](https://github.com/Annbless/OVS_Stabilization)
* [Deep 3D Mask Volume for View Synthesis of Dynamic Scenes](https://arxiv.org/abs/2108.13408)<br>:house:[project](https://cseweb.ucsd.edu//~viscomp/projects/ICCV21Deep/)
* [Embedding Novel Views in a Single JPEG Image](https://arxiv.org/abs/2108.13003)
* [Video Autoencoder: self-supervised disentanglement of static 3D structure and motion](https://arxiv.org/abs/2110.02951)<br>:open_mouth:oral:star:[code](https://github.com/zlai0/VideoAutoencoder/):house:[project](https://zlai0.github.io/VideoAutoencoder/#method_video):tv:[video](https://www.youtube.com/watch?v=UaJZd4FrM8E)
* [Geometry-Free View Synthesis: Transformers and No 3D Priors](https://arxiv.org/abs/2104.07652)<br>:star:[code](https://github.com/CompVis/geometry-free-view-synthesis)
* [Dynamic View Synthesis From Dynamic Monocular Video](https://arxiv.org/abs/2105.06468)<br>:house:[project](https://free-view-video.github.io/):tv:[video](https://youtu.be/j8CUzIR0f8M)
* [Putting NeRF on a Diet: Semantically Consistent Few-Shot View Synthesis](https://arxiv.org/abs/2104.00677)<br>:house:[project](https://www.ajayj.com/dietnerf):tv:[video](https://youtu.be/RF_3hsNizqw)
* [Infinite Nature: Perpetual View Generation of Natural Scenes from a Single Image](https://arxiv.org/abs/2012.09855)<br>:open_mouth:oral:star:[code](https://github.com/google-research/google-research/tree/master/infinite_nature):house:[project](https://infinite-nature.github.io/):tv:[video](https://www.youtube.com/watch?v=oXUf6anNAtc)
* [Worldsheet: Wrapping the World in a 3D Sheet for View Synthesis from a Single Image](https://arxiv.org/abs/2012.09854)<br>:open_mouth:oral:star:[code](https://github.com/facebookresearch/worldsheet):house:[project](https://worldsheet.github.io/):tv:[video](https://youtu.be/j5aT3zRxFlk)

<a name="50"/>

## 50.Continual Learning(æŒç»­å­¦ä¹ )
* [Online Continual Learning with Natural Distribution Shifts: An Empirical Study with Visual Data](https://arxiv.org/abs/2108.09020)<br>:star:[code](https://github.com/IntelLabs/continuallearning)
* [Continual Learning on Noisy Data Streams via Self-Purified Replay](https://openaccess.thecvf.com/content/ICCV2021/papers/Kim_Continual_Learning_on_Noisy_Data_Streams_via_Self-Purified_Replay_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/ecrireme/SPR)
* [Rehearsal Revealed: The Limits and Merits of Revisiting Samples in Continual Learning](https://arxiv.org/abs/2104.07446)<br>:star:[code](https://github.com/Mattdl/RehearsalRevealed)
* [Co2L: Contrastive Continual Learning](https://openaccess.thecvf.com/content/ICCV2021/papers/Cha_Co2L_Contrastive_Continual_Learning_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/chaht01/Co2L)

  
<a name="49"/>

## 49.Human-Object Interaction(äººç‰©äº¤äº’)
* [Exploiting Scene Graphs for Human-Object Interaction Detection](https://arxiv.org/abs/2108.08584)<br>:star:[code](https://github.com/ht014/SG2HOI)
* [Spatially Conditioned Graphs for Detecting Human-Object Interactions](https://arxiv.org/abs/2012.06060)<br>:star:[code](https://github.com/fredzzhang/spatially-conditioned-graphs):tv:[video](https://www.youtube.com/watch?v=gkBWi_rWedU)
* [Virtual Multi-Modality Self-Supervised Foreground Matting for Human-Object Interaction](https://arxiv.org/abs/2110.03278)
* [Detecting Human-Object Relationships in Videos](https://openaccess.thecvf.com/content/ICCV2021/papers/Ji_Detecting_Human-Object_Relationships_in_Videos_ICCV_2021_paper.pdf)
* [Weakly Supervised Human-Object Interaction Detection in Video via Contrastive Spatiotemporal Regions](https://openaccess.thecvf.com/content/ICCV2021/papers/Li_Weakly_Supervised_Human-Object_Interaction_Detection_in_Video_via_Contrastive_Spatiotemporal_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/ShuangLI59/weakly-supervised-human-object-detection-video):house:[project](https://shuangli-project.github.io/weakly-supervised-human-object-detection-video/):sunflower:[dataset](https://shuangli-project.github.io/VHICO-Dataset/)
* [Discovering Human Interactions With Large-Vocabulary Objects via Query and Multi-Scale Detection](https://openaccess.thecvf.com/content/ICCV2021/papers/Wang_Discovering_Human_Interactions_With_Large-Vocabulary_Objects_via_Query_and_Multi-Scale_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/scwangdyd/large_vocabulary_hoi_detection)
* [Visual Relationship Detection Using Part-and-Sum Transformers With Composite Queries](https://arxiv.org/abs/2105.02170)VRDå’ŒHOI
* [Interaction Compass: Multi-Label Zero-Shot Learning of Human-Object Interactions via Spatial Relations](https://openaccess.thecvf.com/content/ICCV2021/papers/Huynh_Interaction_Compass_Multi-Label_Zero-Shot_Learning_of_Human-Object_Interactions_via_Spatial_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/hbdat/iccv21_relational_direction)
* H2O
  * [H2O: A Benchmark for Visual Human-Human Object Handover Analysis](https://arxiv.org/abs/2104.11466)
* Human Interaction Understanding
  * [Consistency-Aware Graph Network for Human Interaction Understanding](https://arxiv.org/abs/2011.10250)<br>:star:[code](https://github.com/deepgogogo/CAGNet?v=1)
  * [H2O: Two Hands Manipulating Objects for First Person Interaction Recognition](https://arxiv.org/abs/2104.11181)<br>:house:[project](https://www.taeinkwon.com/projects/h2o)
* æ‰‹ç‰©äº¤äº’
  * [Toward Human-Like Grasp: Dexterous Grasping via Semantic Representation of Object-Hand](https://openaccess.thecvf.com/content/ICCV2021/papers/Zhu_Toward_Human-Like_Grasp_Dexterous_Grasping_via_Semantic_Representation_of_Object-Hand_ICCV_2021_paper.pdf)
  * [Reconstructing Hand-Object Interactions in the Wild](https://arxiv.org/abs/2012.09856)<br>:house:[project](https://people.eecs.berkeley.edu/~zhecao/rhoi/)
  * [CPF: Learning a Contact Potential Field To Model the Hand-Object Interaction](https://arxiv.org/abs/2012.00924)<br>:star:[code](https://github.com/lixiny/CPF)æ‰‹ç‰©äº¤äº’
* HOI(è¡Œä¸ºç†è§£)
  * [GeomNet: A Neural Network Based on Riemannian Geometries of SPD Matrix Space and Cholesky Space for 3D Skeleton-Based Interaction Recognition](https://openaccess.thecvf.com/content/ICCV2021/papers/Nguyen_GeomNet_A_Neural_Network_Based_on_Riemannian_Geometries_of_SPD_ICCV_2021_paper.pdf)

<a name="48"/>

## 48.6DoF
* [SO-Pose: Exploiting Self-Occlusion for Direct 6D Pose Estimation](https://arxiv.org/abs/2108.08367)<br>:star:[code](https://github.com/shangbuhuan13/SO-Pose)
* [StereOBJ-1M: Large-scale Stereo Image Dataset for 6D Object Pose Estimation](https://arxiv.org/abs/2109.10115)
* [SGPA: Structure-Guided Prior Adaptation for Category-Level 6D Object Pose Estimation](https://openaccess.thecvf.com/content/ICCV2021/papers/Chen_SGPA_Structure-Guided_Prior_Adaptation_for_Category-Level_6D_Object_Pose_Estimation_ICCV_2021_paper.pdf)
* [RePOSE: Fast 6D Object Pose Refinement via Deep Texture Rendering](https://arxiv.org/abs/2104.00633)<br>:star:[code](https://github.com/sh8/repose)
* [DualPoseNet: Category-Level 6D Object Pose and Size Estimation Using Dual Pose Network With Refined Learning of Pose Consistency](https://arxiv.org/abs/2103.06526)<br>:star:[code](https://github.com/Gorilla-Lab-SCUT/DualPoseNet)
* [PR-GCN: A Deep Graph Convolutional Network With Point Refinement for 6D Pose Estimation](https://openaccess.thecvf.com/content/ICCV2021/papers/Zhou_PR-GCN_A_Deep_Graph_Convolutional_Network_With_Point_Refinement_for_ICCV_2021_paper.pdf)
* ç‰©ä½“å§¿åŠ¿ä¼°è®¡
  * [CAPTRA: CAtegory-Level Pose Tracking for Rigid and Articulated Objects From Point Clouds](https://arxiv.org/abs/2104.03437)<br>:open_mouth:oral:star:[code](https://github.com/halfsummer11/CAPTRA):house:[project](https://yijiaweng.github.io/CAPTRA/):tv:[video](https://youtu.be/EkcCEj7gZGg)

<a name="47"/>

## 47.NAS
* [BN-NAS: Neural Architecture Search with Batch Normalization](https://arxiv.org/abs/2108.07375)<br>:star:[code](https://github.com/bychen515/BNNAS)
* [RANK-NOSH: Efficient Predictor-Based Architecture Search via Non-Uniform Successive Halving](https://arxiv.org/abs/2108.08019)
* [Pi-NAS: Improving Neural Architecture Search by Reducing Supernet Training Consistency Shift](https://arxiv.org/abs/2108.09671)<br>:star:[code](https://github.com/Ernie1/Pi-NAS)
* [Evolving Search Space for Neural Architecture Search](https://arxiv.org/abs/2011.10904)<br>:star:[code](https://github.com/orashi/NSE_NAS):tv:[video](https://www.youtube.com/watch?v=fq21WBaumRc)
* [FairNAS: Rethinking Evaluation Fairness of Weight Sharing Neural Architecture Search](https://arxiv.org/abs/1907.01845)<br>:star:[code](https://github.com/xiaomi-automl/FairNAS)
* [GLiT: Neural Architecture Search for Global and Local Image Transformer](https://arxiv.org/abs/2107.02960)<br>:star:[code](https://github.com/bychen515/GLiT)
* [Neural Architecture Search for Joint Human Parsing and Pose Estimation](https://openaccess.thecvf.com/content/ICCV2021/papers/Zeng_Neural_Architecture_Search_for_Joint_Human_Parsing_and_Pose_Estimation_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/GuHuangAI/NPP)
* [Distilling Optimal Neural Networks: Rapid Search in Diverse Spaces](https://arxiv.org/abs/2012.08859)
* [Learning Latent Architectural Distribution in Differentiable Neural Architecture Search via Variational Information Maximization](https://openaccess.thecvf.com/content/ICCV2021/papers/Wang_Learning_Latent_Architectural_Distribution_in_Differentiable_Neural_Architecture_Search_via_ICCV_2021_paper.pdf)
* [Not All Operations Contribute Equally: Hierarchical Operation-Adaptive Predictor for Neural Architecture Search](https://openaccess.thecvf.com/content/ICCV2021/papers/Chen_Not_All_Operations_Contribute_Equally_Hierarchical_Operation-Adaptive_Predictor_for_Neural_ICCV_2021_paper.pdf)
* [Zen-NAS: A Zero-Shot NAS for High-Performance Image Recognition](https://openaccess.thecvf.com/content/ICCV2021/papers/Lin_Zen-NAS_A_Zero-Shot_NAS_for_High-Performance_Image_Recognition_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/idstcv/ZenNAS)
* [BossNAS: Exploring Hybrid CNN-Transformers With Block-Wisely Self-Supervised Neural Architecture Search](https://arxiv.org/abs/2103.12424)<br>:star:[code](https://github.com/changlin31/BossNAS)
* [NAS-OoD: Neural Architecture Search for Out-of-Distribution Generalization](https://openaccess.thecvf.com/content/ICCV2021/papers/Bai_NAS-OoD_Neural_Architecture_Search_for_Out-of-Distribution_Generalization_ICCV_2021_paper.pdf)
* [AutoSpace: Neural Architecture Search With Less Human Interference](https://arxiv.org/abs/2103.11833)<br>:star:[code](https://github.com/zhoudaquan/AutoSpace)
* [IDARTS: Interactive Differentiable Architecture Search](https://openaccess.thecvf.com/content/ICCV2021/papers/Xue_IDARTS_Interactive_Differentiable_Architecture_Search_ICCV_2021_paper.pdf)

<a name="46"/>

## 46.Defect Detection(ç¼ºé™·æ£€æµ‹)
* [DRÃ†M -- A discriminatively trained reconstruction embedding for surface anomaly detection](https://arxiv.org/abs/2108.07610)

<a name="45"/>

## 45.Image Caption(å›¾åƒå­—å¹•)
* [Who's Waldo? Linking People Across Text and Images](https://arxiv.org/abs/2108.07253)<br>:open_mouth:oral:house:[project](https://whoswaldo.github.io/)<br>:newspaper:è§£è¯»:[ICCV2021 Oral-æ–°ä»»åŠ¡ï¼æ–°æ•°æ®é›†ï¼åº·å¥ˆå°”å¤§å­¦æå‡ºäº†ç±»ä¼¼VGä½†åˆä¸æ˜¯VGçš„PVGä»»åŠ¡](https://mp.weixin.qq.com/s/QC1UQRmZKgS0dctTXQ77Bg)
* [Partial Off-Policy Learning: Balance Accuracy and Diversity for Human-Oriented Image Captioning](https://openaccess.thecvf.com/content/ICCV2021/papers/Shi_Partial_Off-Policy_Learning_Balance_Accuracy_and_Diversity_for_Human-Oriented_Image_ICCV_2021_paper.pdf)
* [Topic Scene Graph Generation by Attention Distillation From Caption](https://openaccess.thecvf.com/content/ICCV2021/papers/Wang_Topic_Scene_Graph_Generation_by_Attention_Distillation_From_Caption_ICCV_2021_paper.pdf)<br>:star:[code](https://vipl.ict.ac.cn/view_database.php?id=6)
* [Understanding and Evaluating Racial Biases in Image Captioning](https://arxiv.org/abs/2106.08503)<br>:star:[code](https://github.com/princetonvisualai/imagecaptioning-bias):house:[project](https://princetonvisualai.github.io/imagecaptioning-bias/)
* [In Defense of Scene Graphs for Image Captioning](https://arxiv.org/abs/2102.04990)<br>:star:[code](https://github.com/Kien085/SG2Caps)
* art description generation(è‰ºæœ¯æè¿°ç”Ÿæˆ)
  * [Explain Me the Painting: Multi-Topic Knowledgeable Art Description Generation](https://arxiv.org/abs/2109.05743)<br>:star:[code](https://github.com/noagarcia/explain-paintings)
* Change Captioning
  * [Viewpoint-Agnostic Change Captioning With Cycle Consistency](https://openaccess.thecvf.com/content/ICCV2021/papers/Kim_Viewpoint-Agnostic_Change_Captioning_With_Cycle_Consistency_ICCV_2021_paper.pdf)

<a name="44"/>

## 44.Human motion prediction(äººä½“è¿åŠ¨é¢„æµ‹)
* [MSR-GCN: Multi-Scale Residual Graph Convolution Networks for Human Motion Prediction](https://arxiv.org/abs/2108.07152)<br>:star:[code](https://github.com/Droliven/MSRGCN)
* [Stochastic Scene-Aware Motion Prediction](https://arxiv.org/abs/2108.08284)<br>:star:[code](https://github.com/mohamedhassanmus/SAMP):house:[project](https://samp.is.tue.mpg.de/)  
* [Generating Smooth Pose Sequences for Diverse Human Motion Prediction](https://arxiv.org/abs/2108.08422)<br>:open_mouth:oral:star:[code](https://github.com/wei-mao-2019/gsps)
* [TRiPOD: Human Trajectory and Pose Dynamics Forecasting in the Wild](https://arxiv.org/abs/2104.04029)<br>:house:[project](https://somof.stanford.edu/)
* [Motion Prediction using Trajectory Cues](https://openaccess.thecvf.com/content/ICCV2021/papers/Liu_Motion_Prediction_Using_Trajectory_Cues_ICCV_2021_paper.pdf)
* 3Däººä½“è¿åŠ¨é¢„æµ‹
  * [Contextually Plausible and Diverse 3D Human Motion Prediction](https://arxiv.org/abs/1912.08521)

<a name="43"/>

## 43.Dense Prediction(å¯†é›†é¢„æµ‹)
* [FaPN: Feature-aligned Pyramid Network for Dense Image Prediction](https://arxiv.org/abs/2108.07058)<br>:star:[code](https://github.com/EMI-Group/FaPN)
* å¤šä»»åŠ¡å¯†é›†é¢„æµ‹
  * [Exploring Relational Context for Multi-Task Dense Prediction](https://arxiv.org/abs/2104.13874)

<a name="42"/>

## 42.Representations Learning(è¡¨å¾å­¦ä¹ )
* [Learning From Noisy Data With Robust Representation Learning](https://openaccess.thecvf.com/content/ICCV2021/papers/Li_Learning_From_Noisy_Data_With_Robust_Representation_Learning_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/salesforce/RRL/)
* [Self-Supervised Representation Learning From Flow Equivariance](https://arxiv.org/abs/2101.06553)
* [Exploring Visual Engagement Signals for Representation Learning](https://arxiv.org/abs/2104.07767)<br>:star:[code](https://github.com/KMnP/vise)
* [Switchable K-class Hyperplanes for Noise-Robust Representation Learning](https://openaccess.thecvf.com/content/ICCV2021/papers/Liu_Switchable_K-Class_Hyperplanes_for_Noise-Robust_Representation_Learning_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/liubx07/SKH)
* [Region Similarity Representation Learning](https://arxiv.org/abs/2103.12902)<br>:star:[code](https://github.com/Tete-Xiao/ReSim)
* [Curious Representation Learning for Embodied Intelligence](https://arxiv.org/abs/2105.01060)<br>:star:[code](https://github.com/yilundu/crl):house:[project](https://yilundu.github.io/crl/)
* è§†è§‰è¡¨å¾å­¦ä¹ 
  * [Self-Supervised Visual Representations Learning by Contrastive Mask Prediction](https://arxiv.org/abs/2108.07954)
  * [Temporal Knowledge Consistency for Unsupervised Visual Representation Learning](https://arxiv.org/abs/2108.10668)
  * [Contrasting Contrastive Self-Supervised Representation Learning Pipelines](https://arxiv.org/abs/2103.14005)<br>:star:[code](https://github.com/allenai/virb)
  * [Concept Generalization in Visual Representation Learning](https://arxiv.org/abs/2012.05649)<br>:house:[project](https://europe.naverlabs.com/cog-benchmark)
  * [Collaborative Unsupervised Visual Representation Learning from Decentralized Data](https://arxiv.org/abs/2108.06492)
  * [Episodic Transformer for Vision-and-Language Navigation](https://arxiv.org/abs/2105.06453)<br>:star:[code](https://github.com/alexpashevich/E.T.)
  * [Multi-VAE: Learning Disentangled View-Common and View-Peculiar Visual Representations for Multi-View Clustering](https://openaccess.thecvf.com/content/ICCV2021/papers/Xu_Multi-VAE_Learning_Disentangled_View-Common_and_View-Peculiar_Visual_Representations_for_Multi-View_ICCV_2021_paper.pdf)
* è§†é¢‘è¡¨ç¤ºå­¦ä¹ 
  * [Composable Augmentation Encoding for Video Representation Learning](https://arxiv.org/abs/2104.00616)
  * [Motion-Focused Contrastive Learning of Video Representations](https://openaccess.thecvf.com/content/ICCV2021/papers/Li_Motion-Focused_Contrastive_Learning_of_Video_Representations_ICCV_2021_paper.pdf)
 * [ASCNet: Self-Supervised Video Representation Learning With Appearance-Speed Consistency](https://arxiv.org/abs/2106.02342)
  * [ACAV100M: Automatic Curation of Large-Scale Datasets for Audio-Visual Video Representation Learning](https://arxiv.org/abs/2101.10803)<br>:house:[project](https://acav100m.github.io/)
  * [Time-Equivariant Contrastive Video Representation Learning](https://openaccess.thecvf.com/content/ICCV2021/papers/Jenni_Time-Equivariant_Contrastive_Video_Representation_Learning_ICCV_2021_paper.pdf)
  * [Space-Time Crop & Attend: Improving Cross-Modal Video Representation Learning](https://arxiv.org/abs/2103.10211)<br>:star:[code](https://github.com/facebookresearch/GDT)

<a name="41"/>

## 41.Out-of-Distribution Detection(OOD)
* [CODEs: Chamfer Out-of-Distribution Examples against Overconfidence Issue](https://arxiv.org/abs/2108.06024)
* [Semantically Coherent Out-of-Distribution Detection](https://arxiv.org/abs/2108.11941)<br>:star:[code](https://github.com/jingkang50/ICCV21_SCOOD):house:[project](https://jingkang50.github.io/projects/scood)
* [The Many Faces of Robustness: A Critical Analysis of Out-of-Distribution Generalization](https://arxiv.org/abs/2006.16241)<br>:star:[code](https://github.com/hendrycks/imagenet-r)

<a name="40"/>

## 40.Metric Learning(åº¦é‡å­¦ä¹ )
* [Towards Interpretable Deep Metric Learning with Structural Matching](https://arxiv.org/abs/2108.05889)<br>:star:[code](https://github.com/wl-zhao/DIML)
* [Deep Relational Metric Learning](https://arxiv.org/abs/2108.10026)<br>:star:[code](https://github.com/zbr17/DRML)
* [LoOp: Looking for Optimal Hard Negative Embeddings for Deep Metric Learning](https://arxiv.org/abs/2108.09335)<br>:star:[code](https://github.com/puneesh00/LoOp)
* [Manifold Matching via Deep Metric Learning for Generative Modeling](https://arxiv.org/abs/2106.10777)<br>:star:[code](https://github.com/dzld00/pytorch-manifold-matching)

<a name="39"/>

## 39.Incremental Learning(å¢é‡å­¦ä¹ )
* ç±»å¢é‡å­¦ä¹ 
  * [Always Be Dreaming: A New Approach for Data-Free Class-Incremental Learning](https://arxiv.org/abs/2106.09701)<br>:newspaper:è§£è¯»:[è®©æ¨¡å‹å®ç°â€œç»ˆç”Ÿå­¦ä¹ â€ï¼Œä½æ²»äºšç†å·¥å­¦é™¢æå‡ºData-Freeçš„å¢é‡å­¦ä¹ ](https://mp.weixin.qq.com/s/Fm9ufPD6rzL2VzaqpdFpjg)
  * [Striking a Balance Between Stability and Plasticity for Class-Incremental Learning](https://openaccess.thecvf.com/content/ICCV2021/papers/Wu_Striking_a_Balance_Between_Stability_and_Plasticity_for_Class-Incremental_Learning_ICCV_2021_paper.pdf)
  * [Synthesized Feature Based Few-Shot Class-Incremental Learning on a Mixture of Subspaces](https://openaccess.thecvf.com/content/ICCV2021/papers/Cheraghian_Synthesized_Feature_Based_Few-Shot_Class-Incremental_Learning_on_a_Mixture_of_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/ali-chr/Synthesized-Feature-based-Few-Shot-Class-Incremental-Learningon-a-Mixture-of-Subspaces)

<a name="38"/>

## 38.Weakly/Semi-Supervised/Self-supervised/Unsupervised Learning(è‡ª/åŠ/å¼±ç›‘ç£å­¦ä¹ )
* åŠç›‘ç£
  * [Trash to Treasure: Harvesting OOD Data with Cross-Modal Matching for Open-Set Semi-Supervised Learning](https://arxiv.org/abs/2108.05617)
  * [Semi-Supervised Learning of Visual Features by Non-Parametrically Predicting View Assignments With Support Samples](https://arxiv.org/abs/2104.13963)<br>:star:[code](https://github.com/facebookresearch/suncet)
  * [Semi-Supervised Active Learning for Semi-Supervised Models: Exploit Adversarial Examples With Graph-Based Virtual Labels](https://openaccess.thecvf.com/content/ICCV2021/papers/Guo_Semi-Supervised_Active_Learning_for_Semi-Supervised_Models_Exploit_Adversarial_Examples_With_ICCV_2021_paper.pdf)
  * [CoMatch: Semi-Supervised Learning With Contrastive Graph Regularization](https://arxiv.org/abs/2011.11183)<br>:star:[code](https://github.com/salesforce/CoMatch)
  * [Multiview Pseudo-Labeling for Semi-supervised Learning from Video](https://arxiv.org/abs/2104.00682)
* è‡ªç›‘ç£
  * [Focus on the Positives: Self-Supervised Learning for Biodiversity Monitoring](https://arxiv.org/abs/2108.06435)<br>:star:[code](https://github.com/omipan/camera_traps_self_supervised)
  * [Self-supervised Neural Networks for Spectral Snapshot Compressive Imaging](https://arxiv.org/abs/2108.12654)<br>:star:[code](https://github.com/mengziyi64/CASSI-Self-Supervised)
  * [ISD: Self-Supervised Learning by Iterative Similarity Distillation](https://arxiv.org/abs/2012.09259)<br>:star:[code](https://github.com/UMBCvision/ISD)
  * [Contrast and Order Representations for Video Self-Supervised Learning](https://openaccess.thecvf.com/content/ICCV2021/papers/Hu_Contrast_and_Order_Representations_for_Video_Self-Supervised_Learning_ICCV_2021_paper.pdf)
  * [On Feature Decorrelation in Self-Supervised Learning](https://arxiv.org/abs/2105.00470)<br>:open_mouth:oral
  * [Geography-Aware Self-Supervised Learning](https://arxiv.org/abs/2011.09980)
  * [Multimodal Clustering Networks for Self-supervised Learning from Unlabeled Videos](https://arxiv.org/abs/2104.12671)
  * [Efficient Visual Pretraining with Contrastive Detection](https://arxiv.org/abs/2103.10957)
  * [Broaden Your Views for Self-Supervised Video Learning](https://arxiv.org/abs/2103.16559)
  * [CDS: Cross-Domain Self-supervised Pre-training](https://openaccess.thecvf.com/content/ICCV2021/papers/Kim_CDS_Cross-Domain_Self-Supervised_Pre-Training_ICCV_2021_paper.pdf)
  * [On Compositions of Transformations in Contrastive Self-Supervised Learning](https://openaccess.thecvf.com/content/ICCV2021/papers/Patrick_On_Compositions_of_Transformations_in_Contrastive_Self-Supervised_Learning_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/facebookresearch/GDT)
  * [Solving Inefficiency of Self-Supervised Representation Learning](https://arxiv.org/abs/2104.08760)<br>:star:[code](https://github.com/wanggrun/triplet)
  * [Divide and Contrast: Self-supervised Learning from Uncurated Data](https://openaccess.thecvf.com/content/ICCV2021/papers/Tian_Divide_and_Contrast_Self-Supervised_Learning_From_Uncurated_Data_ICCV_2021_paper.pdf)
  * [Emerging Properties in Self-Supervised Vision Transformers](https://arxiv.org/abs/2104.14294)<br>:star:[code](https://github.com/facebookresearch/dino)
  * [Mean Shift for Self-Supervised Learning](https://arxiv.org/abs/2105.07269)<br>:star:[code](https://github.com/UMBCvision/MSF)
* å¼±ç›‘ç£
  * [Weakly Supervised Representation Learning With Coarse Labels](https://arxiv.org/abs/2005.09681)<br>:star:[code](https://github.com/idstcv/CoIns)

<a name="37"/>

## 37.Multitask Learning(å¤šä»»åŠ¡å­¦ä¹ )
* [MultiTask-CenterNet (MCN): Efficient and Diverse Multitask Learning using an Anchor Free Approach](https://arxiv.org/abs/2108.05060)<br>:newspaper:è§£è¯»:[ICCV2021ã€ŠMultiTask CenterNetã€‹CVå¤šä»»åŠ¡æ–°è¿›å±•ï¼ä¸€èŠ‚æ›´æ¯”ä¸‰èŠ‚å¼º](https://mp.weixin.qq.com/s/toAZS0OHdW4MG30P1wAAUA)
* [Multi-Task Self-Training for Learning General Representations](https://arxiv.org/abs/2108.11353)<br>:newspaper:è§£è¯»:[ICCV2021 MuSTï¼šè¿˜åœ¨ç‰¹å®šä»»åŠ¡é‡Œä¸ºåˆ·ç‚¹è€Œè‹¦è‹¦æŒ£æ‰ï¼Ÿè°·æ­Œçš„å¤§ä½¬ä»¬éƒ½å·²ç»å¼€å§‹ç©å¤šä»»åŠ¡è®­ç»ƒäº†](https://mp.weixin.qq.com/s/nhv1l9xBSaZceibIn5fhqw)
* [UniT: Multimodal Multitask Learning With a Unified Transformer](https://arxiv.org/abs/2102.10772)<br>:star:[code](https://mmf.sh/)
* [Learning Multiple Pixelwise Tasks Based on Loss Scale Balancing](https://openaccess.thecvf.com/content/ICCV2021/papers/Lee_Learning_Multiple_Pixelwise_Tasks_Based_on_Loss_Scale_Balancing_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/jaehanlee-mcl/LSB-MTL)
* [Learning With Privileged Tasks](https://openaccess.thecvf.com/content/ICCV2021/papers/Song_Learning_With_Privileged_Tasks_ICCV_2021_paper.pdf)
* [Task Switching Network for Multi-Task Learning](https://openaccess.thecvf.com/content/ICCV2021/papers/Sun_Task_Switching_Network_for_Multi-Task_Learning_ICCV_2021_paper.pdf)

<a name="36"/>

## 36.SLAM/AR/VR/æœºå™¨äºº
* æœºå™¨äºº
  * å®¤å†…å¯¼èˆª
    * [The Surprising Effectiveness of Visual Odometry Techniques for Embodied PointGoal Navigation](https://arxiv.org/abs/2108.11550)<br>:star:[code](https://github.com/Xiaoming-Zhao/PointNav-VO):house:[project](https://xiaoming-zhao.github.io/projects/pointnav-vo/)
    * [Pathdreamer: A World Model for Indoor Navigation](https://arxiv.org/abs/2105.08756)<br>:tv:[video](https://www.youtube.com/watch?v=StklIENGqs0)
  * æœºå™¨æ‰‹æŠ“å–
    * [Hand-Object Contact Consistency Reasoning for Human Grasps Generation](https://arxiv.org/abs/2104.03304)<br>:open_mouth:oral:star:[code](https://github.com/hwjiang1510/GraspTTA):house:[project](https://hwjiang1510.github.io/GraspTTA/):tv:[video](https://youtu.be/zGVLVXZoVZs)
* VR/AR
  * [The Power of Points for Modeling Humans in Clothing](https://arxiv.org/abs/2109.01137)<br>:star:[code](https://github.com/qianlim/POP):house:[project](https://qianlim.github.io/POP):tv:[video](https://youtu.be/5M4F9zSWIEE)
  * è™šæ‹Ÿè¯•ç©¿  
    * [M3D-VTON: A Monocular-to-3D Virtual Try-On Network](https://arxiv.org/abs/2108.05126)<br>:star:[code](https://github.com/fyviezhao/M3D-VTON)
    * [ZFlow: Gated Appearance Flow-based Virtual Try-on with 3D Priors](https://arxiv.org/abs/2109.07001)
    * [Dressing in Order: Recurrent Person Image Generation for Pose Transfer, Virtual Try-On and Outfit Editing](https://arxiv.org/abs/2104.07021)
    * [FashionMirror: Co-Attention Feature-Remapping Virtual Try-On With Sequential Template Poses](https://openaccess.thecvf.com/content/ICCV2021/papers/Chen_FashionMirror_Co-Attention_Feature-Remapping_Virtual_Try-On_With_Sequential_Template_Poses_ICCV_2021_paper.pdf)
    * [Structure-transformed Texture-enhanced Network for Person Image Synthesis](https://openaccess.thecvf.com/content/ICCV2021/papers/Xu_Structure-Transformed_Texture-Enhanced_Network_for_Person_Image_Synthesis_ICCV_2021_paper.pdf)
* SLAM
  * [On the Limits of Pseudo Ground Truth in Visual Camera Re-localisation](https://arxiv.org/abs/2109.00524)<br>:star:[code](https://github.com/tsattler/visloc_pseudo_gt_limitations/)
  * [Transfusion: A Novel SLAM Method Focused on Transparent Objects](https://openaccess.thecvf.com/content/ICCV2021/papers/Zhu_Transfusion_A_Novel_SLAM_Method_Focused_on_Transparent_Objects_ICCV_2021_paper.pdf)
  * [iMAP: Implicit Mapping and Positioning in Real-Time](https://arxiv.org/abs/2103.12352)
  * [Learning To Bundle-Adjust: A Graph Network Approach to Faster Optimization of Bundle Adjustment for Vehicular SLAM](https://openaccess.thecvf.com/content/ICCV2021/papers/Tanaka_Learning_To_Bundle-Adjust_A_Graph_Network_Approach_to_Faster_Optimization_ICCV_2021_paper.pdf)
  * [R-SLAM: Optimizing Eye Tracking From Rolling Shutter Video of the Retina](https://openaccess.thecvf.com/content/ICCV2021/papers/Shenoy_R-SLAM_Optimizing_Eye_Tracking_From_Rolling_Shutter_Video_of_the_ICCV_2021_paper.pdf)
  * Place Recognition
    * [Attentional Pyramid Pooling of Salient Visual Residuals for Place Recognition](https://openaccess.thecvf.com/content/ICCV2021/papers/Peng_Attentional_Pyramid_Pooling_of_Salient_Visual_Residuals_for_Place_Recognition_ICCV_2021_paper.pdf)
    * [Pyramid Point Cloud Transformer for Large-Scale Place Recognition](https://openaccess.thecvf.com/content/ICCV2021/papers/Hui_Pyramid_Point_Cloud_Transformer_for_Large-Scale_Place_Recognition_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/fpthink/PPT-Net)

<a name="35"/>

## 35.Quantization/Pruning/Knowledge Distillation/Model Compression(é‡åŒ–ã€å‰ªæã€è’¸é¦ã€æ¨¡å‹å‹ç¼©/æ‰©å±•ä¸ä¼˜åŒ–)
* çŸ¥è¯†è’¸é¦
  * [Distilling Holistic Knowledge with Graph Neural Networks](https://arxiv.org/abs/2108.05507)<br>:star:[code](https://github.com/wyc-ruiker/HKD)
  * [Lipschitz Continuity Guided Knowledge Distillation](https://arxiv.org/abs/2108.12905)<br>:star:[code](https://github.com/42Shawn/LONDON/tree/master)
  * [Densely Guided Knowledge Distillation Using Multiple Teacher Assistants](https://arxiv.org/abs/2009.08825)<br>:star:[code](https://github.com/wonchulSon/DGKD)
  * [Revisiting Adversarial Robustness Distillation: Robust Soft Labels Make Student Better](https://arxiv.org/abs/2108.07969)<br>:star:[code](https://github.com/zibojia/RSLAD)
  * [Compressing Visual-linguistic Model via Knowledge Distillation](https://arxiv.org/abs/2104.02096)
  * [Self-Knowledge Distillation With Progressive Refinement of Targets](https://arxiv.org/abs/2006.12000)<br>:star:[code](https://github.com/lgcnsai/PS-KD-Pytorch):tv:[video](https://drive.google.com/file/d/1QxqSbzn-egdYI13IYn3W4dmIvm_Iw4ku/view)
  * [Student Customized Knowledge Distillation: Bridging the Gap Between Student and Teacher](https://openaccess.thecvf.com/content/ICCV2021/papers/Zhu_Student_Customized_Knowledge_Distillation_Bridging_the_Gap_Between_Student_and_ICCV_2021_paper.pdf)
  * [Channel-Wise Knowledge Distillation for Dense Prediction](https://arxiv.org/abs/2011.13256)<br>:star:[code](https://github.com/irfanICMLL/TorchDistiller/tree/main/SemSeg-distill)
  * [Exploring Inter-Channel Correlation for Diversity-Preserved Knowledge Distillation](https://openaccess.thecvf.com/content/ICCV2021/papers/Liu_Exploring_Inter-Channel_Correlation_for_Diversity-Preserved_Knowledge_Distillation_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/ADLab-AutoDrive/ICKD)
* é‡åŒ–
  * [Distance-aware Quantization](https://arxiv.org/abs/2108.06983)<br>:star:[code](https://github.com/cvlab-yonsei/DAQ):house:[project](https://cvlab.yonsei.ac.kr/projects/DAQ/) 
  * [Dynamic Network Quantization for Efficient Video Inference](https://arxiv.org/abs/2108.10394)<br>:star:[code](https://github.com/sunxm2357/VideoIQ):house:[project](https://cs-people.bu.edu/sunxm/VideoIQ/project.html)
  * [Cluster-Promoting Quantization with Bit-Drop for Minimizing Network Quantization Loss](https://arxiv.org/abs/2109.02100)
  * [Improving Low-Precision Network Quantization via Bin Regularization](https://openaccess.thecvf.com/content/ICCV2021/papers/Han_Improving_Low-Precision_Network_Quantization_via_Bin_Regularization_ICCV_2021_paper.pdf)
  * [Towards Mixed-Precision Quantization of Neural Networks via Constrained Optimization](https://openaccess.thecvf.com/content/ICCV2021/papers/Chen_Towards_Mixed-Precision_Quantization_of_Neural_Networks_via_Constrained_Optimization_ICCV_2021_paper.pdf)
  * [Integer-arithmetic-only Certified Robustness for Quantized Neural Networks](https://arxiv.org/abs/2108.09413)
  * [RMSMP: A Novel Deep Neural Network Quantization Framework With Row-Wise Mixed Schemes and Multiple Precisions](https://openaccess.thecvf.com/content/ICCV2021/papers/Chang_RMSMP_A_Novel_Deep_Neural_Network_Quantization_Framework_With_Row-Wise_ICCV_2021_paper.pdf)
  * [Improving Neural Network Efficiency via Post-Training Quantization With Adaptive Floating-Point](https://openaccess.thecvf.com/content/ICCV2021/papers/Liu_Improving_Neural_Network_Efficiency_via_Post-Training_Quantization_With_Adaptive_Floating-Point_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/MXHX7199/ICCV_2021_AFP)
  * [Once Quantization-Aware Training: High Performance Extremely Low-bit Architecture Search](https://arxiv.org/abs/2010.04354)<br>:star:[code](https://github.com/LaVieEnRoseSMZ/OQA)
* æ¨¡å‹å‹ç¼©
  * [GDP: Stabilized Neural Network Pruning via Gates with Differentiable Polarization](https://arxiv.org/abs/2109.02220)
  * [Exploration and Estimation for Model Compression](https://openaccess.thecvf.com/content/ICCV2021/papers/Zhang_Exploration_and_Estimation_for_Model_Compression_ICCV_2021_paper.pdf)
* å‰ªæ
  * [ResRep: Lossless CNN Pruning via Decoupling Remembering and Forgetting](https://arxiv.org/abs/2007.03260)<br>:star:[code](https://github.com/DingXiaoH/ResRep)
  * [Auto Graph Encoder-Decoder for Neural Network Pruning](https://openaccess.thecvf.com/content/ICCV2021/papers/Yu_Auto_Graph_Encoder-Decoder_for_Neural_Network_Pruning_ICCV_2021_paper.pdf)

<a name="34"/>

## 34.Super-Resolution(è¶…åˆ†è¾¨ç‡)
* ISR
  * [Mutual Affine Network for Spatially Variant Kernel Estimation in Blind Image Super-Resolution](https://arxiv.org/abs/2108.05302)<br>:star:[code](https://github.com/JingyunLiang/MANet)
  * [Hierarchical Conditional Flow: A Unified Framework for Image Super-Resolution and Image Rescaling](https://arxiv.org/abs/2108.05301)<br>:star:[code](https://github.com/JingyunLiang/HCFlow)
  * [Deep Reparametrization of Multi-Frame Super-Resolution and Denoising](https://arxiv.org/abs/2108.08286)<br>:open_mouth:oral
  * [Dual-Camera Super-Resolution with Aligned Attention Modules](https://arxiv.org/abs/2109.01349)<br>:star:[code](https://github.com/Tengfei-Wang/DualCameraSR):house:[project](https://tengfei-wang.github.io/Dual-Camera-SR/index.html):tv:[video](https://www.youtube.com/watch?v=5TiUfAcNvuw)
  * [Attention-Based Multi-Reference Learning for Image Super-Resolution](https://arxiv.org/abs/2108.13697)<br>:star:[code](https://github.com/marcopesavento/AMRSR):house:[project](https://marcopesavento.github.io/AMRSR/)
  * [Learning a Single Network for Scale-Arbitrary Super-Resolution](https://arxiv.org/abs/2004.03791)
  * [Fourier Space Losses for Efficient Perceptual Image Super-Resolution](https://arxiv.org/abs/2106.00783)<br>:star:[code](https://github.com/dariofuoli)
  * [Achieving On-Mobile Real-Time Super-Resolution With Neural Architecture and Pruning Search](https://arxiv.org/abs/2108.08910)
  * [Designing a Practical Degradation Model for Deep Blind Image Super-Resolution](https://arxiv.org/abs/2103.14006)<br>:star:[code](https://github.com/cszn/BSRGAN)
  * [Event Stream Super-Resolution via Spatiotemporal Constraint Learning](https://openaccess.thecvf.com/content/ICCV2021/papers/Li_Event_Stream_Super-Resolution_via_Spatiotemporal_Constraint_Learning_ICCV_2021_paper.pdf)
  * [Dynamic High-Pass Filtering and Multi-Spectral Attention for Image Super-Resolution](https://openaccess.thecvf.com/content/ICCV2021/papers/Magid_Dynamic_High-Pass_Filtering_and_Multi-Spectral_Attention_for_Image_Super-Resolution_ICCV_2021_paper.pdf)
  * [Super-Resolving Cross-Domain Face Miniatures by Peeking at One-Shot Exemplar](https://arxiv.org/abs/2103.08863)
  * [Context Reasoning Attention Network for Image Super-Resolution](https://openaccess.thecvf.com/content/ICCV2021/papers/Zhang_Context_Reasoning_Attention_Network_for_Image_Super-Resolution_ICCV_2021_paper.pdf)
  * [EvIntSR-Net: Event Guided Multiple Latent Frames Reconstruction and Super-Resolution](https://openaccess.thecvf.com/content/ICCV2021/papers/Han_EvIntSR-Net_Event_Guided_Multiple_Latent_Frames_Reconstruction_and_Super-Resolution_ICCV_2021_paper.pdf)
  * [Super Resolve Dynamic Scene from Continuous Spike Streams](https://openaccess.thecvf.com/content/ICCV2021/papers/Zhao_Super_Resolve_Dynamic_Scene_From_Continuous_Spike_Streams_ICCV_2021_paper.pdf)
  * [Deep Blind Video Super-Resolution](https://openaccess.thecvf.com/content/ICCV2021/papers/Pan_Deep_Blind_Video_Super-Resolution_ICCV_2021_paper.pdf)
  * [Benchmarking Ultra-High-Definition Image Super-Resolution](https://openaccess.thecvf.com/content/ICCV2021/papers/Zhang_Benchmarking_Ultra-High-Definition_Image_Super-Resolution_ICCV_2021_paper.pdf)
  * [Lucas-Kanade Reloaded: End-to-End Super-Resolution From Raw Image Bursts](https://arxiv.org/abs/2104.06191)
  * [Unsupervised Real-World Super-Resolution: A Domain Adaptation Perspective](https://openaccess.thecvf.com/content/ICCV2021/papers/Wang_Unsupervised_Real-World_Super-Resolution_A_Domain_Adaptation_Perspective_ICCV_2021_paper.pdf)
  * [Real-World Video Super-Resolution: A Benchmark Dataset and a Decomposition Based Learning Scheme](https://openaccess.thecvf.com/content/ICCV2021/papers/Yang_Real-World_Video_Super-Resolution_A_Benchmark_Dataset_and_a_Decomposition_Based_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/IanYeung/RealVSR)
* VSR
  * [Omniscient Video Super-Resolution](https://arxiv.org/abs/2103.15683)<br>:star:[code](https://github.com/psychopa4/OVSR)
  * [COMISR: Compression-Informed Video Super-Resolution](https://arxiv.org/abs/2105.01237)<br>:star:[code](https://github.com/google-research/google-research/tree/master/comisr)
  * [Learning Frequency-Aware Dynamic Network for Efficient Super-Resolution](https://arxiv.org/abs/2103.08357)
  * [Efficient Video Compression via Content-Adaptive Super-Resolution](https://arxiv.org/abs/2104.02322)<br>:star:[code](https://github.com/AdaptiveVC/SRVC)

<a name="33"/>

## 33.Remote Sensing Images(é¥æ„Ÿå½±åƒ)
* [SUNet: Symmetric Undistortion Network for Rolling Shutter Correction](https://arxiv.org/abs/2108.04775)
* [Change is Everywhere: Single-Temporal Supervised Object Change Detection in Remote Sensing Imagery](https://arxiv.org/abs/2108.07002)<br>:star:[code](https://github.com/Z-Zheng/ChangeStar)
* å«æ˜Ÿå›¾åƒå…¨æ™¯è§†é¢‘åˆæˆ
  * [Sat2Vid: Street-view Panoramic Video Synthesis from a Single Satellite Image](https://arxiv.org/abs/2012.06628)
* åŸºäºå«æ˜Ÿå½±åƒçš„äº¤é€šäº‹æ•…æ£€æµ‹
  * [Inferring High-Resolution Traffic Accident Risk Maps Based on Satellite Imagery and GPS Trajectories](https://openaccess.thecvf.com/content/ICCV2021/papers/He_Inferring_High-Resolution_Traffic_Accident_Risk_Maps_Based_on_Satellite_Imagery_ICCV_2021_paper.pdf)
* é¥æ„Ÿæ•°æ®
  * [Seasonal Contrast: Unsupervised Pre-Training From Uncurated Remote Sensing Data](https://openaccess.thecvf.com/content/ICCV2021/papers/Manas_Seasonal_Contrast_Unsupervised_Pre-Training_From_Uncurated_Remote_Sensing_Data_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/ElementAI/seasonal-contrast)
  * [Dynamic Cross Feature Fusion for Remote Sensing Pansharpening](https://openaccess.thecvf.com/content/ICCV2021/papers/Wu_Dynamic_Cross_Feature_Fusion_for_Remote_Sensing_Pansharpening_ICCV_2021_paper.pdf)
* åˆ†å‰²
  * [Self-Mutating Network for Domain Adaptive Segmentation in Aerial Images](https://openaccess.thecvf.com/content/ICCV2021/papers/Lee_Self-Mutating_Network_for_Domain_Adaptive_Segmentation_in_Aerial_Images_ICCV_2021_paper.pdf)
  * å«æ˜Ÿå›¾åƒçš„å…¨æ™¯åˆ†å‰²
    * [Panoptic Segmentation of Satellite Image Time Series With Convolutional Temporal Attention Networks](https://arxiv.org/abs/2107.07933)<br>:star:[code](https://github.com/VSainteuf/utae-paps):sunflower:[PASTIS dataset](https://github.com/VSainteuf/pastis-benchmark)
* ä¸‰ç»´é‡å»º     
  * [3D Building Reconstruction from Monocular Remote Sensing Images](https://openaccess.thecvf.com/content/ICCV2021/papers/Li_3D_Building_Reconstruction_From_Monocular_Remote_Sensing_Images_ICCV_2021_paper.pdf)<br>:house:[project](https://liweijia.github.io/projects/building_3d/)

<a name="32"/>

## 32.è¯­éŸ³
* [The Right to Talk: An Audio-Visual Transformer Approach](https://arxiv.org/abs/2108.03256)<br>:star:[code](https://github.com/uark-cviu/Right2Talk)
* [Image2Reverb: Cross-Modal Reverb Impulse Response Synthesis](https://arxiv.org/abs/2103.14201)<br>:star:[code](https://github.com/nikhilsinghmus/image2reverb):house:[project](https://web.media.mit.edu/~nsingh1/image2reverb/)
* éŸ³é¢‘åˆ†ç¦»
  * [Visual Scene Graphs for Audio Source Separation](https://arxiv.org/abs/2109.11955)
* éŸ³é¢‘-æ‰‹åŠ¿
  * [Audio2Gestures: Generating Diverse Gestures from Speech Audio with Conditional Variational Autoencoders](https://arxiv.org/abs/2108.06720)<br>:house:[project](https://openaccess.thecvf.com/content/ICCV2021/papers/Xiang_SnowflakeNet_Point_Cloud_Completion_by_Snowflake_Point_Deconvolution_With_Skip-Transformer_ICCV_2021_paper.pdf)
* Active Speaker Detection(ASDä¸»åŠ¨å¼æ‰¬å£°å™¨æ£€æµ‹)
  * [How To Design a Three-Stage Architecture for Audio-Visual Active Speaker Detection in the Wild](https://arxiv.org/abs/2106.03932)<br>:star:[code](https://github.com/okankop/ASDNet)
  * [MAAS: Multi-Modal Assignation for Active Speaker Detection](https://openaccess.thecvf.com/content/ICCV2021/papers/Alcazar_MAAS_Multi-Modal_Assignation_for_Active_Speaker_Detection_ICCV_2021_paper.pdf)
* ä»äººè„¸è§†é¢‘ä¸­é‡æ–°æ”¶é›†éŸ³é¢‘
  * [Multi-Modality Associative Bridging Through Memory: Speech Sound Recollected From Face Video](https://openaccess.thecvf.com/content/ICCV2021/papers/Kim_Multi-Modality_Associative_Bridging_Through_Memory_Speech_Sound_Recollected_From_Face_ICCV_2021_paper.pdf)
* è§†å¬æºå®šä½
  * [Localize to Binauralize: Audio Spatialization From Visual Sound Source Localization](https://openaccess.thecvf.com/content/ICCV2021/papers/Rachavarapu_Localize_to_Binauralize_Audio_Spatialization_From_Visual_Sound_Source_Localization_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/KranthiKumarR/Localize-to-Binauralize):tv:[video](https://drive.google.com/drive/folders/1a5BV0U3RaQJS5wXyR7pzIAPMKOsGQz_q)
* è§†å¬æºåˆ†ç¦»
  * [Move2Hear: Active Audio-Visual Source Separation](https://openaccess.thecvf.com/content/ICCV2021/papers/Majumder_Move2Hear_Active_Audio-Visual_Source_Separation_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/SAGNIKMJR/move2hear-active-AV-separation):house:[project](http://vision.cs.utexas.edu/projects/move2hear/)
* è§†å¬å¹³é¢å›¾é‡å»º
  * [Audio-Visual Floorplan Reconstruction](https://openaccess.thecvf.com/content/ICCV2021/papers/Purushwalkam_Audio-Visual_Floorplan_Reconstruction_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/senthilps8/avmap):house:[project](http://www.cs.cmu.edu/~spurushw/publication/avmap/):tv:[video](https://youtu.be/wRslVfd1hOI)

<a name="31"/>

## 31.Style Transfer(é£æ ¼è¿ç§»)
* [AdaAttN: Revisit Attention Mechanism in Arbitrary Neural Style Transfer](https://arxiv.org/abs/2108.03647)<br>:star:[code](https://github.com/Huage001/AdaAttN)
* [Domain-Aware Universal Style Transfer](https://arxiv.org/abs/2108.04441)<br>:star:[code](https://github.com/Kibeom-Hong/Domain-Aware-Style-Transfer)
* [Diverse Image Style Transfer via Invertible Cross-Space Mapping](https://openaccess.thecvf.com/content/ICCV2021/papers/Chen_Diverse_Image_Style_Transfer_via_Invertible_Cross-Space_Mapping_ICCV_2021_paper.pdf)
* [StyleFormer: Real-Time Arbitrary Style Transfer via Parametric Style Composition](https://openaccess.thecvf.com/content/ICCV2021/papers/Wu_StyleFormer_Real-Time_Arbitrary_Style_Transfer_via_Parametric_Style_Composition_ICCV_2021_paper.pdf)
* [Manifold Alignment for Semantically Aligned Style Transfer](https://arxiv.org/abs/2005.10777)<br>:star:[code](https://github.com/NJUHuoJing/MAST)

<a name="30"/>

## 30.Image Generation/synthesis(å›¾åƒç”Ÿæˆ/åˆæˆ)
* [ILVR: Conditioning Method for Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2108.02938)<br>:open_mouth:oral
* [Image Synthesis via Semantic Composition](https://arxiv.org/abs/2109.07053)<br>:star:[code](https://github.com/dvlab-research/SCGAN):house:[project](https://shepnerd.github.io/scg/)
* [Image Synthesis From Layout With Locality-Aware Mask Adaption](https://openaccess.thecvf.com/content/ICCV2021/papers/Li_Image_Synthesis_From_Layout_With_Locality-Aware_Mask_Adaption_ICCV_2021_paper.pdf)
* å›¾åƒèåˆ
  * [DTMNet: A Discrete Tchebichef Moments-Based Deep Neural Network for Multi-Focus Image Fusion](https://openaccess.thecvf.com/content/ICCV2021/papers/Xiao_DTMNet_A_Discrete_Tchebichef_Moments-Based_Deep_Neural_Network_for_Multi-Focus_ICCV_2021_paper.pdf)

<a name="29"/>

## 29.Image Retrieval(å›¾åƒæ£€ç´¢)
* [DOLG: Single-Stage Image Retrieval with Deep Orthogonal Fusion of Local and Global Features](https://arxiv.org/abs/2108.02927)<br>:star:[code](https://github.com/feymanpriv/DOLG)
* [Image Retrieval on Real-life Images with Pre-trained Vision-and-Language Models](https://arxiv.org/abs/2108.04024)<br>:star:[code](https://github.com/Cuberick-Orion/CIRR):house:[project](https://cuberick-orion.github.io/CIRR/)
* [Self-supervised Product Quantization for Deep Unsupervised Image Retrieval](https://arxiv.org/abs/2109.02244)<br>:star:[code](https://github.com/youngkyunJang/SPQ)
* [Instance-Level Image Retrieval Using Reranking Transformers](https://arxiv.org/abs/2103.12236)<br>:star:[code](https://github.com/uvavision/RerankingTransformer)
* [Learning Attribute-Driven Disentangled Representations for Interactive Fashion Retrieval](https://openaccess.thecvf.com/content/ICCV2021/papers/Hou_Learning_Attribute-Driven_Disentangled_Representations_for_Interactive_Fashion_Retrieval_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/amzn/fashion-attribute-disentanglement)
* [Telling the What While Pointing to the Where: Multimodal Queries for Image Retrieval](https://openaccess.thecvf.com/content/ICCV2021/papers/Changpinyo_Telling_the_What_While_Pointing_to_the_Where_Multimodal_Queries_ICCV_2021_paper.pdf)
* [Frozen in Time: A Joint Video and Image Encoder for End-to-End Retrieval](https://arxiv.org/abs/2104.00650)
* [Learning Deep Local Features With Multiple Dynamic Attentions for Large-Scale Image Retrieval](https://openaccess.thecvf.com/content/ICCV2021/papers/Wu_Learning_Deep_Local_Features_With_Multiple_Dynamic_Attentions_for_Large-Scale_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/CHANWH/MDA)
* [Bayesian Triplet Loss: Uncertainty Quantification in Image Retrieval](https://openaccess.thecvf.com/content/ICCV2021/papers/Warburg_Bayesian_Triplet_Loss_Uncertainty_Quantification_in_Image_Retrieval_ICCV_2021_paper.pdf)
* è·¨åŸŸæ£€ç´¢
  * [Universal Cross-Domain Retrieval: Generalizing Across Classes and Domains](https://arxiv.org/abs/2108.08356)
* Visual Geolocalization
  * [Viewpoint Invariant Dense Matching for Visual Geolocalization](https://arxiv.org/abs/2109.09827)<br>:star:[code](https://github.com/gmberton/geo_warp)
* è·¨æ¨¡æ€æ£€ç´¢
  * [Ask&Confirm: Active Detail Enriching for Cross-Modal Retrieval With Partial Query](https://openaccess.thecvf.com/content/ICCV2021/papers/Cai_AskConfirm_Active_Detail_Enriching_for_Cross-Modal_Retrieval_With_Partial_Query_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/CuthbertCai/Ask-Confirm)
  * [Product1M: Towards Weakly Supervised Instance-Level Product Retrieval via Cross-modal Pretraining](https://arxiv.org/abs/2107.14572)<br>:star:[code](https://github.com/zhanxlin/Product1M)
  * [Wasserstein Coupled Graph Learning for Cross-Modal Retrieval](https://openaccess.thecvf.com/content/ICCV2021/papers/Wang_Wasserstein_Coupled_Graph_Learning_for_Cross-Modal_Retrieval_ICCV_2021_paper.pdf)
  * [Adversarial Attack on Deep Cross-Modal Hamming Retrieval](https://openaccess.thecvf.com/content/ICCV2021/papers/Li_Adversarial_Attack_on_Deep_Cross-Modal_Hamming_Retrieval_ICCV_2021_paper.pdf)
* æ–‡æœ¬-è§†é¢‘æ£€ç´¢
  * [TEACHTEXT: CrossModal Generalized Distillation for Text-Video Retrieval](https://arxiv.org/abs/2104.08271)<br>:house:[project](https://www.robots.ox.ac.uk/~vgg/research/teachtext/)
* è§†é¢‘- æ–‡æœ¬æ£€ç´¢
  * [HiT: Hierarchical Transformer With Momentum Contrast for Video-Text Retrieval](https://arxiv.org/abs/2103.15049)
* image-based 3D shape retrieval 
  * [Single Image 3D Shape Retrieval via Cross-Modal Instance and Category Contrastive Learning](https://openaccess.thecvf.com/content/ICCV2021/papers/Lin_Single_Image_3D_Shape_Retrieval_via_Cross-Modal_Instance_and_Category_ICCV_2021_paper.pdf)
* è¿‘é‚»æœç´¢
  * [Product Quantizer Aware Inverted Index for Scalable Nearest Neighbor Search](https://openaccess.thecvf.com/content/ICCV2021/papers/Noh_Product_Quantizer_Aware_Inverted_Index_for_Scalable_Nearest_Neighbor_Search_ICCV_2021_paper.pdf)


<a name="28"/>

## 28.Contrastive Learning(å¯¹æ¯”å­¦ä¹ )
* [Improving Contrastive Learning by Visualizing Feature Transformation](https://arxiv.org/abs/2108.02982)<br>:open_mouth:oral:star:[code](https://github.com/DTennant/CL-Visualizing-Feature-Transformation)
* [TACo: Token-aware Cascade Contrastive Learning for Video-Text Alignment](https://arxiv.org/abs/2108.09980)<br>:newspaper:è§£è¯»:[ICCV2021-TOCo-å¾®è½¯&CMUæå‡ºTokenæ„ŸçŸ¥çš„çº§è”å¯¹æ¯”å­¦ä¹ æ–¹æ³•ï¼Œåœ¨è§†é¢‘æ–‡æœ¬å¯¹é½ä»»åŠ¡ä¸Šâ€œåŠæ‰“â€å…¶ä»–SOTAæ–¹æ³•](https://mp.weixin.qq.com/s/sNwvYL1qsgyVrRe3-QmzhA)
* [A Broad Study on the Transferability of Visual Representations With Contrastive Learning](https://arxiv.org/abs/2103.13517)<br>:star:[code](https://github.com/asrafulashiq/transfer_broad)
* [Vi2CLR: Video and Image for Visual Contrastive Learning of Representation](https://openaccess.thecvf.com/content/ICCV2021/papers/Diba_Vi2CLR_Video_and_Image_for_Visual_Contrastive_Learning_of_Representation_ICCV_2021_paper.pdf)
* [LatentCLR: A Contrastive Learning Approach for Unsupervised Discovery of Interpretable Directions](https://arxiv.org/abs/2104.00820)<br>:star:[code](https://github.com/catlab-team/latentclr)
* [CrossCLR: Cross-Modal Contrastive Learning for Multi-Modal Video Representations](https://arxiv.org/abs/2109.14910)
* [Social NCE: Contrastive Learning of Socially-Aware Motion Representations](https://arxiv.org/abs/2012.11717)(https://github.com/vita-epfl/social-nce):tv:[video](https://www.youtube.com/watch?v=s1khZWWiQfA)
* [With a Little Help From My Friends: Nearest-Neighbor Contrastive Learning of Visual Representations](https://arxiv.org/abs/2104.14548)
* [Contrastive Learning of Image Representations With Cross-Video Cycle-Consistency](https://arxiv.org/abs/2105.06463)<br>:house:[project](https://happywu.github.io/cycle_contrast_video/)
* [Weakly Supervised Contrastive Learning](https://arxiv.org/abs/2110.04770)

<a name="27"/>

## 27.Multi-label image recognition(å¤šæ ‡ç­¾å›¾åƒè¯†åˆ«)
* [Residual Attention: A Simple but Effective Method for Multi-Label Recognition](https://arxiv.org/abs/2108.02456)<br>:star:[code](https://github.com/Kevinz-code/CSRA)
* [Transformer-based Dual Relation Graph for Multi-label Image Recognition](https://arxiv.org/abs/2110.04722)

<a name="26"/>

## 26.Image Processing(å›¾åƒå¤„ç†)
* [Aligning Latent and Image Spaces to Connect the Unconnectable](https://arxiv.org/abs/2104.06954)<br>:star:[code](https://github.com/universome/alis):house:[project](https://universome.github.io/alis)
* å›¾åƒå½¢çŠ¶æ“çºµ
  * [Image Shape Manipulation from a Single Augmented Training Sample](https://arxiv.org/abs/2109.06151)<br>:open_mouth:oral:star:[code](https://github.com/eliahuhorwitz/DeepSIM):house:[project](http://www.vision.huji.ac.il/deepsim/)
* è¾¹ç¼˜æ£€æµ‹
  * [RINDNet: Edge Detection for Discontinuity in Reflectance, Illumination, Normal and Depth](https://arxiv.org/abs/2108.00616)<br>:open_mouth:oral:star:[code](https://github.com/MengyangPu/RINDNet)
  * [Pixel Difference Networks for Efficient Edge Detection](https://arxiv.org/abs/2108.07009)<br>:star:[code](https://github.com/zhuoinoulu/pidinet)
* å›¾åƒè¯†åˆ«
  * [MicroNet: Improving Image Recognition with Extremely Low FLOPs](https://arxiv.org/abs/2108.05894)<br>:star:[code](https://github.com/liyunsheng13/micronet)
* å›¾åƒå»æ¨¡ç³Š
  * [Rethinking Coarse-to-Fine Approach in Single Image Deblurring](https://arxiv.org/abs/2108.05054)<br>:star:[code](https://github.com/chosj95/MIMO-UNet)
  * [Single Image Defocus Deblurring Using Kernel-Sharing Parallel Atrous Convolutions](https://arxiv.org/abs/2108.09108)
  * [Defocus Map Estimation and Deblurring From a Single Dual-Pixel Image](https://openaccess.thecvf.com/content/ICCV2021/papers/Xin_Defocus_Map_Estimation_and_Deblurring_From_a_Single_Dual-Pixel_Image_ICCV_2021_paper.pdf)
  * [Motion Deblurring with Real Events](https://arxiv.org/abs/2109.13695)
  * [Pyramid Architecture Search for Real-Time Image Deblurring](https://openaccess.thecvf.com/content/ICCV2021/papers/Hu_Pyramid_Architecture_Search_for_Real-Time_Image_Deblurring_ICCV_2021_paper.pdf)
  * è¿åŠ¨å»æ¨¡ç³Š
    * [Perceptual Variousness Motion Deblurring With Light Global Context Refinement](https://openaccess.thecvf.com/content/ICCV2021/papers/Li_Perceptual_Variousness_Motion_Deblurring_With_Light_Global_Context_Refinement_ICCV_2021_paper.pdf)
* è§†é¢‘å»æ¨¡ç³Š
  * [Bringing Events Into Video Deblurring With Non-Consecutively Blurry Frames](https://openaccess.thecvf.com/content/ICCV2021/papers/Shang_Bringing_Events_Into_Video_Deblurring_With_Non-Consecutively_Blurry_Frames_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/shangwei5/D2Net)
* Image quality assessment(å›¾åƒè´¨é‡è¯„ä¼°IQA)
  * [MUSIQ: Multi-scale Image Quality Transformer](https://arxiv.org/abs/2108.05997)<br>:star:[code](https://github.com/google-research/google-research/tree/master/musiq)
* Image Harmonization
  * [SSH: A Self-Supervised Framework for Image Harmonization](https://arxiv.org/abs/2108.06805)<br>:star:[code](https://github.com/VITA-Group/SSHarmonization)
  * [Learning Conditional Knowledge Distillation for Degraded-Reference Image Quality Assessment](https://arxiv.org/abs/2108.07948)<br>:star:[code](https://github.com/researchmm/CKDN)
* å»é˜´å½±
  * [CANet: A Context-Aware Network for Shadow Removal](https://arxiv.org/abs/2108.09894)<br>:star:[code](https://github.com/Zipei-Chen/CANet)
  * [DC-ShadowNet: Single-Image Hard and Soft Shadow Removal Using Unsupervised Domain-Classifier Guided Network](https://openaccess.thecvf.com/content/ICCV2021/papers/Jin_DC-ShadowNet_Single-Image_Hard_and_Soft_Shadow_Removal_Using_Unsupervised_Domain-Classifier_ICCV_2021_paper.pdf)
* å»å™ª
  * [Rethinking Deep Image Prior for Denoising](https://arxiv.org/abs/2108.12841)<br>:star:[code](https://github.com/gistvision/DIP-denosing)
  * [Rethinking Noise Synthesis and Modeling in Raw Denoising](https://arxiv.org/abs/2110.04756)<br>:star:[code](https://github.com/zhangyi-3/noise-synthesis)
  * [C2N: Practical Generative Noise Modeling for Real-World Denoising](https://openaccess.thecvf.com/content/ICCV2021/papers/Jang_C2N_Practical_Generative_Noise_Modeling_for_Real-World_Denoising_ICCV_2021_paper.pdf)
  * [The Benefit of Distraction: Denoising Camera-Based Physiological Measurements Using Inverse Attention](https://openaccess.thecvf.com/content/ICCV2021/papers/Nowara_The_Benefit_of_Distraction_Denoising_Camera-Based_Physiological_Measurements_Using_Inverse_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/ewanowara/benefitofdistraction)
  * [Hyperspectral Image Denoising with Realistic Data](https://openaccess.thecvf.com/content/ICCV2021/papers/Zhang_Hyperspectral_Image_Denoising_With_Realistic_Data_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/ColinTaoZhang/HSIDwRD)
  * [End-to-End Unsupervised Document Image Blind Denoising](https://openaccess.thecvf.com/content/ICCV2021/papers/Gangeh_End-to-End_Unsupervised_Document_Image_Blind_Denoising_ICCV_2021_paper.pdf)
  * [Cross-Patch Graph Convolutional Network for Image Denoising](https://openaccess.thecvf.com/content/ICCV2021/papers/Li_Cross-Patch_Graph_Convolutional_Network_for_Image_Denoising_ICCV_2021_paper.pdf)
  * è§†é¢‘å»å™ª
    * [Patch Craft: Video Denoising by Deep Modeling and Patch Matching](http://arxiv.org/abs/2103.13767)
* å›¾åƒç€è‰²
  * [Towards Vivid and Diverse Image Colorization with Generative Color Prior](https://arxiv.org/abs/2108.08826)
  * [Deep Edge-Aware Interactive Colorization Against Color-Bleeding Effects](https://arxiv.org/abs/2107.01619)<br>:open_mouth:oral:house:[project](https://eungyeupkim.github.io/edge-enhancing-colorization/)
* å›¾åƒå¢å¼º
  * [Real-time Image Enhancer via Learnable Spatial-aware 3D Lookup Tables](https://arxiv.org/abs/2108.08697)
  * [Adaptive Unfolding Total Variation Network for Low-Light Image Enhancement](https://arxiv.org/abs/2110.00984)<br>:star:[code](https://github.com/CharlieZCJ/UTVNet)
  * [Representative Color Transform for Image Enhancement](https://openaccess.thecvf.com/content/ICCV2021/papers/Kim_Representative_Color_Transform_for_Image_Enhancement_ICCV_2021_paper.pdf)
  * [STAR: A Structure-Aware Lightweight Transformer for Real-Time Image Enhancement](https://openaccess.thecvf.com/content/ICCV2021/papers/Zhang_STAR_A_Structure-Aware_Lightweight_Transformer_for_Real-Time_Image_Enhancement_ICCV_2021_paper.pdf)
  * [Deep Symmetric Network for Underexposed Image Enhancement With Recurrent Attentional Learning](https://openaccess.thecvf.com/content/ICCV2021/papers/Zhao_Deep_Symmetric_Network_for_Underexposed_Image_Enhancement_With_Recurrent_Attentional_ICCV_2021_paper.pdf)<br>:star:[code](https://www.shaopinglu.net/proj-iccv21/ImageEnhancement.html):house:[project](https://github.com/lin-zhao-resoLve/Deep-Symmetric-Network-Enhancement)
  * [StarEnhancer: Learning Real-Time and Style-Aware Image Enhancement](https://arxiv.org/abs/2107.12898)
* å›¾åƒæ¢å¤
  * [Spatially-Adaptive Image Restoration using Distortion-Guided Networks](https://arxiv.org/abs/2108.08617)<br>:star:[code](https://github.com/human-analysis/spatially-adaptive-image-restoration)
  * [Dynamic Attentive Graph Learning for Image Restoration](https://arxiv.org/abs/2109.06620)<br>:star:[code](https://github.com/jianzhangcs/DAGL)
  * [Self-Supervised Cryo-Electron Tomography Volumetric Image Restoration From Single Noisy Volume With Sparsity Constraint](https://openaccess.thecvf.com/content/ICCV2021/papers/Yang_Self-Supervised_Cryo-Electron_Tomography_Volumetric_Image_Restoration_From_Single_Noisy_Volume_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/icthrm/SC-Net)
  * [Searching for Controllable Image Restoration Networks](https://arxiv.org/abs/2012.11225)<br>:star:[code](https://github.com/ghimhw)
* å›¾åƒå‹ç¼©
  * [Variable-Rate Deep Image Compression through Spatially-Adaptive Feature Transform](https://arxiv.org/abs/2108.09551)<br>:star:[code](https://github.com/micmic123/QmapCompression)
  * [Neural Image Compression via Attentional Multi-Scale Back Projection and Frequency Decomposition](https://openaccess.thecvf.com/content/ICCV2021/papers/Gao_Neural_Image_Compression_via_Attentional_Multi-Scale_Back_Projection_and_Frequency_ICCV_2021_paper.pdf)
* å›¾åƒä¿®å¤
  * [Image Inpainting via Conditional Texture and Structure Dual Generation](https://arxiv.org/abs/2108.09760)<br>:star:[code](https://github.com/Xiefan-Guo/CTSDG)
  * [CR-Fill: Generative Image Inpainting With Auxiliary Contextual Reconstruction](https://openaccess.thecvf.com/content/ICCV2021/papers/Zeng_CR-Fill_Generative_Image_Inpainting_With_Auxiliary_Contextual_Reconstruction_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/zengxianyu/crfill)
  * [Parallel Multi-Resolution Fusion Network for Image Inpainting](https://openaccess.thecvf.com/content/ICCV2021/papers/Wang_Parallel_Multi-Resolution_Fusion_Network_for_Image_Inpainting_ICCV_2021_paper.pdf)
  * [Painting from Part](https://openaccess.thecvf.com/content/ICCV2021/papers/Guo_Painting_From_Part_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/zhenglab/partpainting)
  * [WaveFill: A Wavelet-Based Generation Network for Image Inpainting](https://arxiv.org/abs/2107.11027)
  * [Distillation-Guided Image Inpainting](https://openaccess.thecvf.com/content/ICCV2021/papers/Suin_Distillation-Guided_Image_Inpainting_ICCV_2021_paper.pdf)
  * [Learning a Sketch Tensor Space for Image Inpainting of Man-made Scenes](https://arxiv.org/abs/2103.15087)<br>:star:[code](https://github.com/ewrfcas/MST_inpainting):house:[project](https://ewrfcas.github.io/MST_inpainting/)
* Image extrapolation
  * [SemIE: Semantically-aware Image Extrapolation](https://arxiv.org/abs/2108.13702)<br>:house:[project](https://semie-iccv.github.io/)
* Reversible Image Conversion
  * [IICNet: A Generic Framework for Reversible Image Conversion](https://arxiv.org/abs/2109.04242)<br>:star:[code](https://github.com/felixcheng97/IICNet)
* ä¼ªå½±å»é™¤
  * [Towards Flexible Blind JPEG Artifacts Removal](https://arxiv.org/abs/2109.14573)<br>:star:[code](https://github.com/jiaxi-jiang/FBCNN)
  * [Learning Dual Priors for JPEG Compression Artifacts Removal](https://openaccess.thecvf.com/content/ICCV2021/papers/Fu_Learning_Dual_Priors_for_JPEG_Compression_Artifacts_Removal_ICCV_2021_paper.pdf)
  * [Let's See Clearly: Contaminant Artifact Removal for Moving Cameras](https://openaccess.thecvf.com/content/ICCV2021/papers/Li_Lets_See_Clearly_Contaminant_Artifact_Removal_for_Moving_Cameras_ICCV_2021_paper.pdf)
* De-rendering
  * [De-rendering Stylized Texts](https://arxiv.org/abs/2110.01890)<br>:star:[code](https://github.com/CyberAgentAILab/derendering-text):house:[project](https://cyberagentailab.github.io/derendering-text/)
* å»é™¤å…‰æ™•
  * [Light Source Guided Single-Image Flare Removal From Unpaired Data](https://openaccess.thecvf.com/content/ICCV2021/papers/Qiao_Light_Source_Guided_Single-Image_Flare_Removal_From_Unpaired_Data_ICCV_2021_paper.pdf)
* å…¨æ™¯å›¾æ‹¼æ¥
  * [Minimal Solutions for Panoramic Stitching Given Gravity Prior](https://arxiv.org/abs/2012.00465)
* Flare Removal
  * [How to Train Neural Networks for Flare Removal](https://arxiv.org/abs/2011.12485)<br>:house:[project](https://yichengwu.github.io/flare-removal/):tv:[video](https://www.youtube.com/watch?v=eAXhcDjWoZ0)
* å›¾åƒè£å‰ª
  * [TransView: Inside, Outside, and Across the Cropping View Boundaries](https://openaccess.thecvf.com/content/ICCV2021/papers/Pan_TransView_Inside_Outside_and_Across_the_Cropping_View_Boundaries_ICCV_2021_paper.pdf)
  * [Dissecting Image Crops](https://arxiv.org/abs/2011.11831)<br>:star:[code](https://github.com/basilevh/dissecting-image-crops)
* å»åå°„
  * [Location-Aware Single Image Reflection Removal](https://arxiv.org/abs/2012.07131)<br>:star:[code](https://github.com/zdlarr/Location-aware-SIRR)
  * [V-DESIRR: Very Fast Deep Embedded Single Image Reflection Removal](https://openaccess.thecvf.com/content/ICCV2021/papers/Prasad_V-DESIRR_Very_Fast_Deep_Embedded_Single_Image_Reflection_Removal_ICCV_2021_paper.pdf)<br>:star:[code](https://www.github.com/ee19d005/vdesirr)
* å»é›¨
  * [Improving De-Raining Generalization via Neural Reorganization](https://openaccess.thecvf.com/content/ICCV2021/papers/Xiao_Improving_De-Raining_Generalization_via_Neural_Reorganization_ICCV_2021_paper.pdf)
  * [Unpaired Learning for Deep Image Deraining with Rain Direction Regularize](https://openaccess.thecvf.com/content/ICCV2021/papers/Liu_Unpaired_Learning_for_Deep_Image_Deraining_With_Rain_Direction_Regularizer_ICCV_2021_paper.pdf)<br>:house:[project](https://lewisyangliu.github.io/projects/UDRDR/)
  * [Structure-Preserving Deraining With Residue Channel Prior Guidance](https://arxiv.org/abs/2108.09079)<br>:star:[code](https://github.com/Joyies/SPDNet)
* å›¾åƒå¤±çœŸå»é™¤
  * [Unsupervised Non-Rigid Image Distortion Removal via Grid Deformation](https://openaccess.thecvf.com/content/ICCV2021/papers/Li_Unsupervised_Non-Rigid_Image_Distortion_Removal_via_Grid_Deformation_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/Nianyi-Li/unsupervised-NDIR):tv:[video](https://www.youtube.com/watch?v=aeJkb5u0Cb8)
* æ¶ˆé™¤æ°´ä¸‹å›¾åƒçš„æŠ˜å°„å¤±çœŸ
  * [Learning To Remove Refractive Distortions From Underwater Images](https://openaccess.thecvf.com/content/ICCV2021/papers/Thapa_Learning_To_Remove_Refractive_Distortions_From_Underwater_Images_ICCV_2021_paper.pdf)
* å›¾åƒè¡¥å…¨
  * [High-Fidelity Pluralistic Image Completion With Transformers](https://arxiv.org/abs/2103.14031)<br>:star:[code](https://github.com/raywzy/ICT):house:[project](http://raywzy.com/ICT/)
* Image Decomposition
  * [Unsupervised Layered Image Decomposition into Object Prototypes](https://arxiv.org/abs/2104.14575)
* å¤±çœŸçŸ«æ­£
  * [Towards Complete Scene and Regular Shape for Distortion Rectification by Curve-Aware Extrapolation](https://openaccess.thecvf.com/content/ICCV2021/papers/Liao_Towards_Complete_Scene_and_Regular_Shape_for_Distortion_Rectification_by_ICCV_2021_paper.pdf)
* HDR
  * [Unpaired Learning for High Dynamic Range Image Tone Mapping](https://openaccess.thecvf.com/content/ICCV2021/papers/Vinker_Unpaired_Learning_for_High_Dynamic_Range_Image_Tone_Mapping_ICCV_2021_paper.pdf)
  * è¶…é«˜æ¸…å›¾åƒHDRé‡å»º
    * [Ultra-High-Definition Image HDR Reconstruction via Collaborative Bilateral Learning](https://openaccess.thecvf.com/content/ICCV2021/papers/Zheng_Ultra-High-Definition_Image_HDR_Reconstruction_via_Collaborative_Bilateral_Learning_ICCV_2021_paper.pdf)
* å›¾åƒå»é›ª
  * [ALL Snow Removed: Single Image Desnowing Algorithm Using Hierarchical Dual-Tree Complex Wavelet Representation and Contradict Channel Loss](https://openaccess.thecvf.com/content/ICCV2021/papers/Chen_ALL_Snow_Removed_Single_Image_Desnowing_Algorithm_Using_Hierarchical_Dual-Tree_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/weitingchen83/ICCV2021-Single-Image-Desnowing-HDCWNet)
* Image Harmonization
  * [Image Harmonization With Transformer](https://openaccess.thecvf.com/content/ICCV2021/papers/Guo_Image_Harmonization_With_Transformer_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/zhenglab/HarmonyTransformer)
* å›¾åƒç¼–è¾‘
  * [Language-Guided Global Image Editing via Cross-Modal Cyclic Mechanism](https://openaccess.thecvf.com/content/ICCV2021/papers/Jiang_Language-Guided_Global_Image_Editing_via_Cross-Modal_Cyclic_Mechanism_ICCV_2021_paper.pdf)
* image hiding(å›¾åƒéšè—)
  * [HiNet: Deep Image Hiding by Invertible Network](https://openaccess.thecvf.com/content/ICCV2021/papers/Jing_HiNet_Deep_Image_Hiding_by_Invertible_Network_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/TomTomTommi/HiNet)

<a name="25"/>

## 25.Medical Image(åŒ»å­¦å½±åƒ)
* [Equivariant Imaging: Learning Beyond the Range Space](https://arxiv.org/abs/2103.14756)<br>:open_mouth:oral:star:[code](https://github.com/edongdongchen/EI)
* [Deep Survival Analysis With Longitudinal X-Rays for COVID-19](https://openaccess.thecvf.com/content/ICCV2021/papers/Shu_Deep_Survival_Analysis_With_Longitudinal_X-Rays_for_COVID-19_ICCV_2021_paper.pdf)
* åŒ»å­¦å›¾åƒåˆ†å‰²
  * [Recurrent Mask Refinement for Few-Shot Medical Image Segmentation](https://arxiv.org/abs/2108.00622)<br>:star:[code](https://github.com/uci-cbcl/RP-Net)
  * [Graph-BAS3Net: Boundary-Aware Semi-Supervised Segmentation Network With Bilateral Graph Convolution](https://openaccess.thecvf.com/content/ICCV2021/papers/Huang_Graph-BAS3Net_Boundary-Aware_Semi-Supervised_Segmentation_Network_With_Bilateral_Graph_Convolution_ICCV_2021_paper.pdf)åŒ»å­¦å›¾åƒåˆ†å‰²
  * ç—…å˜åˆ†å‰²
    * [T-AutoML: Automated Machine Learning for Lesion Segmentation Using Transformers in 3D Medical Imaging](https://openaccess.thecvf.com/content/ICCV2021/papers/Yang_T-AutoML_Automated_Machine_Learning_for_Lesion_Segmentation_Using_Transformers_in_ICCV_2021_paper.pdf)
  * æ¯è‚‰åˆ†å‰²
    * [Collaborative and Adversarial Learning of Focused and Dispersive Representations for Semi-Supervised Polyp Segmentation](https://openaccess.thecvf.com/content/ICCV2021/papers/Wu_Collaborative_and_Adversarial_Learning_of_Focused_and_Dispersive_Representations_for_ICCV_2021_paper.pdf)
  * è¡€ç®¡åˆ†å‰²
    * [Self-Supervised Vessel Segmentation via Adversarial Learning](https://github.com/AISIGSJTU/SSVS)
  * è„‘è‚¿ç˜¤åˆ†å‰²
    * [RFNet: Region-Aware Fusion Network for Incomplete Multi-Modal Brain Tumor Segmentation](https://openaccess.thecvf.com/content/ICCV2021/papers/Ding_RFNet_Region-Aware_Fusion_Network_for_Incomplete_Multi-Modal_Brain_Tumor_Segmentation_ICCV_2021_paper.pdf)
* ç—…ç†å­¦å›¾åƒè¡¨ç¤º
  * [A QuadTree Image Representation for Computational Pathology](https://arxiv.org/abs/2108.10873)
* åŒ»å­¦å›¾åƒåˆ†æ
  * [Preservational Learning Improves Self-supervised Medical Image Models by Reconstructing Diverse Contexts](https://arxiv.org/abs/2109.04379)<br>:star:[code](https://github.com/Luchixiang/PCRL)
* åŒ»å­¦å›¾åƒå»å™ª
  * [Eformer: Edge Enhancement based Transformer for Medical Image Denoising](https://arxiv.org/abs/2109.08044)
* è§†é¢‘ç¿»è¯‘
  * [Long-Term Temporally Consistent Unpaired Video Translation From Simulated Surgical 3D Data](https://arxiv.org/abs/2103.17204)<br>:star:[code](https://gitlab.com/nct_tso_public/surgical-video-sim2real):house:[project](http://opencas.dkfz.de/video-sim2real/)
* ç—…ç†å­¦å›¾åƒæ ¸æ£€æµ‹åˆ†å‰²
  * [Mutual-Complementing Framework for Nuclei Detection and Segmentation in Pathology Image](https://openaccess.thecvf.com/content/ICCV2021/papers/Feng_Mutual-Complementing_Framework_for_Nuclei_Detection_and_Segmentation_in_Pathology_Image_ICCV_2021_paper.pdf)
* åŒ»å­¦æŠ¥å‘Šç”Ÿæˆ
  * [Visual-Textual Attentive Semantic Consistency for Medical Report Generation](https://openaccess.thecvf.com/content/ICCV2021/papers/Zhou_Visual-Textual_Attentive_Semantic_Consistency_for_Medical_Report_Generation_ICCV_2021_paper.pdf)
* CT
  * [3DeepCT: Learning Volumetric Scattering Tomography of Clouds](https://openaccess.thecvf.com/content/ICCV2021/papers/Sde-Chen_3DeepCT_Learning_Volumetric_Scattering_Tomography_of_Clouds_ICCV_2021_paper.pdf)
  * [IntraTomo: Self-Supervised Learning-Based Tomography via Sinogram Synthesis and Prediction](https://openaccess.thecvf.com/content/ICCV2021/papers/Zang_IntraTomo_Self-Supervised_Learning-Based_Tomography_via_Sinogram_Synthesis_and_Prediction_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/vccimaging/IntraTomo)
  * CTé‡å»º
    * [Dynamic CT Reconstruction From Limited Views With Implicit Neural Representations and Parametric Motion Fields](https://arxiv.org/abs/2104.11745)<br>:star:[code](https://github.com/awreed/DynamicCTReconstruction)
* åŒ»å­¦å›¾åƒè¯†åˆ«
  * [GLoRIA: A Multimodal Global-Local Representation Learning Framework for Label-Efficient Medical Image Recognition](https://openaccess.thecvf.com/content/ICCV2021/papers/Huang_GLoRIA_A_Multimodal_Global-Local_Representation_Learning_Framework_for_Label-Efficient_Medical_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/marshuang80/gloria)
* åŒ»å­¦å›¾åƒåˆ†ç±»
  * [Big Self-Supervised Models Advance Medical Image Classification](https://arxiv.org/abs/2101.05224)
  * [Large-Scale Robust Deep AUC Maximization: A New Surrogate Loss and Empirical Studies on Medical Image Classification](https://arxiv.org/abs/2012.03173)<br>:star:[code](https://github.com/Optimization-AI/LibAUC)

<a name="24"/>

## 24.Face(äººè„¸)
* [VariTex: Variational Neural Face Textures](https://arxiv.org/abs/2104.05988)<br>:star:[code](https://github.com/mcbuehler/VariTex):house:[project](https://mcbuehler.github.io/VariTex/):tv:[video](https://www.youtube.com/watch?v=6-GFHcLkbik)
* äººè„¸é€ å‡æ£€æµ‹
  * [OpenForensics: Large-Scale Challenging Dataset For Multi-Face Forgery Detection And Segmentation In-The-Wild](https://arxiv.org/abs/2107.14480)<br>:house:[project](https://sites.google.com/view/ltnghia/research/openforensics)
  * [Exploring Temporal Coherence for More General Video Face Forgery Detection](https://arxiv.org/abs/2108.06693)
* äººè„¸åˆæˆ
  * [Disentangled Lifespan Face Synthesis](https://arxiv.org/abs/2108.02874)<br>:star:[code](https://github.com/SenHe/DLFS):house:[project](https://senhe.github.io/projects/iccv_2021_lifespan_face/):tv:[video](https://youtu.be/uklX03ns0m0)
* äººè„¸è¯†åˆ«                                                                                      
  * [PASS: Protected Attribute Suppression System for Mitigating Bias in Face Recognition](https://arxiv.org/abs/2108.03764)
  * [SynFace: Face Recognition with Synthetic Data](https://arxiv.org/abs/2108.07960)
  * [Adaptive Label Noise Cleaning With Meta-Supervision for Deep Face Recognition](https://openaccess.thecvf.com/content/ICCV2021/papers/Zhang_Adaptive_Label_Noise_Cleaning_With_Meta-Supervision_for_Deep_Face_Recognition_ICCV_2021_paper.pdf)
  * [Disentangled Representation for Age-Invariant Face Recognition: A Mutual Information Minimization Perspective](https://openaccess.thecvf.com/content/ICCV2021/papers/Hou_Disentangled_Representation_for_Age-Invariant_Face_Recognition_A_Mutual_Information_Minimization_ICCV_2021_paper.pdf)
  * [Teacher-Student Adversarial Depth Hallucination To Improve Face Recognition](https://openaccess.thecvf.com/content/ICCV2021/papers/Uppal_Teacher-Student_Adversarial_Depth_Hallucination_To_Improve_Face_Recognition_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/hardik-uppal/teacher-student-gan)
  * [DAM: Discrepancy Alignment Metric for Face Recognition](https://openaccess.thecvf.com/content/ICCV2021/papers/Liu_DAM_Discrepancy_Alignment_Metric_for_Face_Recognition_ICCV_2021_paper.pdf)
  * â€œå»â€è¯†åˆ«
    * [Personalized and Invertible Face De-Identification by Disentangled Identity Information Manipulation](https://openaccess.thecvf.com/content/ICCV2021/papers/Cao_Personalized_and_Invertible_Face_De-Identification_by_Disentangled_Identity_Information_Manipulation_ICCV_2021_paper.pdf)
* Face perceptioné¢éƒ¨æ„ŸçŸ¥
 * [Learning Facial Representations from the Cycle-consistency of Face](https://arxiv.org/abs/2108.03427)<br>:star:[code](https://github.com/JiaRenChang/FaceCycle)
* è¯´è¯äººè„¸ç”Ÿæˆ
  * [FACIAL: Synthesizing Dynamic Talking Face with Implicit Attribute Learning](https://arxiv.org/abs/2108.07938)
* è¯´è¯å¤´åˆæˆ
  * [AD-NeRF: Audio Driven Neural Radiance Fields for Talking Head Synthesis](https://openaccess.thecvf.com/content/ICCV2021/papers/Guo_AD-NeRF_Audio_Driven_Neural_Radiance_Fields_for_Talking_Head_Synthesis_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/YudongGuo/AD-NeRF)
  * [Learned Spatial Representations for Few-Shot Talking-Head Synthesis](https://arxiv.org/abs/2104.14557)<br>:star:[code](https://github.com/MoustafaMeshry/lsr):house:[project](http://www.cs.umd.edu/~mmeshry/projects/lsr/)
* äººè„¸è¡¨æƒ…è¯†åˆ«
  * [Understanding and Mitigating Annotation Bias in Facial Expression Recognition](https://arxiv.org/abs/2108.08504)
  * [TransFER: Learning Relation-aware Facial Expression Representations with Transformers](https://arxiv.org/abs/2108.11116)
* äººè„¸å‘ˆç°æ”»å‡»æ£€æµ‹
  * [Detection and Continual Learning of Novel Face Presentation Attacks](https://arxiv.org/abs/2108.12081)<br>:star:[code](https://github.com/mrostami1366)
* äººè„¸ç¼–è¾‘
  * [Talk-to-Edit: Fine-Grained Facial Editing via Dialog](https://arxiv.org/abs/2109.04425)<br>:star:[code](https://github.com/yumingj/Talk-to-Edit):house:[project](https://www.mmlab-ntu.com/project/talkedit/)<br>:newspaper:è§£è¯»:[ICCV2021 | å—æ´‹ç†å·¥å¤§å­¦ã€æ¸¯ä¸­å¤§æå‡ºTalk-to-Editï¼Œå¯¹è¯å®ç°é«˜ç»†ç²’åº¦äººè„¸ç¼–è¾‘](https://mp.weixin.qq.com/s/48FsUqsppXaXUu-QMUIhCQ)
  * [A Latent Transformer for Disentangled Face Editing in Images and Videos](https://arxiv.org/abs/2106.11895)<br>:star:[code](https://github.com/InterDigitalInc/latent-transformer)
* äººè„¸å¯¹é½
  * [ADNet: Leveraging Error-Bias Towards Normal Direction in Face Alignment](https://arxiv.org/abs/2109.05721) 
* äººè„¸å›¾åƒé‡å»º
  * [Focal Frequency Loss for Image Reconstruction and Synthesis](https://arxiv.org/abs/2012.12821)<br>:star:[code](https://github.com/EndlessSora/focal-frequency-loss):house:[project](https://www.mmlab-ntu.com/project/ffl/index.html):tv:[video](https://www.youtube.com/watch?v=RNTnDtKvcpc)
  * [Towards High Fidelity Monocular Face Reconstruction With Rich Reflectance Using Self-Supervised Learning and Ray Tracing](https://arxiv.org/abs/2103.15432)
  * [Neural Photofit: Gaze-Based Mental Image Reconstruction](https://arxiv.org/abs/2108.07524)<br>:house:[project](https://perceptualui.org/publications/strohm21_iccv/)
* 3Däººè„¸é‡å»º
  * [Topologically Consistent Multi-View Face Inference Using Volumetric Sampling](https://arxiv.org/abs/2110.02948)<br>:star:[code](https://tianyeli.github.io/tofu) 
  * [Self-Supervised 3D Face Reconstruction via Conditional Estimation](https://arxiv.org/abs/2110.04800)
* ä¸‰ç»´äººè„¸åŠ¨ç”»
  * [MeshTalk: 3D Face Animation From Speech Using Cross-Modality Disentanglement](https://openaccess.thecvf.com/content/ICCV2021/papers/Richard_MeshTalk_3D_Face_Animation_From_Speech_Using_Cross-Modality_Disentanglement_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/facebookresearch/meshtalk):tv:[video](https://research.fb.com/wp-content/uploads/2021/04/mesh_talk.mp4)
* Remote Photoplethysmography (rPPGè¿œç¨‹å…‰ç”µå®¹ç§¯æè®°æœ¯)
  * [The Way to My Heart Is Through Contrastive Learning: Remote Photoplethysmography From Unlabelled Video](https://openaccess.thecvf.com/content/ICCV2021/papers/Gideon_The_Way_to_My_Heart_Is_Through_Contrastive_Learning_Remote_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/ToyotaResearchInstitute/RemotePPG)
* äººè„¸åŠ å¯†
  * [Towards Face Encryption by Generating Adversarial Identity Masks](https://arxiv.org/abs/2003.06814)<br>:star:[code](https://github.com/ShawnXYang/TIP-IM)
* Deepfakeæ£€æµ‹
  * [Learning Self-Consistency for Deepfake Detection](https://arxiv.org/abs/2012.09311)<br>:open_mouth:oral
  * [Joint Audio-Visual Deepfake Detection](https://openaccess.thecvf.com/content/ICCV2021/papers/Zhou_Joint_Audio-Visual_Deepfake_Detection_ICCV_2021_paper.pdf)
  * [Artificial Fingerprinting for Generative Models: Rooting Deepfake Attribution in Training Data](https://arxiv.org/abs/2007.08457)<br>:open_mouth:oral
* äººè„¸çº¹ç†è¡¥å…¨
  * [Learning High-Fidelity Face Texture Completion Without Complete Face Texture](https://openaccess.thecvf.com/content/ICCV2021/papers/Kim_Learning_High-Fidelity_Face_Texture_Completion_Without_Complete_Face_Texture_ICCV_2021_paper.pdf)
* é¢éƒ¨åŠ¨ä½œå•å…ƒæ£€æµ‹
  * [PIAP-DF: Pixel-Interested and Anti Person-Specific Facial Action Unit Detection Net With Discrete Feedback Learning](https://openaccess.thecvf.com/content/ICCV2021/papers/Tang_PIAP-DF_Pixel-Interested_and_Anti_Person-Specific_Facial_Action_Unit_Detection_Net_ICCV_2021_paper.pdf)
* äººè„¸åˆ†æ
  * [Fake It Till You Make It: Face analysis in the wild using synthetic data alone](https://arxiv.org/abs/2109.15102)<br>:house:[project](https://microsoft.github.io/FaceSynthetics/):tv:[video](https://youtu.be/wlOMpQe8luQ)
* 3Då¤´é‡å»º
  * [H3D-Net: Few-Shot High-Fidelity 3D Head Reconstruction](https://openaccess.thecvf.com/content/ICCV2021/papers/Ramon_H3D-Net_Few-Shot_High-Fidelity_3D_Head_Reconstruction_ICCV_2021_paper.pdf)
* äººè„¸å…³é”®ç‚¹æ£€æµ‹
  * [Improving Robustness of Facial Landmark Detection by Defending Against Adversarial Attacks](https://openaccess.thecvf.com/content/ICCV2021/papers/Zhu_Improving_Robustness_of_Facial_Landmark_Detection_by_Defending_Against_Adversarial_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/zhuccly/SAAT)
* äººè„¸å›¾åƒæ£€ç´¢
  * [Face Image Retrieval with Attribute Manipulation](https://openaccess.thecvf.com/content/ICCV2021/papers/Zaeemzadeh_Face_Image_Retrieval_With_Attribute_Manipulation_ICCV_2021_paper.pdf)

<a name="23"/>

## 23.Gaze Estimation(è§†çº¿ä¼°è®¡)
* [Generalizing Gaze Estimation with Outlier-guided Collaborative Adaptation](https://arxiv.org/abs/2107.13780)<br>:star:[code](https://github.com/DreamtaleCore/PnP-GA)
* è§†çº¿è·Ÿè¸ª
  * [Looking Here or There? Gaze Following in 360-Degree Images](https://openaccess.thecvf.com/content/ICCV2021/papers/Li_Looking_Here_or_There_Gaze_Following_in_360-Degree_Images_ICCV_2021_paper.pdf)
* è§†ç‚¹ä¼°è®¡
  * [ViewNet: Unsupervised Viewpoint Estimation From Conditional Generation](https://openaccess.thecvf.com/content/ICCV2021/papers/Mariotti_ViewNet_Unsupervised_Viewpoint_Estimation_From_Conditional_Generation_ICCV_2021_paper.pdf)

<a name="22"/>

## 22.GAN
* [Sketch Your Own GAN](https://arxiv.org/abs/2108.02774)<br>:star:[code](https://github.com/peterwang512/GANSketching):house:[project](https://peterwang512.github.io/GANSketching/)
* [Online Multi-Granularity Distillation for GAN Compression](https://arxiv.org/abs/2108.06908)<br>:star:[code](https://github.com/bytedance/OMGD)
* [Dual Projection Generative Adversarial Networks for Conditional Image Generation](https://arxiv.org/abs/2108.09016)<br>:star:[code](https://github.com/phymhan/P2GAN)
* [InSeGAN: A Generative Approach to Segmenting Identical Instances in Depth Images](https://arxiv.org/abs/2108.13865)
* [ReStyle: A Residual-Based StyleGAN Encoder via Iterative Refinement](https://arxiv.org/abs/2104.02699)<br>:star:[code](https://github.com/yuval-alaluf/restyle-encoder):house:[project](https://yuval-alaluf.github.io/restyle-encoder/):tv:[video](https://youtu.be/6pGzLECSIWM)
* [WarpedGANSpace: Finding non-linear RBF paths in GAN latent space](https://arxiv.org/abs/2109.13357)<br>:star:[code](https://github.com/chi0tzp/WarpedGANSpace)
* [Toward a Visual Concept Vocabulary for GAN Latent Space](https://arxiv.org/abs/2110.04292)
* [Collaging Class-specific GANs for Semantic Image Synthesis](https://arxiv.org/abs/2110.04281)<br>:house:[project](https://yuheng-li.github.io/CollageGAN/)
* [Latent Transformations via NeuralODEs for GAN-Based Image Editing](https://openaccess.thecvf.com/content/ICCV2021/papers/Khrulkov_Latent_Transformations_via_NeuralODEs_for_GAN-Based_Image_Editing_ICCV_2021_paper.pdf)
* [Reality Transform Adversarial Generators for Image Splicing Forgery Detection and Localization](https://openaccess.thecvf.com/content/ICCV2021/papers/Bi_Reality_Transform_Adversarial_Generators_for_Image_Splicing_Forgery_Detection_and_ICCV_2021_paper.pdf)
* [GAN-Control: Explicitly Controllable GANs](https://openaccess.thecvf.com/content/ICCV2021/papers/Shoshan_GAN-Control_Explicitly_Controllable_GANs_ICCV_2021_paper.pdf)(https://alonshoshan10.github.io/gan_control/)<br>:house:[project](https://alonshoshan10.github.io/gan_control/)
* [Omni-GAN: On the Secrets of cGANs and Beyond](https://openaccess.thecvf.com/content/ICCV2021/papers/Zhou_Omni-GAN_On_the_Secrets_of_cGANs_and_Beyond_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/PeterouZh/Omni-GAN-PyTorch)
* [Unsupervised Image Generation with Infinite Generative Adversarial Networks](https://arxiv.org/abs/2108.07975)<br>:star:[code](https://github.com/yinghdb/MICGANs)
* [DAE-GAN: Dynamic Aspect-Aware GAN for Text-to-Image Synthesis](https://openaccess.thecvf.com/content/ICCV2021/papers/Ruan_DAE-GAN_Dynamic_Aspect-Aware_GAN_for_Text-to-Image_Synthesis_ICCV_2021_paper.pdf)
* [Detail Me More: Improving GANâ€™s photo-realism of complex scenes](https://openaccess.thecvf.com/content/ICCV2021/papers/Gadde_Detail_Me_More_Improving_GANs_Photo-Realism_of_Complex_Scenes_ICCV_2021_paper.pdf)
* [Unsupervised Segmentation Incorporating Shape Prior via Generative Adversarial Networks](https://openaccess.thecvf.com/content/ICCV2021/papers/Kim_Unsupervised_Segmentation_Incorporating_Shape_Prior_via_Generative_Adversarial_Networks_ICCV_2021_paper.pdf)
* [DRB-GAN: A Dynamic ResBlock Generative Adversarial Network for Artistic Style Transfer](https://openaccess.thecvf.com/content/ICCV2021/papers/Xu_DRB-GAN_A_Dynamic_ResBlock_Generative_Adversarial_Network_for_Artistic_Style_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/xuwenju123/DRB-GAN)
* [Dual Contrastive Loss and Attention for GANs](https://arxiv.org/abs/2103.16748)
* [Semi-Supervised Single-Stage Controllable GANs for Conditional Fine-Grained Image Generation](https://openaccess.thecvf.com/content/ICCV2021/papers/Chen_Semi-Supervised_Single-Stage_Controllable_GANs_for_Conditional_Fine-Grained_Image_Generation_ICCV_2021_paper.pdf)
* [Gradient Normalization for Generative Adversarial Networks](https://arxiv.org/abs/2109.02235)<br>:star:[code](https://github.com/basiclab/GNGAN-PyTorch)
* [EigenGAN: Layer-Wise Eigen-Learning for GANs](https://arxiv.org/abs/2104.12476)<br>:star:[code](https://github.com/LynnHo/EigenGAN-Tensorflow)
* [Retrieve in Style: Unsupervised Facial Feature Transfer and Retrieval](https://arxiv.org/abs/2107.06256)<br>:star:[code](https://github.com/mchong6/RetrieveInStyle)
* [HeadGAN: One-shot Neural Head Synthesis and Editing](https://arxiv.org/abs/2012.08261)<br>:house:[project](https://michaildoukas.github.io/HeadGAN/):tv:[video](https://www.youtube.com/watch?v=Xo9IW3cMGTg)
* [Explaining in Style: Training a GAN To Explain a Classifier in StyleSpace](https://arxiv.org/abs/2104.13369)<br>:star:[code](https://github.com/google/explaining-in-style):house:[project](https://explaining-in-style.github.io/):tv:[video](https://youtu.be/wLk2eBdXH4M)
* [StyleCLIP: Text-Driven Manipulation of StyleGAN Imagery](https://arxiv.org/abs/2103.17249)<br>:open_mouth:oral:star:[code](https://github.com/orpatashnik/StyleCLIP):tv:[video](https://www.youtube.com/watch?v=PhR1gpXDu0w)
* [Towards Discovery and Attribution of Open-World GAN Generated Images](https://openaccess.thecvf.com/content/ICCV2021/papers/Girish_Towards_Discovery_and_Attribution_of_Open-World_GAN_Generated_Images_ICCV_2021_paper.pdf)
* [Diagonal Attention and Style-Based GAN for Content-Style Disentanglement in Image Generation and Translation](https://openaccess.thecvf.com/content/ICCV2021/papers/Kwon_Diagonal_Attention_and_Style-Based_GAN_for_Content-Style_Disentanglement_in_Image_ICCV_2021_paper.pdf)
* [Re-Aging GAN: Toward Personalized Face Age Transformation](https://openaccess.thecvf.com/content/ICCV2021/papers/Makhmudkhujaev_Re-Aging_GAN_Toward_Personalized_Face_Age_Transformation_ICCV_2021_paper.pdf)
* [When do GANs replicate? On the choice of dataset size](https://openaccess.thecvf.com/content/ICCV2021/papers/Feng_When_Do_GANs_Replicate_On_the_Choice_of_Dataset_Size_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/chenqiguo/GAN_replication)
* [LoFGAN: Fusing Local Representations for Few-shot Image Generation](https://openaccess.thecvf.com/content/ICCV2021/papers/Gu_LoFGAN_Fusing_Local_Representations_for_Few-Shot_Image_Generation_ICCV_2021_paper.pdf)
* [Multi-Class Multi-Instance Count Conditioned Adversarial Image Generation](https://arxiv.org/abs/2103.16795)<br>:star:[code](https://github.com/boschresearch/MCCGAN)
* [Generative Adversarial Registration for Improved Conditional Deformable Templates](https://arxiv.org/abs/2105.04349)<br>:star:[code](https://github.com/neel-dey/Atlas-GAN)
* [F-Drop&Match: GANs with a Dead Zone in the High-Frequency Domain](https://openaccess.thecvf.com/content/ICCV2021/papers/Yamaguchi_F-DropMatch_GANs_With_a_Dead_Zone_in_the_High-Frequency_Domain_ICCV_2021_paper.pdf)
* GAN inversion(GANé€†æ˜ å°„)
  * [From Continuity to Editability: Inverting GANs with Consecutive Images](https://arxiv.org/abs/2107.13812)<br>:star:[code](https://github.com/cnnlstm/InvertingGANs_with_ConsecutiveImgs)
  * [GAN Inversion for Out-of-Range Images with Geometric Transformations](https://arxiv.org/abs/2108.08998)<br>:house:[project](https://kkang831.github.io/publication/ICCV_2021_BDInvert/)
* å›¾åƒåˆ°å›¾åƒç¿»è¯‘
  * [Unaligned Image-to-Image Translation by Learning to Reweight](https://arxiv.org/abs/2109.11736)<br>:star:[code](https://github.com/Mid-Push/IrwGAN)
  * [Bridging the Gap between Label- and Reference-based Synthesis in Multi-attribute Image-to-Image Translation](https://arxiv.org/abs/2110.05055)<br>:star:[code](https://github.com/huangqiusheng/BridgeGAN)
  * [Instance-Wise Hard Negative Example Generation for Contrastive Learning in Unpaired Image-to-Image Translation](https://arxiv.org/abs/2108.04547)
  * [TransferI2I: Transfer Learning for Image-to-Image Translation from Small Datasets](https://arxiv.org/abs/2105.06219)<br>:star:[code](https://github.com/yaxingwang/TransferI2I)
  * [Rethinking the Truly Unsupervised Image-to-Image Translation](https://arxiv.org/abs/2006.06500)
  * [SPatchGAN: A Statistical Feature Based Discriminator for Unsupervised Image-to-Image Translation](https://arxiv.org/abs/2103.16219)
* Image translation
  * [Scaling-up Disentanglement for Image Translation](https://arxiv.org/abs/2103.14017)<br>:star:[code](https://github.com/avivga/overlord):house:[project](http://www.vision.huji.ac.il/overlord/)
  * [Harnessing the Conditioning Sensorium for Improved Image Translation](https://openaccess.thecvf.com/content/ICCV2021/papers/Nederhood_Harnessing_the_Conditioning_Sensorium_for_Improved_Image_Translation_ICCV_2021_paper.pdf)
  * [Frequency Domain Image Translation: More Photo-Realistic, Better Identity-Preserving](https://arxiv.org/abs/2011.13611)<br>:star:[code](https://github.com/mu-cai/frequency-domain-image-translation)
  * [Dual Transfer Learning for Event-based End-task Prediction via Pluggable Event to Image Translation](https://arxiv.org/abs/2109.01801)<br>:star:[code](https://github.com/addisonwang2013/DTL)
  * [Semantically Robust Unpaired Image Translation for Data with Unmatched Semantics Statistics](https://arxiv.org/abs/2012.04932)

<a name="21"/>

## 21.Active Learning(ä¸»åŠ¨å­¦ä¹ )
* [Semi-Supervised Active Learning with Temporal Output Discrepancy](https://arxiv.org/abs/2107.14153)<br>:star:[code](https://github.com/siyuhuang/TOD)
* [Influence Selection for Active Learning](https://arxiv.org/abs/2108.09331)
* [Active Domain Adaptation via Clustering Uncertainty-weighted Embeddings](https://arxiv.org/abs/2010.08666)<br>:star:[code](https://github.com/virajprabhu/CLUE)
* [Contrastive Coding for Active Learning under Class Distribution Mismatch](https://openaccess.thecvf.com/content/ICCV2021/papers/Du_Contrastive_Coding_for_Active_Learning_Under_Class_Distribution_Mismatch_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/RUC-DWBI-ML/CCAL)  

<a name="20"/>

## 20.Adversarial Learning(å¯¹æŠ—å­¦ä¹ )
* [Low Curvature Activations Reduce Overfitting in Adversarial Training](https://arxiv.org/abs/2102.07861)
* [Removing Adversarial Noise in Class Activation Feature Space](https://arxiv.org/abs/2104.09197)
* [Sample Efficient Detection and Classification of Adversarial Attacks via Self-Supervised Embeddings](https://arxiv.org/abs/2108.13797)
* [Invisible Backdoor Attack With Sample-Specific Triggers](https://arxiv.org/abs/2012.03816)<br>:star:[code](https://github.com/yuezunli/ISSBA)
* [Defending Against Universal Adversarial Patches by Clipping Feature Norms](https://openaccess.thecvf.com/content/ICCV2021/papers/Yu_Defending_Against_Universal_Adversarial_Patches_by_Clipping_Feature_Norms_ICCV_2021_paper.pdf)
* å¯¹æŠ—æ”»å‡»
  * [Feature Importance-aware Transferable Adversarial Attacks](https://arxiv.org/abs/2107.14185)<br>:star:[code](https://github.com/hcguoO0/FIA)
  * [TkML-AP: Adversarial Attacks to Top-k Multi-Label Learning](https://arxiv.org/abs/2108.00146)<br>:star:[code](https://github.com/discovershu/TKML-AP)
  * [Meta Gradient Adversarial Attack](https://arxiv.org/abs/2108.04204)
  * [AGKD-BML: Defense Against Adversarial Attack by Attention Guided Knowledge Distillation and Bi-directional Metric Learning](https://arxiv.org/abs/2108.06017)<br>:star:[code](https://github.com/hongw579/AGKD-BML)
  * [Exploiting Multi-Object Relationships for Detecting Adversarial Attacks in Complex Scenes](https://arxiv.org/abs/2108.08421)
  * [AdvDrop: Adversarial Attack to DNNs by Dropping Information](https://arxiv.org/abs/2108.09034)<br>:star:[code](https://github.com/RjDuan/AdvDrop)
  * [Adversarial Attacks Are Reversible With Natural Supervision](https://arxiv.org/abs/2103.14222)
  * [Attack As the Best Defense: Nullifying Image-to-Image Translation GANs via Limit-Aware Adversarial Attack](https://openaccess.thecvf.com/content/ICCV2021/papers/Yeh_Attack_As_the_Best_Defense_Nullifying_Image-to-Image_Translation_GANs_via_ICCV_2021_paper.pdf)
  * [Learnable Boundary Guided Adversarial Training](https://arxiv.org/abs/2011.11164)<br>:star:[code](https://github.com/dvlab-research/LBGAT)
  * [Augmented Lagrangian Adversarial Attacks](https://arxiv.org/abs/2011.11857)<br>:star:[code](https://github.com/jeromerony/augmented_lagrangian_adversarial_attacks)
  * [Meta-Attack: Class-Agnostic and Model-Agnostic Physical Adversarial Attack](https://openaccess.thecvf.com/content/ICCV2021/papers/Feng_Meta-Attack_Class-Agnostic_and_Model-Agnostic_Physical_Adversarial_Attack_ICCV_2021_paper.pdf)
  * [On Generating Transferable Targeted Perturbations](https://arxiv.org/abs/2103.14641)<br>:star:[code](https://github.com/Muzammal-Naseer/TTP)
  * [Admix: Enhancing the Transferability of Adversarial Attacks](https://arxiv.org/abs/2102.00436)<br>:star:[code](https://github.com/JHL-HUST/Admix)
  * [Consistency-Sensitivity Guided Ensemble Black-Box Adversarial Attacks in Low-Dimensional Spaces](https://openaccess.thecvf.com/content/ICCV2021/papers/Yuan_Consistency-Sensitivity_Guided_Ensemble_Black-Box_Adversarial_Attacks_in_Low-Dimensional_Spaces_ICCV_2021_paper.pdf)
  * [Adversarial Attacks On Multi-Agent Communication](https://arxiv.org/abs/2101.06560)
  * [Interpreting Attributions and Interactions of Adversarial Attacks](https://arxiv.org/abs/2108.06895)
  * [RDA: Robust Domain Adaptation via Fourier Adversarial Attacking](https://arxiv.org/abs/2106.02874)
* å¯¹æŠ—æ ·æœ¬
  * [Adversarial Example Detection Using Latent Neighborhood Graph](https://openaccess.thecvf.com/content/ICCV2021/papers/Abusnaina_Adversarial_Example_Detection_Using_Latent_Neighborhood_Graph_ICCV_2021_paper.pdf)
  * [On the Robustness of Vision Transformers to Adversarial Examples](https://arxiv.org/abs/2104.02610)
* é»‘ç›’
  * [Black-Box Detection of Backdoor Attacks With Limited Information and Data](https://arxiv.org/abs/2103.13127)
  * [Aha! Adaptive History-Driven Attack for Decision-Based Black-Box Models](https://openaccess.thecvf.com/content/ICCV2021/papers/Li_Aha_Adaptive_History-Driven_Attack_for_Decision-Based_Black-Box_Models_ICCV_2021_paper.pdf)
  * [Data-Free Universal Adversarial Perturbation and Black-Box Attack](https://openaccess.thecvf.com/content/ICCV2021/papers/Zhang_Data-Free_Universal_Adversarial_Perturbation_and_Black-Box_Attack_ICCV_2021_paper.pdf)

<a name="19"/>

## 19.Self-Driving Vehicles(è‡ªåŠ¨é©¾é©¶)
* [End-to-End Urban Driving by Imitating a Reinforcement Learning Coach](https://arxiv.org/abs/2108.08265)<br>:star:[code](https://github.com/zhejz/carla-roach)  
* [MultiSiam: Self-supervised Multi-instance Siamese Representation Learning for Autonomous Driving](https://arxiv.org/abs/2108.12178)<br>:star:[code](https://github.com/KaiChen1998/MultiSiam)
* [NEAT: Neural Attention Fields for End-to-End Autonomous Driving](https://arxiv.org/abs/2109.04456)<br>:star:[code](https://github.com/autonomousvision/neat)
* [Safety-aware Motion Prediction with Unseen Vehicles for Autonomous Driving](https://arxiv.org/abs/2109.01510)<br>:star:[code](https://github.com/xrenaa/Safety-Aware-Motion-Prediction)
* [Social-NCE: Contrastive Learning of Socially-aware Motion Representations](https://arxiv.org/abs/2012.11717)<br>:star:[code](https://github.com/vita-epfl/social-nce-crowdnav):tv:[video](https://www.youtube.com/watch?v=s1khZWWiQfA)
* [Learning To Drive From a World on Rails](https://arxiv.org/abs/2105.00636)<br>:open_mouth:oral:star:[code](https://github.com/dotchen/WorldOnRails):house:[project](https://dotchen.github.io/world_on_rails/)
* [DRIVE: Deep Reinforced Accident Anticipation With Visual Explanation](https://arxiv.org/abs/2107.10189)<br>:star:[code](https://github.com/Cogito2012/DRIVE):house:[project](https://www.rit.edu/actionlab/drive):tv:[video](https://youtu.be/e2K2wTorKOc)
* [LookOut: Diverse Multi-Future Prediction and Planning for Self-Driving](https://arxiv.org/abs/2101.06547)
* [Prediction by Anticipation: An Action-Conditional Prediction Method Based on Interaction Learning](https://arxiv.org/abs/2012.13478)<br>:star:[code](https://github.com/Atcold/pytorch-PPUU):tv:[video](https://www.youtube.com/watch?v=X2s7gy3wIYw)
* [TMCOSS: Thresholded Multi-Criteria Online Subset Selection for Data-Efficient Autonomous Driving](https://openaccess.thecvf.com/content/ICCV2021/papers/Das_TMCOSS_Thresholded_Multi-Criteria_Online_Subset_Selection_for_Data-Efficient_Autonomous_Driving_ICCV_2021_paper.pdf)
* [FIERY: Future Instance Prediction in Bird's-Eye View From Surround Monocular Cameras](https://openaccess.thecvf.com/content/ICCV2021/papers/Hu_FIERY_Future_Instance_Prediction_in_Birds-Eye_View_From_Surround_Monocular_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/wayveai/fiery)
* [On Exposing the Challenging Long Tail in Future Prediction of Traffic Actors](https://arxiv.org/abs/2103.12474)<br>:star:[code](https://github.com/lmb-freiburg/Contrastive-Future-Trajectory-Prediction)
* [MGNet: Monocular Geometric Scene Understanding for Autonomous Driving](https://openaccess.thecvf.com/content/ICCV2021/papers/Schon_MGNet_Monocular_Geometric_Scene_Understanding_for_Autonomous_Driving_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/markusschoen/MGNet):tv:[video](https://www.youtube.com/watch?v=GXdQNtVQYmY)
* Human trajectory prediction(äººä½“è½¨è¿¹é¢„æµ‹)
  * [Personalized Trajectory Prediction via Distribution Discrimination](https://arxiv.org/abs/2107.14204)<br>:star:[code](https://github.com/CHENGY12/DisDis)
  * [Human Trajectory Prediction via Counterfactual Analysis](https://arxiv.org/abs/2107.14202)<br>:star:[code](https://github.com/CHENGY12/CausalHTP)
  * [From Goals, Waypoints & Paths to Long Term Human Trajectory Forecasting](https://arxiv.org/abs/2012.01526)<br>:star:[code](https://github.com/HarshayuGirase/PECNet):house:[project](https://karttikeya.github.io/publication/ynet/):tv:[video](https://youtu.be/XCWCHwGlBgE)
* è½¨è¿¹é¢„æµ‹
  * [Unlimited Neighborhood Interaction for Heterogeneous Trajectory Prediction](https://arxiv.org/abs/2108.00238)
  * [LOKI: Long Term and Key Intentions for Trajectory Prediction](https://arxiv.org/abs/2108.08236)
  * [MG-GAN: A Multi-Generator Model Preventing Out-of-Distribution Samples in Pedestrian Trajectory Prediction](https://arxiv.org/abs/2108.09274)<br>:star:[code](https://github.com/selflein/MG-GAN)
  * [DenseTNT: End-to-end Trajectory Prediction from Dense Goal Sets](https://arxiv.org/abs/2108.09640)
  * [Where Are You Heading? Dynamic Trajectory Prediction With Expert Goal Examples](https://openaccess.thecvf.com/content/ICCV2021/papers/Zhao_Where_Are_You_Heading_Dynamic_Trajectory_Prediction_With_Expert_Goal_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/JoeHEZHAO/expert_traj)
  * [Three Steps to Multimodal Trajectory Prediction: Modality Clustering, Classification and Synthesis](https://arxiv.org/abs/2103.07854)
  * [Spatial-Temporal Consistency Network for Low-Latency Trajectory Forecasting](https://openaccess.thecvf.com/content/ICCV2021/papers/Li_Spatial-Temporal_Consistency_Network_for_Low-Latency_Trajectory_Forecasting_ICCV_2021_paper.pdf)
  * [Likelihood-Based Diverse Sampling for Trajectory Forecasting](https://arxiv.org/abs/2011.15084)<br>:star:[code](https://github.com/JasonMa2016/LDS)
* è¿åŠ¨é¢„æµ‹
  * [RAIN: Reinforced Hybrid Attention Inference Network for Motion Forecasting](https://arxiv.org/abs/2108.01316)<br>:house:[project](https://jiachenli94.github.io/publications/RAIN/)
  * [SLAMP: Stochastic Latent Appearance and Motion Prediction](https://arxiv.org/abs/2108.02760)<br>:house:[project](https://kuis-ai.github.io/slamp/)
  * [SlowFast Rolling-Unrolling LSTMs for Action Anticipation in Egocentric Videos](https://arxiv.org/abs/2109.00829)
* è‡ªåŠ¨å¯¼èˆª
  * [FOVEA: Foveated Image Magnification for Autonomous Navigation](https://arxiv.org/abs/2108.12102)<br>:house:[project](https://www.cs.cmu.edu/~mengtial/proj/fovea/)  
* äº¤é€šåœºæ™¯ç†è§£
  * [Structured Bird's-Eye-View Traffic Scene Understanding from Onboard Images](https://arxiv.org/abs/2110.01997)<br>:star:[code](https://github.com/ybarancan/STSU)
* è½¦è¾†è½¦ç‰Œè¯†åˆ«
  * è½¦è¾†é‡è¯†åˆ«
    * [Heterogeneous Relational Complement for Vehicle Re-identification](https://arxiv.org/abs/2109.07894)
    * [Self-Supervised Geometric Features Discovery via Interpretable Attention for Vehicle Re-Identification and Beyond](https://openaccess.thecvf.com/content/ICCV2021/papers/Li_Self-Supervised_Geometric_Features_Discovery_via_Interpretable_Attention_for_Vehicle_Re-Identification_ICCV_2021_paper.pdf)
 * è‡ªä¸»èµ›è½¦
   * [Learn-to-Race: A Multimodal Control Environment for Autonomous Racing](https://openaccess.thecvf.com/content/ICCV2021/papers/Herman_Learn-To-Race_A_Multimodal_Control_Environment_for_Autonomous_Racing_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/learn-to-race/l2r)
* é¢„æµ‹å¸æœºçš„è§†è§‰æ³¨æ„åŠ›
  * [MEDIRL: Predicting the Visual Attention of Drivers via Maximum Entropy Deep Inverse Reinforcement Learning](https://arxiv.org/abs/1912.07773)<br>:star:[code](https://github.com/soniabaee/MEDIRL-EyeCar)
* å§¿åŠ¿é¢„æµ‹
  * [Space-Time-Separable Graph Convolutional Network for Pose Forecasting](https://openaccess.thecvf.com/content/ICCV2021/papers/Sofianos_Space-Time-Separable_Graph_Convolutional_Network_for_Pose_Forecasting_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/FraLuca/STSGCN):tv:[video](https://www.youtube.com/watch?v=tQIygtJrrtk)
* è½¦è¾†è·Ÿè¸ª
  * [Track Without Appearance: Learn Box and Tracklet Embedding With Local and Global Motion Patterns for Vehicle Tracking](https://arxiv.org/abs/2108.06029)
* å¯¹ä»»æ„ç›¸æœºè§†è§’çš„è½¦è¾†è¿›è¡Œæ£€æµ‹åˆ†æ
  * [Robust 2D/3D Vehicle Parsing in Arbitrary Camera Views for CVIS](https://openaccess.thecvf.com/content/ICCV2021/papers/Miao_Robust_2D3D_Vehicle_Parsing_in_Arbitrary_Camera_Views_for_CVIS_ICCV_2021_paper.pdf)
* è½¦é“çº¿æ£€æµ‹
  * [CondLaneNet: A Top-To-Down Lane Detection Framework Based on Conditional Convolution](https://arxiv.org/abs/2105.05003)<br>:star:[code](https://github.com/aliyun/conditional-lane-detection)
  * [Active Learning for Lane Detection: A Knowledge Distillation Approach](https://openaccess.thecvf.com/content/ICCV2021/papers/Peng_Active_Learning_for_Lane_Detection_A_Knowledge_Distillation_Approach_ICCV_2021_paper.pdf)
* è½¦é€Ÿä¼°è®¡
  * [Robust Automatic Monocular Vehicle Speed Estimation for Traffic Surveillance](https://openaccess.thecvf.com/content/ICCV2021/papers/Revaud_Robust_Automatic_Monocular_Vehicle_Speed_Estimation_for_Traffic_Surveillance_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/naver/cctv)

<a name="18"/>

## 18.Transformers
* [Generic Attention-model Explainability for Interpreting Bi-Modal and Encoder-Decoder Transformers](https://arxiv.org/abs/2103.15679)<br>:open_mouth:oral:star:[code](https://github.com/hila-chefer/Transformer-MM-Explainability)<br>:newspaper:è§£è¯»:[ICCV2021 Oral-TAU&Facebookæå‡ºäº†é€šç”¨çš„Attentionæ¨¡å‹å¯è§£é‡Šæ€§](https://mp.weixin.qq.com/s/3skb8XMiwM3QtdH7ZazoYg)
* [Transformer-Based Attention Networks for Continuous Pixel-Wise Prediction](https://openaccess.thecvf.com/content/ICCV2021/papers/Yang_Transformer-Based_Attention_Networks_for_Continuous_Pixel-Wise_Prediction_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/ygjwd12345/TransDepth)
* [PlaneTR: Structure-Guided Transformers for 3D Plane Recovery](https://arxiv.org/abs/2107.13108)<br>:star:[code](https://github.com/IceTTTb/PlaneTR3D)
* [Rethinking and Improving Relative Position Encoding for Vision Transformer](https://arxiv.org/abs/2107.14222)<br>:star:[code](https://github.com/microsoft/AutoML/tree/main/iRPE)
* [Vision Transformer with Progressive Sampling](https://arxiv.org/abs/2108.01684)<br>:star:[code](https://github.com/yuexy/PS-ViT)
* [Paint Transformer: Feed Forward Neural Painting with Stroke Prediction](https://arxiv.org/abs/2108.03798)<br>:open_mouth:oral:star:[code](https://github.com/Huage001/PaintTransformer)
* [Rethinking Spatial Dimensions of Vision Transformers](https://arxiv.org/abs/2103.16302)<br>:star:[code](https://github.com/naver-ai/pit)<br>:newspaper:è§£è¯»:[ICCV2021-PiT-æ± åŒ–æ“ä½œä¸æ˜¯CNNçš„ä¸“å±ï¼ŒViTè¯´ï¼šâ€œæˆ‘ä¹Ÿå¯ä»¥â€ï¼›å—å¤§æå‡ºæ± åŒ–è§†è§‰Transformerï¼ˆPiTï¼‰](https://mp.weixin.qq.com/s/b051uw8SSu6x-5R27e8AMg)
* [PnP-DETR: Towards Efficient Visual Analysis with Transformers](https://arxiv.org/abs/2109.07036)<br>:star:[code](https://github.com/twangnh/pnp-detr)
* [Describing and Localizing Multiple Changes With Transformers](https://arxiv.org/abs/2103.14146)<br>:star:[code](https://github.com/cvpaperchallenge/Describing-and-Localizing-Multiple-Change-with-Transformers):house:[project](https://cvpaperchallenge.github.io/Describing-and-Localizing-Multiple-Change-with-Transformers/)
* [LeViT: A Vision Transformer in ConvNet's Clothing for Faster Inference](https://openaccess.thecvf.com/content/ICCV2021/papers/Graham_LeViT_A_Vision_Transformer_in_ConvNets_Clothing_for_Faster_Inference_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/facebookresearch/LeViT)
* [VidTr: Video Transformer Without Convolutions](http://arxiv.org/abs/2104.11746)
* [Visformer: The Vision-Friendly Transformer](https://arxiv.org/abs/2104.12533)<br>:star:[code](https://github.com/danczs/Visformer)
* [Going Deeper With Image Transformers](https://arxiv.org/abs/2103.17239)<br>:star:[code](https://github.com/facebookresearch/deit)
* [Multiscale Vision Transformers](https://arxiv.org/abs/2104.11227)<br>:star:[code](https://github.com/facebookresearch/SlowFast)
* [Learning Multi-Scene Absolute Pose Regression With Transformers](https://arxiv.org/abs/2103.11468)<br>:star:[code](https://github.com/yolish/multi-scene-pose-transformer)
* [Visual Saliency Transformer](https://arxiv.org/abs/2104.12099)<br>:star:[code](https://github.com/nnizhang/VST)
* [Event-Based Video Reconstruction Using Transformer](https://openaccess.thecvf.com/content/ICCV2021/papers/Weng_Event-Based_Video_Reconstruction_Using_Transformer_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/WarranWeng/ET-Net)
* [Swin Transformer: Hierarchical Vision Transformer Using Shifted Windows](https://arxiv.org/abs/2103.14030)<br>:star:[code](https://github.com/microsoft/Swin-Transformer)
* [An Empirical Study of Training Self-Supervised Vision Transformers](https://arxiv.org/abs/2104.02057)<br>:open_mouth:oral:star:[code](https://github.com/facebookresearch/moco-v3)
* [Tokens-to-Token ViT: Training Vision Transformers From Scratch on ImageNet](https://arxiv.org/abs/2101.11986)<br>:star:[code](https://github.com/yitu-opensource/T2T-ViT)
* [CvT: Introducing Convolutions to Vision Transformers](https://arxiv.org/abs/2103.15808)<br>:star:[code](https://github.com/leoxiaobin/CvT)
* [COTR: Correspondence Transformer for Matching Across Images](https://arxiv.org/abs/2103.14167)
* [ViViT: A Video Vision Transformer](https://arxiv.org/abs/2103.15691)<br>:star:[code](https://github.com/google-research/scenic)
* [AgentFormer: Agent-Aware Transformers for Socio-Temporal Multi-Agent Forecasting](https://arxiv.org/abs/2103.14023)<br>:star:[code](https://github.com/Khrylx/AgentFormer):house:[project](https://www.ye-yuan.com/agentformer/)
* [Incorporating Convolution Designs into Visual Transformers](https://arxiv.org/abs/2103.11816)<br>:star:[code](https://github.com/coeusguo/ceit)
* [LayoutTransformer: Layout Generation and Completion with Self-attention](https://arxiv.org/abs/2006.14615)<br>:star:[code](https://github.com/kampta/DeepLayout):house:[project](https://kampta.github.io/layout/)
* [AutoFormer: Searching Transformers for Visual Recognition](https://arxiv.org/abs/2107.00651)<br>:star:[code](https://github.com/microsoft/Cream)
* [Scalable Vision Transformers With Hierarchical Pooling](https://arxiv.org/abs/2103.10619)<br>:star:[code](https://github.com/MonashAI/HVT)
* [Visual Transformers: Where Do Transformers Really Belong in Vision Models?](https://openaccess.thecvf.com/content/ICCV2021/papers/Wu_Visual_Transformers_Where_Do_Transformers_Really_Belong_in_Vision_Models_ICCV_2021_paper.pdf)
* [Anticipative Video Transformer](https://arxiv.org/abs/2106.02036)<br>:star:[code](https://github.com/facebookresearch/AVT):house:[project](https://facebookresearch.github.io/AVT/)
* å¯†é›†é¢„æµ‹
  * [Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions](https://arxiv.org/abs/2102.12122)<br>:open_mouth:oral:star:[code](https://github.com/whai362/PVT)<br>:newspaper:è§£è¯»:[å¤§ç™½è¯Pyramid Vision Transformer](https://mp.weixin.qq.com/s/oJHZWmStQYYzEEOveiuQrQ)
  * [Vision Transformers for Dense Prediction](https://arxiv.org/abs/2103.13413)<br>:star:[code](https://github.com/isl-org/DPT)
* 3Däººä½“çº¹ç†ä¼°è®¡
  * [3D Human Texture Estimation from a Single Image with Transformers](https://arxiv.org/abs/2109.02563)<br>:open_mouth:oral:house:[project](https://www.mmlab-ntu.com/project/texformer/)
* å›¾åƒç¼–è¾‘  
  * [Multi-Scale Vision Longformer: A New Vision Transformer for High-Resolution Image Encoding](https://arxiv.org/abs/2103.15358)<br>:star:[code](https://github.com/microsoft/vision-longformer)
* OCR  
  * [DocFormer: End-to-End Transformer for Document Understanding](https://arxiv.org/abs/2106.11539)
* æ ¹æ®éŸ³ä¹ç”Ÿæˆèˆè¹ˆ
  * [AI Choreographer: Music Conditioned 3D Dance Generation With AIST++](https://openaccess.thecvf.com/content/ICCV2021/papers/Li_AI_Choreographer_Music_Conditioned_3D_Dance_Generation_With_AIST_ICCV_2021_paper.pdf)<br>:house:[project](https://google.github.io/aichoreographer/)<br>:newspaper:ç®€ä»‹:[Transformeråˆåˆæ¥äº†ï¼Œç”Ÿæˆé…æœ‰éŸ³ä¹çš„ä¸æ»‘3Dèˆè¹ˆï¼Œå¼€æ”¾æœ€å¤§è§„æ¨¡æ•°æ®é›†AIST++](https://zhuanlan.zhihu.com/p/346151291)


<a name="17"/>

## 17.3D(ä¸‰ç»´è§†è§‰)
* [Discovering 3D Parts from Image Collections](https://arxiv.org/abs/2107.13629)<br>:open_mouth:oral:star:[code](https://github.com/chhankyao/lpd):house:[project](https://chhankyao.github.io/lpd/):tv:[video](https://youtu.be/erNQANNwK6k)
* [PixelSynth: Generating a 3D-Consistent Experience from a Single Image](https://arxiv.org/abs/2108.05892)<br>:star:[code](https://github.com/crockwell/pixelsynth):house:[project](https://crockwell.github.io/pixelsynth/)
* [Towers of Babel: Combining Images, Language, and 3D Geometry for Learning Multimodal Vision](https://arxiv.org/abs/2108.05863)<br>:star:[code](https://github.com/tgxs002/wikiscenes):house:[project](https://www.cs.cornell.edu/projects/babel/)
* [Pixel-Perfect Structure-from-Motion with Featuremetric Refinement](https://arxiv.org/abs/2108.08291)<br>:star:[code](https://github.com/cvg/pixel-perfect-sfm)
* [Learning Anchored Unsigned Distance Functions with Gradient Direction Alignment for Single-view Garment Reconstruction](https://arxiv.org/abs/2108.08478)<br>:open_mouth:oral:star:[code](https://github.com/zhaofang0627/AnchorUDF)
* [LSD-StructureNet: Modeling Levels of Structural Detail in 3D Part Hierarchies](https://arxiv.org/abs/2108.13459)
* [Rational Polynomial Camera Model Warping for Deep Learning Based Satellite Multi-View Stereo Matching](https://arxiv.org/abs/2109.11121)<br>:star:[code](https://github.com/WHU-GPCV/SatMVS)
* [Where2Act: From Pixels to Actions for Articulated 3D Objects](https://arxiv.org/abs/2101.02692)<br>:tv:[video](https://www.youtube.com/watch?v=cdMSZru3Aa8)
* [BuildingNet: Learning to Label 3D Buildings](https://arxiv.org/abs/2110.04955)<br>:open_mouth:oral:star:[code](https://github.com/buildingnet/buildingnet_dataset):house:[project](https://buildingnet.org/)
* [SurfGen: Adversarial 3D Shape Synthesis With Explicit Surface Discriminators](https://openaccess.thecvf.com/content/ICCV2021/papers/Luo_SurfGen_Adversarial_3D_Shape_Synthesis_With_Explicit_Surface_Discriminators_ICCV_2021_paper.pdf)
* [Deep Virtual Markers for Articulated 3D Shapes](https://arxiv.org/abs/2108.09000)<br>:star:[code](https://github.com/T2Kim/DeepVirtualMarkers):tv:[video](https://www.youtube.com/watch?v=Raq5axLdG6E)
* [Learning Efficient Photometric Feature Transform for Multi-view Stereo](https://arxiv.org/abs/2103.14794)<br>:house:[project](https://svbrdf.github.io/publications/ptmvs/project.html)
* [Neural-GIF: Neural Generalized Implicit Functions for Animating People in Clothing](https://openaccess.thecvf.com/content/ICCV2021/papers/Tiwari_Neural-GIF_Neural_Generalized_Implicit_Functions_for_Animating_People_in_Clothing_ICCV_2021_paper.pdf)
* [Just a Few Points Are All You Need for Multi-View Stereo: A Novel Semi-Supervised Learning Method for Multi-View Stereo](https://openaccess.thecvf.com/content/ICCV2021/papers/Kim_Just_a_Few_Points_Are_All_You_Need_for_Multi-View_ICCV_2021_paper.pdf)
* [3D-FRONT: 3D Furnished Rooms With layOuts and semaNTics](https://openaccess.thecvf.com/content/ICCV2021/papers/Fu_3D-FRONT_3D_Furnished_Rooms_With_layOuts_and_semaNTics_ICCV_2021_paper.pdf)
* [Learning Generative Models of Textured 3D Meshes from Real-World Images](https://arxiv.org/abs/2103.15627)<br>:star:[code](https://github.com/dariopavllo/textured-3d-gan)
* [Self-Supervised Pretraining of 3D Features on any Point-Cloud](https://arxiv.org/abs/2101.02691)
* [High Quality Disparity Remapping with Two-Stage Warping](https://openaccess.thecvf.com/content/ICCV2021/papers/Li_High_Quality_Disparity_Remapping_With_Two-Stage_Warping_ICCV_2021_paper.pdf)
* [Structure-From-Sherds: Incremental 3D Reassembly of Axially Symmetric Pots From Unordered and Mixed Fragment Collections](https://openaccess.thecvf.com/content/ICCV2021/papers/Hong_Structure-From-Sherds_Incremental_3D_Reassembly_of_Axially_Symmetric_Pots_From_Unordered_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/SeongJong-Yoo/structure-from-sherds)
* [Interpolation-Aware Padding for 3D Sparse Convolutional Neural Networks](https://arxiv.org/abs/2108.06925)
* æ·±åº¦ä¼°è®¡
  * [StructDepth: Leveraging the structural regularities for self-supervised indoor depth estimation](https://arxiv.org/abs/2108.08574)<br>:star:[code](https://github.com/SJTU-ViSYS/StructDepth)
  * [Bridging Unsupervised and Supervised Depth from Focus via All-in-Focus Supervision](https://arxiv.org/abs/2108.10843)<br>:star:[code](https://github.com/albert100121/AiFDepthNet):house:[project](https://albert100121.github.io/AiFDepthNet/)
  * [Augmenting Depth Estimation with Geospatial Context](https://arxiv.org/abs/2109.09879)
  * [Can Scale-Consistent Monocular Depth Be Learned in a Self-Supervised Scale-Invariant Manner?](https://openaccess.thecvf.com/content/ICCV2021/papers/Wang_Can_Scale-Consistent_Monocular_Depth_Be_Learned_in_a_Self-Supervised_Scale-Invariant_ICCV_2021_paper.pdf)
  * [Revisiting Stereo Depth Estimation From a Sequence-to-Sequence Perspective With Transformers](https://arxiv.org/abs/2011.02910)<br>:open_mouth:oral:star:[code](https://github.com/mli0603/stereo-transformer)
  * [Adaptive Surface Normal Constraint for Depth Estimation](https://arxiv.org/abs/2103.15483)<br>:star:[code](https://github.com/xxlong0/ASNDepth)
  * [Event-Intensity Stereo: Estimating Depth by the Best of Both Worlds](https://openaccess.thecvf.com/content/ICCV2021/papers/Mostafavi_Event-Intensity_Stereo_Estimating_Depth_by_the_Best_of_Both_Worlds_ICCV_2021_paper.pdf)
  * [DnD: Dense Depth Estimation in Crowded Dynamic Indoor Scenes](https://arxiv.org/abs/2108.05615)
  * [DepthInSpace: Exploitation and Fusion of Multiple Video Frames for Structured-Light Depth Estimation](https://openaccess.thecvf.com/content/ICCV2021/papers/Johari_DepthInSpace_Exploitation_and_Fusion_of_Multiple_Video_Frames_for_Structured-Light_ICCV_2021_paper.pdf)<br>:house:[project](https://www.idiap.ch/paper/depthinspace/)
  * [Boosting Monocular Depth Estimation With Lightweight 3D Point Fusion](https://arxiv.org/abs/2012.10296)
  * Monocular Depth Estimation(å•ç›®æ·±åº¦ä¼°è®¡)
    * [Revealing the Reciprocal Relations Between Self-Supervised Stereo and Monocular Depth Estimation](https://openaccess.thecvf.com/content/ICCV2021/papers/Chen_Revealing_the_Reciprocal_Relations_Between_Self-Supervised_Stereo_and_Monocular_Depth_ICCV_2021_paper.pdf)
    * [MonoIndoor: Towards Good Practice of Self-Supervised Monocular Depth Estimation for Indoor Environments](https://arxiv.org/abs/2107.12429)
    * [Regularizing Nighttime Weirdness: Efficient Self-supervised Monocular Depth Estimation in the Dark](https://arxiv.org/abs/2108.03830)<br>:star:[code](https://github.com/w2kun/RNW)
    * [Towards Interpretable Deep Networks for Monocular Depth Estimation](https://arxiv.org/abs/2108.05312)<br>:star:[code](https://github.com/youzunzhi/InterpretableMDE):tv:[video](https://www.youtube.com/watch?v=er7TF2CusWo)
    * [Self-supervised Monocular Depth Estimation for All Day Images using Domain Separation](https://arxiv.org/abs/2108.07628)<br>:star:[code](https://github.com/LINA-lln/ADDS-DepthNet)
    * [Fine-grained Semantics-aware Representation Enhancement for Self-supervised Monocular Depth Estimation](https://arxiv.org/abs/2108.08829)<br>:open_mouth:oral:star:[code](https://github.com/hyBlue/FSRE-Depth)
    * [Excavating the Potential Capacity of Self-Supervised Monocular Depth Estimation](https://arxiv.org/abs/2109.12484)<br>:star:[code](https://github.com/prstrive/EPCDepth)
    * [R-MSFM: Recurrent Multi-Scale Feature Modulation for Monocular Depth Estimating](https://openaccess.thecvf.com/content/ICCV2021/papers/Zhou_R-MSFM_Recurrent_Multi-Scale_Feature_Modulation_for_Monocular_Depth_Estimating_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/jsczzzk/R-MSFM)
    * [Adaptive Confidence Thresholding for Monocular Depth Estimation](https://arxiv.org/abs/2009.12840)(https://github.com/megvii-research/OMNet)
    * [SaccadeCam: Adaptive Visual Attention for Monocular Depth Sensing](https://arxiv.org/abs/2103.12981)
* æ·±åº¦è¡¥å…¨
  * [Bayesian Deep Basis Fitting for Depth Completion With Uncertainty](https://arxiv.org/abs/2103.15254)
  * [Unsupervised Depth Completion With Calibrated Backprojection Layers](https://arxiv.org/abs/2108.10531)<br>:star:[code](https://github.com/alexklwong/calibrated-backprojection-network)
* Omnidirectional Localization
  * [PICCOLO: Point Cloud-Centric Omnidirectional Localization](https://arxiv.org/abs/2108.06545)
* ä¸‰ç»´é‡å»º
  * [Learning Signed Distance Field for Multi-view Surface Reconstruction](https://arxiv.org/abs/2108.09964)<br>:open_mouth:oral
  * [3DStyleNet: Creating 3D Shapes with Geometric and Texture Style Variations](https://arxiv.org/abs/2108.12958)<br>:open_mouth:oral:house:[project](https://nv-tlabs.github.io/3DStyleNet/)
  * [DensePose 3D: Lifting Canonical Surface Maps of Articulated Objects to the Third Dimension](https://arxiv.org/abs/2109.00033)
  * [In-the-Wild Single Camera 3D Reconstruction Through Moving Water Surfaces](https://vccimaging.org/Publications/Xiong2021MovingWater/Xiong2021MovingWater.pdf)<br>:open_mouth:oral:star:[code](https://github.com/vccimaging/Reconstrution_Through_Moving_Water):tv:[video](https://www.youtube.com/watch?v=F6R52hfAs6s)
  * [Gaussian Fusion: Accurate 3D Reconstruction via Geometry-Guided Displacement Interpolation](https://openaccess.thecvf.com/content/ICCV2021/papers/Chen_Gaussian_Fusion_Accurate_3D_Reconstruction_via_Geometry-Guided_Displacement_Interpolation_ICCV_2021_paper.pdf)
  * [RetrievalFuse: Neural 3D Scene Reconstruction With a Database](https://arxiv.org/abs/2104.00024)<br>:star:[code](https://github.com/nihalsid/retrieval-fuse):house:[project](https://nihalsid.github.io/retrieval-fuse/):tv:[video](https://www.youtube.com/watch?v=HbsUU0YODqE)
  * [Multi-View 3D Reconstruction With Transformers](https://arxiv.org/abs/2103.12957)
  * [Polarimetric Helmholtz Stereopsis](https://openaccess.thecvf.com/content/ICCV2021/papers/Ding_Polarimetric_Helmholtz_Stereopsis_ICCV_2021_paper.pdf)
  * [MINE: Towards Continuous Depth MPI with NeRF for Novel View Synthesis](https://arxiv.org/abs/2103.14910)<br>:star:[code](https://github.com/vincentfung13/MINE):tv:[video](https://www.youtube.com/watch?v=I_92BXju350)
  * [Toward Realistic Single-View 3D Object Reconstruction With Unsupervised Learning From Multiple Images](https://arxiv.org/abs/2109.02288)<br>:star:[code](https://github.com/VinAIResearch/LeMul)
  * [CryoDRGN2: Ab initio neural reconstruction of 3D protein structures from real cryo-EM images](https://openaccess.thecvf.com/content/ICCV2021/papers/Zhong_CryoDRGN2_Ab_Initio_Neural_Reconstruction_of_3D_Protein_Structures_From_ICCV_2021_paper.pdf)
  * ä¸‰ç»´åœºæ™¯é‡å»º
    * [Graph-to-3D: End-to-End Generation and Manipulation of 3D Scenes Using Scene Graphs](https://arxiv.org/abs/2108.08841)
    * [VolumeFusion: Deep Depth Fusion for 3D Scene Reconstruction](https://arxiv.org/abs/2108.08623)
  * ä¸‰ç»´å½¢çŠ¶é‡å»º  
    * [3DIAS: 3D Shape Reconstruction with Implicit Algebraic Surfaces](https://arxiv.org/abs/2108.08653)<br>:house:[project](https://myavartanoo.github.io/3dias/)
    * [Multiresolution Deep Implicit Functions for 3D Shape Representation](https://arxiv.org/abs/2109.05591)
    * [MVTN: Multi-View Transformation Network for 3D Shape Recognition](https://arxiv.org/abs/2011.13244)<br>:star:[code](https://github.com/ajhamdi/MVTN):tv:[video](https://www.youtube.com/watch?v=1zaHx8ztlhk)
    * [Sketch2Mesh: Reconstructing and Editing 3D Shapes from Sketches](https://arxiv.org/abs/2104.00482)
  * ä¸‰ç»´ç½‘æ ¼é‡å»º    
    * [Vis2Mesh: Efficient Mesh Reconstruction from Unstructured Point Clouds of Large Scenes with Learned Virtual View Visibility](https://arxiv.org/abs/2108.08378)<br>:star:[code](https://github.com/GDAOSU/vis2mesh) 
* ä¸‰ç»´åœºæ™¯
  * [Scene Synthesis via Uncertainty-Driven Attribute Synchronization](https://arxiv.org/abs/2108.13499)<br>:star:[code](https://github.com/yanghtr/Sync2Gen)
  * [Graph-to-3D: End-to-End Generation and Manipulation of 3D Scenes Using Scene Graphs](https://openaccess.thecvf.com/content/ICCV2021/papers/Dhamo_Graph-to-3D_End-to-End_Generation_and_Manipulation_of_3D_Scenes_Using_Scene_ICCV_2021_paper.pdf)<br>:house:[project](https://he-dhamo.github.io/Graphto3D/)
* ç›¸æœºæ ¡å‡†
  * [CTRL-C: Camera calibration TRansformer with Line-Classification](https://arxiv.org/abs/2109.02259)<br>:star:[code](https://github.com/jwlee-vcl/CTRL-C) 
  * [BabelCalib: A Universal Approach to Calibrating Central Cameras](https://arxiv.org/abs/2109.09704)<br>:star:[code](https://github.com/ylochman/babelcalib)
* è¡¨é¢é‡å»º
  * [Temporally-Coherent Surface Reconstruction via Metric-Consistent Atlases](https://arxiv.org/abs/2104.06950)
  * [Planar Surface Reconstruction from Sparse Views](https://arxiv.org/abs/2103.14644)<br>:open_mouth:oral:star:[code](https://github.com/jinlinyi/SparsePlanes):house:[project](https://jinlinyi.github.io/SparsePlanes/):tv:[video](https://www.youtube.com/watch?v=US3EKPe3nAw)
  * [Adaptive Surface Reconstruction With Multiscale Convolutional Kernels](https://openaccess.thecvf.com/content/ICCV2021/papers/Ummenhofer_Adaptive_Surface_Reconstruction_With_Multiscale_Convolutional_Kernels_ICCV_2021_paper.pdf)
* 3Dåœºæ™¯åˆæˆ
  * [Indoor Scene Generation From a Collection of Semantic-Segmented Depth Images](https://arxiv.org/abs/2108.09022)<br>:star:[code](https://github.com/mingjiayang/SGSDI)
* 3Då½¢çŠ¶è¯†åˆ«
  * [Learning Canonical View Representation for 3D Shape Recognition With Arbitrary Views](https://arxiv.org/abs/2108.07084)<br>:star:[code](https://github.com/weixmath/CVR)
* å›¾åƒé‡å»º
  * [Semantic-embedded Unsupervised Spectral Reconstruction from Single RGB Images in the Wild](https://arxiv.org/abs/2108.06659)<br>:star:[code](https://github.com/zbzhzhy/Unsupervised-Spectral-Reconstruction)
* Multi-view Stereo(MVS)
  * [Digging into Uncertainty in Self-supervised Multi-view Stereo](https://arxiv.org/abs/2108.12966)<br>:star:[code](https://github.com/ToughStoneX/U-MVS)
  * [PatchMatch-RL: Deep MVS with Pixelwise Depth, Normal, and Visibility](https://openaccess.thecvf.com/content/ICCV2021/papers/Lee_PatchMatch-RL_Deep_MVS_With_Pixelwise_Depth_Normal_and_Visibility_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/leejaeyong7/patchmatch-rl)
  * [A Confidence-Based Iterative Solver of Depths and Surface Normals for Deep Multi-View Stereo](https://openaccess.thecvf.com/content/ICCV2021/papers/Zhao_A_Confidence-Based_Iterative_Solver_of_Depths_and_Surface_Normals_for_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/thuzhaowang/idn-solver)
  * [EPP-MVSNet: Epipolar-Assembling Based Depth Prediction for Multi-View Stereo](https://openaccess.thecvf.com/content/ICCV2021/papers/Ma_EPP-MVSNet_Epipolar-Assembling_Based_Depth_Prediction_for_Multi-View_Stereo_ICCV_2021_paper.pdf)

<a name="16"/>

## 16.Re-Identification(é‡è¯†åˆ«)
##### Object Re-Identificationç›®æ ‡(ç‰©ä½“)é‡è¯†åˆ«
* [TransReID: Transformer-Based Object Re-Identification](https://arxiv.org/abs/2102.04378)<br>:star:[code](https://github.com/damo-cv/TransReID)
#### Person Re-Identification(äººå‘˜é‡è¯†åˆ«)
* [Spatio-Temporal Representation Factorization for Video-based Person Re-Identification](https://arxiv.org/abs/2107.11878)
* [Learning Instance-level Spatial-Temporal Patterns for Person Re-identification](https://arxiv.org/abs/2108.00171)<br>:star:[code](https://github.com/RenMin1991/cleaned-DukeMTMC-reID/)
* [Towards Discriminative Representation Learning for Unsupervised Person Re-identification](https://arxiv.org/abs/2108.03439)
* [Learning by Aligning: Visible-Infrared Person Re-identification using Cross-Modal Correspondences](https://arxiv.org/abs/2108.07422)<br>:star:[code](https://github.com/cvlab-yonsei/LbA):house:[project](https://cvlab.yonsei.ac.kr/projects/LbA/)
* [Video-based Person Re-identification with Spatial and Temporal Memory Networks](https://arxiv.org/abs/2108.09039)<br>:house:[project](https://cvlab.yonsei.ac.kr/projects/STMN/)
* [Multi-Expert Adversarial Attack Detection in Person Re-identification Using Context Inconsistency](https://arxiv.org/abs/2108.09891)
* [Clothing Status Awareness for Long-Term Person Re-Identification](https://openaccess.thecvf.com/content/ICCV2021/papers/Huang_Clothing_Status_Awareness_for_Long-Term_Person_Re-Identification_ICCV_2021_paper.pdf)
* [Dense Interaction Learning for Video-Based Person Re-Identification](https://arxiv.org/abs/2103.09013)<br>:open_mouth:oral
* [Explainable Person Re-Identification With Attribute-Guided Metric Distillation](https://arxiv.org/abs/2103.01451)<br>:house:[project](https://xiaodongchen.cn/AMD.github.io/)
* [Online Pseudo Label Generation by Hierarchical Cluster Dynamics for Adaptive Person Re-Identification](https://openaccess.thecvf.com/content/ICCV2021/papers/Zheng_Online_Pseudo_Label_Generation_by_Hierarchical_Cluster_Dynamics_for_Adaptive_ICCV_2021_paper.pdf)
* [Pyramid Spatial-Temporal Aggregation for Video-Based Person Re-Identification](https://openaccess.thecvf.com/content/ICCV2021/papers/Wang_Pyramid_Spatial-Temporal_Aggregation_for_Video-Based_Person_Re-Identification_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/WangYQ9/VideoReID_PSTA)
* [ICE: Inter-Instance Contrastive Encoding for Unsupervised Person Re-Identification](https://arxiv.org/abs/2103.16364)<br>:star:[code](https://github.com/chenhao2345/ICE):tv:[video](https://drive.google.com/file/d/1E__ru9u_oRcb44-WIH_GjBTv1-_5rcO2/view)
* [Learning To Know Where To See: A Visibility-Aware Approach for Occluded Person Re-Identification](https://openaccess.thecvf.com/content/ICCV2021/papers/Yang_Learning_To_Know_Where_To_See_A_Visibility-Aware_Approach_for_ICCV_2021_paper.pdf)
* [Attack-Guided Perceptual Data Generation for Real-world Re-Identification](https://openaccess.thecvf.com/content/ICCV2021/papers/Huang_Attack-Guided_Perceptual_Data_Generation_for_Real-World_Re-Identification_ICCV_2021_paper.pdf)
* [BV-Person: A Large-Scale Dataset for Bird-View Person Re-Identification](https://openaccess.thecvf.com/content/ICCV2021/papers/Yan_BV-Person_A_Large-Scale_Dataset_for_Bird-View_Person_Re-Identification_ICCV_2021_paper.pdf)<br>:sunflower:[dataset](https://github.com/daidaidouer/BVPerson)
* [CM-NAS: Cross-Modality Neural Architecture Search for Visible-Infrared Person Re-Identification](https://openaccess.thecvf.com/content/ICCV2021/papers/Fu_CM-NAS_Cross-Modality_Neural_Architecture_Search_for_Visible-Infrared_Person_Re-Identification_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/JDAI-CV/CM-NAS)
* [Meta Pairwise Relationship Distillation for Unsupervised Person Re-Identification](https://openaccess.thecvf.com/content/ICCV2021/papers/Ji_Meta_Pairwise_Relationship_Distillation_for_Unsupervised_Person_Re-Identification_ICCV_2021_paper.pdf)
* [Syncretic Modality Collaborative Learning for Visible Infrared Person Re-Identification](https://openaccess.thecvf.com/content/ICCV2021/papers/Wei_Syncretic_Modality_Collaborative_Learning_for_Visible_Infrared_Person_Re-Identification_ICCV_2021_paper.pdf)
* [Weakly Supervised Text-Based Person Re-Identification](https://openaccess.thecvf.com/content/ICCV2021/papers/Zhao_Weakly_Supervised_Text-Based_Person_Re-Identification_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/X-BrainLab/WS_Text-ReID)
* [Occlude Them All: Occlusion-Aware Attention Network for Occluded Person Re-ID](https://openaccess.thecvf.com/content/ICCV2021/papers/Chen_Occlude_Them_All_Occlusion-Aware_Attention_Network_for_Occluded_Person_Re-ID_ICCV_2021_paper.pdf)
* [Occluded Person Re-Identification with Single-scale Global Representations](https://openaccess.thecvf.com/content/ICCV2021/papers/Yan_Occluded_Person_Re-Identification_With_Single-Scale_Global_Representations_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/daidaidouer/OP-ReID)
* åŸŸé€‚åº”äººå‘˜é‡è¯†åˆ«
  * [IDM: An Intermediate Domain Module for Domain Adaptive Person Re-ID](https://arxiv.org/abs/2108.02413)<br>:open_mouth:oral:star:[code](https://github.com/SikaStar/IDM)
* Crowd Counting(æ‹¥æŒ¤äººç¾¤è®¡æ•°)
  * [Rethinking Counting and Localization in Crowds:A Purely Point-Based Framework](https://arxiv.org/abs/2107.12746)<br>:open_mouth:oral:star:[code](https://github.com/TencentYoutuResearch)
  * [Uniformity in Heterogeneity:Diving Deep into Count Interval Partition for Crowd Counting](https://arxiv.org/abs/2107.12619)<br>:star:[code](https://github.com/TencentYoutuResearch)
  * [Spatial Uncertainty-Aware Semi-Supervised Crowd Counting](https://arxiv.org/abs/2107.13271)<br>:star:[code](https://github.com/smallmax00/SUA_crowd_counting)
  * [Variational Attention: Propagating Domain-Specific Knowledge for Multi-Domain Learning in Crowd Counting](https://arxiv.org/abs/2108.08023)<br>:star:[code](https://github.com/Zhaoyi-Yan/DKPNet)
  * [Exploiting Sample Correlation for Crowd Counting With Multi-Expert Network](https://openaccess.thecvf.com/content/ICCV2021/papers/Liu_Exploiting_Sample_Correlation_for_Crowd_Counting_With_Multi-Expert_Network_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/streamer-AP)
  * [Crowd Counting With Partial Annotations in an Image](https://openaccess.thecvf.com/content/ICCV2021/papers/Xu_Crowd_Counting_With_Partial_Annotations_in_an_Image_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/svip-lab/CrwodCountingPAL)
  * [Towards A Universal Model for Cross-Dataset Crowd Counting](https://openaccess.thecvf.com/content/ICCV2021/papers/Ma_Towards_a_Universal_Model_for_Cross-Dataset_Crowd_Counting_ICCV_2021_paper.pdf)
  * [Rethinking Counting and Localization in Crowds:A Purely Point-Based Framework](https://arxiv.org/abs/2107.12746)<br>:open_mouth:oral:star:[code](https://github.com/TencentYoutuResearch/CrowdCounting-P2PNet)
  * [Uniformity in Heterogeneity: Diving Deep Into Count Interval Partition for Crowd Counting](https://arxiv.org/abs/2107.12619)<br>:star:[code](https://github.com/TencentYoutuResearch/CrowdCounting-UEPNet)
* è·¨æ¨¡æ€äººå‘˜é‡è¯†åˆ«
  * [Cross-Modality Person Re-Identification via Modality Confusion and Center Aggregation](https://openaccess.thecvf.com/content/ICCV2021/papers/Hao_Cross-Modality_Person_Re-Identification_via_Modality_Confusion_and_Center_Aggregation_ICCV_2021_paper.pdf)
* è¡Œäººæ£€æµ‹
  * [MOTSynth: How Can Synthetic Data Help Pedestrian Detection and Tracking?](https://arxiv.org/abs/2108.09518)
  * [Stacked Homography Transformations for Multi-View Pedestrian Detection](https://openaccess.thecvf.com/content/ICCV2021/papers/Song_Stacked_Homography_Transformations_for_Multi-View_Pedestrian_Detection_ICCV_2021_paper.pdf)
  * [Robust Small-scale Pedestrian Detection with Cued Recall via Memory Learning](https://openaccess.thecvf.com/content/ICCV2021/papers/Kim_Robust_Small-Scale_Pedestrian_Detection_With_Cued_Recall_via_Memory_Learning_ICCV_2021_paper.pdf)
  * [Body-Face Joint Detection via Embedding and Head Hook](https://openaccess.thecvf.com/content/ICCV2021/papers/Wan_Body-Face_Joint_Detection_via_Embedding_and_Head_Hook_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/AibeeDetect/BFJDet)
* è¡Œäººå±æ€§è¯†åˆ«
  * [Spatial and Semantic Consistency Regularizations for Pedestrian Attribute Recognition](https://arxiv.org/abs/2109.05686)
  * [LapsCore: Language-Guided Person Search via Color Reasoning](https://openaccess.thecvf.com/content/ICCV2021/papers/Wu_LapsCore_Language-Guided_Person_Search_via_Color_Reasoning_ICCV_2021_paper.pdf)
* Person Search(è¡Œäººæœç´¢)
  * [Weakly Supervised Person Search with Region Siamese Networks](https://arxiv.org/abs/2109.06109)
  * [End-to-End Trainable Trident Person Search Network Using Adaptive Gradient Propagation](https://openaccess.thecvf.com/content/ICCV2021/papers/Han_End-to-End_Trainable_Trident_Person_Search_Network_Using_Adaptive_Gradient_Propagation_ICCV_2021_paper.pdf)
  * [ASMR: Learning Attribute-Based Person Search with Adaptive Semantic Margin Regularizer](https://arxiv.org/abs/2108.04533)<br>:house:[project](http://cvlab.postech.ac.kr/research/ASMR/)
  * [Weakly Supervised Person Search with Region Siamese Networks](https://arxiv.org/abs/2109.06109)
* è¡Œäººè¡Œä¸ºé¢„æµ‹
  * [Bifold and Semantic Reasoning for Pedestrian Behavior Prediction](https://arxiv.org/abs/2012.03298)
* æ­¥æ€è¯†åˆ«
  * [Gait Recognition via Effective Global-Local Feature Representation and Local Temporal Aggregation](https://arxiv.org/abs/2011.01461)
  * [Context-Sensitive Temporal Feature Learning for Gait Recognition](https://openaccess.thecvf.com/content/ICCV2021/papers/Huang_Context-Sensitive_Temporal_Feature_Learning_for_Gait_Recognition_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/OliverHxh/CSTL)
  * [3D Local Convolutional Neural Networks for Gait Recognition](https://openaccess.thecvf.com/content/ICCV2021/papers/Huang_3D_Local_Convolutional_Neural_Networks_for_Gait_Recognition_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/yellowtownhz/3DLocalCNN)
  * [Gait Recognition in the Wild: A Benchmark](https://openaccess.thecvf.com/content/ICCV2021/papers/Zhu_Gait_Recognition_in_the_Wild_A_Benchmark_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/GREW-Benchmark/GREW-Benchmark):house:[project](https://www.grew-benchmark.org/)

<a name="15"/>

## 15.Object Tracking(ç›®æ ‡è·Ÿè¸ª)
* [Saliency-Associated Object Tracking](https://arxiv.org/abs/2108.03637)
* [Box-Aware Feature Enhancement for Single Object Tracking on Point Clouds](https://arxiv.org/abs/2108.04728)<br>:star:[code](https://github.com/Ghostish/BAT)
* [Learning to Track Objects from Unlabeled Videos](https://arxiv.org/abs/2108.12711)<br>:star:[code](https://github.com/VISION-SJTU/USOT)
* [DepthTrack : Unveiling the Power of RGBD Tracking](https://arxiv.org/abs/2108.13962)<br>:star:[code](https://github.com/xiaozai/DeT)
* [Learning Target Candidate Association To Keep Track of What Not To Track](https://arxiv.org/abs/2103.16556)<br>:star:[code](https://github.com/visionml/pytracking)
* [Transparent Object Tracking Benchmark](https://arxiv.org/abs/2011.10875)<br>:house:[project](https://hengfan2010.github.io/projects/TOTB/)
* [DepthTrack: Unveiling the Power of RGBD Tracking](https://arxiv.org/abs/2108.13962)<br>:star:[code](https://github.com/xiaozai/DeT)
* [Object Tracking by Jointly Exploiting Frame and Event Domain](https://arxiv.org/abs/2109.09052)
* [High-Performance Discriminative Tracking with Transformers](https://openaccess.thecvf.com/content/ICCV2021/papers/Yu_High-Performance_Discriminative_Tracking_With_Transformers_ICCV_2021_paper.pdf)
* [Visio-Temporal Attention for Multi-Camera Multi-Target Association](https://openaccess.thecvf.com/content/ICCV2021/papers/Li_Visio-Temporal_Attention_for_Multi-Camera_Multi-Target_Association_ICCV_2021_paper.pdf)
* è§†è§‰ç›®æ ‡è·Ÿè¸ª
  * [Learning to Adversarially Blur Visual Object Tracking](https://arxiv.org/abs/2107.12085)<br>:star:[code](https://github.com/tsingqguo/ABA)
  * [Learn to Match: Automatic Matching Network Design for Visual Tracking](https://arxiv.org/abs/2108.00803)<br>:star:[code](https://github.com/JudasDie/SOTS)
  * [Video Annotation for Visual Tracking via Selection and Refinement](https://arxiv.org/abs/2108.03821)<br>:star:[code](https://github.com/Daikenan/VASR)
  * [Learning Spatio-Temporal Transformer for Visual Tracking](https://arxiv.org/abs/2103.17154)<br>:star:[code](https://github.com/researchmm/Stark)
  * 3Dè§†è§‰è·Ÿè¸ª
    * [MLVSNet: Multi-Level Voting Siamese Network for 3D Visual Tracking](https://openaccess.thecvf.com/content/ICCV2021/papers/Wang_MLVSNet_Multi-Level_Voting_Siamese_Network_for_3D_Visual_Tracking_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/CodeWZT/MLVSNet)
* å«æ˜Ÿå›¾åƒè·Ÿè¸ª 
  * [HiFT: Hierarchical Feature Transformer for Aerial Tracking](https://arxiv.org/abs/2108.00202)<br>:star:[code](https://github.com/vision4robotics/HiFT)
* 3Då¤šç›®æ ‡è·Ÿè¸ª
  * [Exploring Simple 3D Multi-Object Tracking for Autonomous Driving](https://arxiv.org/abs/2108.10312)<br>:star:[code](https://github.com/qcraftai/simtrack)<br>:newspaper:è§£è¯»:[ICCV 2021ä¸¨è½»èˆŸæ™ºèˆªæå‡ºSimTrack: 3Då¤šç›®æ ‡ä¸€ä½“åŒ–æ£€æµ‹ä¸è·Ÿè¸ªï¼Œç®€å•åˆç²¾ç¡®](https://mp.weixin.qq.com/s/7vCckbjGd65NMgW9evR4Ag)
* å¤šç›®æ ‡è·Ÿè¸ªä¸åˆ†å‰²
  * [Assignment-Space-Based Multi-Object Tracking and Segmentation](https://openaccess.thecvf.com/content/ICCV2021/papers/Choudhuri_Assignment-Space-Based_Multi-Object_Tracking_and_Segmentation_ICCV_2021_paper.pdf)
  * [Continuous Copy-Paste for One-Stage Multi-Object Tracking and Segmentation](https://openaccess.thecvf.com/content/ICCV2021/papers/Xu_Continuous_Copy-Paste_for_One-Stage_Multi-Object_Tracking_and_Segmentation_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/detectRecog/CCP)
* å¤šç›®æ ‡è·Ÿè¸ª
  * [A General Recurrent Tracking Framework Without Real Data](https://openaccess.thecvf.com/content/ICCV2021/papers/Wang_A_General_Recurrent_Tracking_Framework_Without_Real_Data_ICCV_2021_paper.pdf)
  * [Making Higher Order MOT Scalable: An Efficient Approximate Solver for Lifted Disjoint Paths](https://arxiv.org/abs/2108.10606)<br>:star:[code](https://github.com/TimoK93/ApLift)
  * [MOTSynth: How Can Synthetic Data Help Pedestrian Detection and Tracking?](https://openaccess.thecvf.com/content/ICCV2021/papers/Fabbri_MOTSynth_How_Can_Synthetic_Data_Help_Pedestrian_Detection_and_Tracking_ICCV_2021_paper.pdf)
  * [Learning To Track With Object Permanence](https://arxiv.org/abs/2103.14258)<br>:star:[code](https://github.com/TRI-ML/permatrack)å¤šç›®æ ‡è·Ÿè¸ª
* è§†é¢‘ç›®æ ‡è·Ÿè¸ª
  * [TF-Blender: Temporal Feature Blender for Video Object Detection](https://openaccess.thecvf.com/content/ICCV2021/papers/Cui_TF-Blender_Temporal_Feature_Blender_for_Video_Object_Detection_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/goodproj13/TF-Blender)


<a name="14"/>

## 14.Object Detection(ç›®æ ‡æ£€æµ‹)
* [Rank & Sort Loss for Object Detection and Instance Segmentation](https://arxiv.org/abs/2107.11669)<br>:open_mouth:oral:star:[code](https://github.com/kemaloksuz/RankSortLoss)
* [MDETR : Modulated Detection for End-to-End Multi-Modal Understanding](https://arxiv.org/abs/2104.12763)<br>:open_mouth:oral:star:[code](https://github.com/ashkamath/mdetr)
* [SimROD: A Simple Adaptation Method for Robust Object Detection](https://arxiv.org/abs/2107.13389)<br>:open_mouth:oral:house:[project](https://marketplace.huaweicloud.com/markets/aihub/notebook/detail/?id=d6d7162f-32b9-483d-97d7-b16b32b148e2)
* [GraphFPN: Graph Feature Pyramid Network for Object Detection](https://arxiv.org/abs/2108.00580)
* [Fast Convergence of DETR with Spatially Modulated Co-Attention](https://arxiv.org/abs/2108.02404)<br>:star:[code](https://github.com/gaopengcuhk/SMCA-DETR)
* [Oriented R-CNN for Object Detection](https://arxiv.org/abs/2108.05699)<br>:star:[code](https://github.com/jbwang1997/OBBDetection) 
* [Conditional DETR for Fast Training Convergence](https://arxiv.org/abs/2108.06152)<br>:newspaper:è§£è¯»:[é€šè¿‡æ˜¾å¼å¯»æ‰¾ç‰©ä½“çš„ extremity åŒºåŸŸåŠ å¿« DETR çš„æ”¶æ•›ï¼šConditional DETR](https://mp.weixin.qq.com/s/RbtdfFDczrSxi0F4apbx1w)
* [Vector-Decomposed Disentanglement for Domain-Invariant Object Detection](https://arxiv.org/abs/2108.06685)<br>:star:[code](https://github.com/AmingWu/VDD-DAOD)
* [G-DetKD: Towards General Distillation Framework for Object Detectors via Contrastive and Semantic-guided Feature Imitation](https://arxiv.org/abs/2108.07482)
* [ODAM: Object Detection, Association, and Mapping using Posed RGB Video](https://arxiv.org/abs/2108.10165)<br>:open_mouth:oral
* [Reconcile Prediction Consistency for Balanced Object Detection](https://arxiv.org/abs/2108.10809)
* [Deep Structured Instance Graph for Distilling Object Detectors](https://arxiv.org/abs/2109.12862)<br>:star:[code](https://github.com/dvlab-research/Dsig)
* [Towards Rotation Invariance in Object Detection](https://arxiv.org/abs/2109.13488)<br>:star:[code](https://github.com/akasha-imaging/ICCV2021)
* [Morphable Detector for Object Detection on Demand](https://arxiv.org/abs/2110.04917)<br>:star:[code](https://github.com/Zhaoxiangyun/Morphable-Detector)
* [DetCo: Unsupervised Contrastive Learning for Object Detection](https://arxiv.org/abs/2102.04803)<br>:star:[code](https://github.com/xieenze/DetCo)
* [Domain-Invariant Disentangled Network for Generalizable Object Detection](https://openaccess.thecvf.com/content/ICCV2021/papers/Lin_Domain-Invariant_Disentangled_Network_for_Generalizable_Object_Detection_ICCV_2021_paper.pdf)
* [MDETR - Modulated Detection for End-to-End Multi-Modal Understanding](https://openaccess.thecvf.com/content/ICCV2021/papers/Kamath_MDETR_-_Modulated_Detection_for_End-to-End_Multi-Modal_Understanding_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/ashkamath/mdetr)
* [Detecting Persuasive Atypicality by Modeling Contextual Compatibility](https://openaccess.thecvf.com/content/ICCV2021/papers/Guo_Detecting_Persuasive_Atypicality_by_Modeling_Contextual_Compatibility_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/MeiqiGuo/ICCV2021-AtypicalityDetection)
* [Wanderlust: Online Continual Object Detection in the Real World](https://openaccess.thecvf.com/content/ICCV2021/papers/Wang_Wanderlust_Online_Continual_Object_Detection_in_the_Real_World_ICCV_2021_paper.pdf)<br>:house:[project](https://oakdata.github.io/)
* [PreDet: Large-Scale Weakly Supervised Pre-Training for Detection](https://openaccess.thecvf.com/content/ICCV2021/papers/Ramanathan_PreDet_Large-Scale_Weakly_Supervised_Pre-Training_for_Detection_ICCV_2021_paper.pdf)
* [FMODetect: Robust Detection of Fast Moving Objects](https://arxiv.org/abs/2012.08216)
* [Multi-Source Domain Adaptation for Object Detection](https://arxiv.org/abs/2106.15793)
* [Self-Supervised Object Detection via Generative Image Synthesis](https://openaccess.thecvf.com/content/ICCV2021/papers/Mustikovela_Self-Supervised_Object_Detection_via_Generative_Image_Synthesis_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/NVlabs/SSOD)
* [Naturalistic Physical Adversarial Patch for Object Detectors](https://openaccess.thecvf.com/content/ICCV2021/papers/Hu_Naturalistic_Physical_Adversarial_Patch_for_Object_Detectors_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/aiiu-lab/Naturalistic-Adversarial-Patch)
* [Rethinking Transformer-Based Set Prediction for Object Detection](https://arxiv.org/abs/2011.10881)<br>:star:[code](https://github.com/Edward-Sun/TSP-Detection)
* [Detecting Invisible People](https://arxiv.org/abs/2012.08419)<br>:house:[project](http://www.cs.cmu.edu/~tkhurana/invisible.htm):tv:[video](https://youtu.be/StEfnshXrCE)
* [Dynamic DETR: End-to-End Object Detection With Dynamic Attention](https://openaccess.thecvf.com/content/ICCV2021/papers/Dai_Dynamic_DETR_End-to-End_Object_Detection_With_Dynamic_Attention_ICCV_2021_paper.pdf)
* [CrossDet: Crossline Representation for Object Detection](https://openaccess.thecvf.com/content/ICCV2021/papers/Qiu_CrossDet_Crossline_Representation_for_Object_Detection_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/QiuHeqian/CrossDet)
* [Robust Object Detection via Instance-Level Temporal Cycle Confusion](https://arxiv.org/abs/2104.08381)<br>:star:[code](https://github.com/xinw1012/cycle-confusion)
* [End-to-End Semi-Supervised Object Detection With Soft Teacher](https://arxiv.org/abs/2106.09018)<br>:star:[code](https://github.com/microsoft/SoftTeacher)
* [Parallel Rectangle Flip Attack: A Query-Based Black-Box Attack Against Object Detection](https://openaccess.thecvf.com/content/ICCV2021/papers/Liang_Parallel_Rectangle_Flip_Attack_A_Query-Based_Black-Box_Attack_Against_Object_ICCV_2021_paper.pdf)
* [Fooling LiDAR Perception via Adversarial Trajectory Perturbation](https://openaccess.thecvf.com/content/ICCV2021/papers/Li_Fooling_LiDAR_Perception_via_Adversarial_Trajectory_Perturbation_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/ai4ce/FLAT):house:[project](https://ai4ce.github.io/FLAT/)
* [TOOD: Task-Aligned One-Stage Object Detection](https://arxiv.org/abs/2108.07755)<br>:open_mouth:oral:star:[code](https://github.com/fcjian/TOOD)
* [Active Learning for Deep Object Detection via Probabilistic Modeling](https://arxiv.org/abs/2103.16130)<br>:star:[code](https://github.com/NVlabs/AL-MDN)
* [Dual Bipartite Graph Learning: A General Approach for Domain Adaptive Object Detection](https://openaccess.thecvf.com/content/ICCV2021/papers/Chen_Dual_Bipartite_Graph_Learning_A_General_Approach_for_Domain_Adaptive_ICCV_2021_paper.pdf)
* [WB-DETR: Transformer-Based Detector without Backbone](https://openaccess.thecvf.com/content/ICCV2021/papers/Liu_WB-DETR_Transformer-Based_Detector_Without_Backbone_ICCV_2021_paper.pdf)
* 3Dç›®æ ‡æ£€æµ‹
  * [Geometry Uncertainty Projection Network for Monocular 3D Object Detection](https://arxiv.org/abs/2107.13774)<br>:star:[code](https://github.com/SuperMHP/GUPNet)
  * [Fog Simulation on Real LiDAR Point Clouds for 3D Object Detection in Adverse Weather](https://arxiv.org/abs/2108.05249)<br>:star:[code](https://github.com/MartinHahner/LiDAR_fog_sim):house:[project](https://www.trace.ethz.ch/publications/2021/lidar_fog_simulation/)
  * [Is Pseudo-Lidar needed for Monocular 3D Object detection?](https://arxiv.org/abs/2108.06417)<br>:star:[code](https://github.com/TRI-ML/dd3d)
  * [RandomRooms: Unsupervised Pre-training from Synthetic Shapes and Randomized Layouts for 3D Object Detection](https://arxiv.org/abs/2108.07794)
  * [LIGA-Stereo: Learning LiDAR Geometry Aware Representations for Stereo-based 3D Detector](https://arxiv.org/abs/2108.08258)<br>:star:[code](https://github.com/xy-guo/LIGA-Stereo):house:[project](https://xy-guo.github.io/liga/)  
  * [Improving 3D Object Detection with Channel-wise Transformer](https://arxiv.org/abs/2108.10723)
  * [4D-Net for Learned Multi-Modal Alignment](https://arxiv.org/abs/2109.01066)
  * [Pyramid R-CNN: Towards Better Performance and Adaptability for 3D Object Detection](https://arxiv.org/abs/2109.02499)
  * [Voxel Transformer for 3D Object Detection](https://arxiv.org/abs/2109.02497)
  * [An End-to-End Transformer Model for 3D Object Detection](https://arxiv.org/abs/2109.08141)<br>:open_mouth:oral:star:[code](https://github.com/facebookresearch/3detr):house:[project](https://facebookresearch.github.io/3detr/)
  * [Unsupervised Domain Adaptive 3D Detection With Multi-Level Consistency](https://arxiv.org/abs/2107.11355)
  * [Group-Free 3D Object Detection via Transformers](https://arxiv.org/abs/2104.00678)<br>:star:[code](https://github.com/zeliu98/Group-Free-3D)
  * [VENet: Voting Enhancement Network for 3D Object Detection](https://openaccess.thecvf.com/content/ICCV2021/papers/Xie_VENet_Voting_Enhancement_Network_for_3D_Object_Detection_ICCV_2021_paper.pdf)
  * [Multi-Echo LiDAR for 3D Object Detection](https://openaccess.thecvf.com/content/ICCV2021/papers/Man_Multi-Echo_LiDAR_for_3D_Object_Detection_ICCV_2021_paper.pdf)
  * [Voxel Transformer for 3D Object Detection](https://arxiv.org/abs/2109.02497)
  * [RangeDet: In Defense of Range View for LiDAR-Based 3D Object Detection](https://arxiv.org/abs/2103.10039)<br>:star:[code](https://github.com/TuSimple/RangeDet)
  * [The Devil Is in the Task: Exploiting Reciprocal Appearance-Localization Features for Monocular 3D Object Detection](https://openaccess.thecvf.com/content/ICCV2021/papers/Zou_The_Devil_Is_in_the_Task_Exploiting_Reciprocal_Appearance-Localization_Features_ICCV_2021_paper.pdf)
  * [Gated3D: Monocular 3D Object Detection From Temporal Illumination Cues](https://openaccess.thecvf.com/content/ICCV2021/papers/Julca-Aguilar_Gated3D_Monocular_3D_Object_Detection_From_Temporal_Illumination_Cues_ICCV_2021_paper.pdf)<br>:house:[project](https://light.princeton.edu/publication/gated3d/)
  * [Are We Missing Confidence in Pseudo-LiDAR Methods for Monocular 3D Object Detection?](https://arxiv.org/abs/2012.05796)
  * [SPG: Unsupervised Domain Adaptation for 3D Object Detection via Semantic Point Generation](https://arxiv.org/abs/2108.06709)
  * [You Don't Only Look Once: Constructing Spatial-Temporal Memory for Integrated 3D Object Detection and Tracking](https://openaccess.thecvf.com/content/ICCV2021/papers/Sun_You_Dont_Only_Look_Once_Constructing_Spatial-Temporal_Memory_for_Integrated_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/zju3dv/UDOLO):house:[project](https://zju3dv.github.io/UDOLO/):tv:[video](https://youtu.be/Qn14m31siWA)
  * [Exploring Geometry-Aware Contrast and Clustering Harmonization for Self-Supervised 3D Object Detection](https://openaccess.thecvf.com/content/ICCV2021/papers/Liang_Exploring_Geometry-Aware_Contrast_and_Clustering_Harmonization_for_Self-Supervised_3D_Object_ICCV_2021_paper.pdf)
  * [AutoShape: Real-Time Shape-Aware Monocular 3D Object Detection](https://arxiv.org/abs/2108.11127)<br>:star:[code](https://github.com/zongdai/AutoShape)
  * [Geometry-Based Distance Decomposition for Monocular 3D Object Detection](https://openaccess.thecvf.com/content/ICCV2021/papers/Shi_Geometry-Based_Distance_Decomposition_for_Monocular_3D_Object_Detection_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/Rock-100/MonoDet)
* ç›®æ ‡å®šä½
  * [Contrastive Attention Maps for Self-supervised Co-localization](https://openaccess.thecvf.com/content/ICCV2021/papers/Ki_Contrastive_Attention_Maps_for_Self-Supervised_Co-Localization_ICCV_2021_paper.pdf)
  * [TS-CAM: Token Semantic Coupled Attention Map for Weakly Supervised Object Localization](https://openaccess.thecvf.com/content/ICCV2021/papers/Gao_TS-CAM_Token_Semantic_Coupled_Attention_Map_for_Weakly_Supervised_Object_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/vasgaowei/TS-CAM)
  * å¼±ç›‘ç£ç›®æ ‡å®šä½
    * [Normalization Matters in Weakly Supervised Object Localization](https://arxiv.org/abs/2107.13221)
    * [Online Refinement of Low-Level Feature Based Activation Map for Weakly Supervised Object Localization](https://openaccess.thecvf.com/content/ICCV2021/papers/Xie_Online_Refinement_of_Low-Level_Feature_Based_Activation_Map_for_Weakly_ICCV_2021_paper.pdf)
    * [Foreground Activation Maps for Weakly Supervised Object Localization](https://openaccess.thecvf.com/content/ICCV2021/papers/Meng_Foreground_Activation_Maps_for_Weakly_Supervised_Object_Localization_ICCV_2021_paper.pdf)
* Anomaly Detection(å›¾åƒå¼‚å¸¸æ£€æµ‹)
  * [Divide-and-Assemble: Learning Block-wise Memory for Unsupervised Anomaly Detection](https://arxiv.org/abs/2107.13118)
* å¼±ç›‘ç£ç›®æ ‡æ£€æµ‹
  * [Boosting Weakly Supervised Object Detection via Learning Bounding Box Adjusters](https://arxiv.org/abs/2108.01499)<br>:star:[code](https://github.com/DongSky/lbba_boosted_wsod)
  * [CaT: Weakly Supervised Object Detection With Category Transfer](https://arxiv.org/abs/2108.07487)<br>:star:[code](https://github.com/MediaBrain-SJTU/CaT)
* OOD æ£€æµ‹
  * [Triggering Failures: Out-Of-Distribution detection by learning from local adversarial attacks in Semantic Segmentation](https://arxiv.org/abs/2108.01634)<br>:star:[code](https://github.com/valeoai/obsnet)
* æ˜¾è‘—ç›®æ ‡æ£€æµ‹
  * [Disentangled High Quality Salient Object Detection](https://arxiv.org/abs/2108.03551)
  * [Specificity-preserving RGB-D Saliency Detection](https://arxiv.org/abs/2108.08162)<br>:star:[code](https://github.com/taozh2017/SPNet)
  * [Light Field Saliency Detection with Dual Local Graph Learning and Reciprocative Guidance](https://arxiv.org/abs/2108.06384)
  * [MFNet: Multi-Filter Directive Network for Weakly Supervised Salient Object Detection](https://openaccess.thecvf.com/content/ICCV2021/papers/Piao_MFNet_Multi-Filter_Directive_Network_for_Weakly_Supervised_Salient_Object_Detection_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/OIPLab-DUT/MFNet)
  * [Scene Context-Aware Salient Object Detection](https://openaccess.thecvf.com/content/ICCV2021/papers/Siris_Scene_Context-Aware_Salient_Object_Detection_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/SirisAvishek/Scene_Context_Aware_Saliency)
  * [Dynamic Context-Sensitive Filtering Network for Video Salient Object Detection](https://openaccess.thecvf.com/content/ICCV2021/papers/Zhang_Dynamic_Context-Sensitive_Filtering_Network_for_Video_Salient_Object_Detection_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/OIPLab-DUT/DCFNet)
  * [iNAS: Integral NAS for Device-Aware Salient Object Detection](https://openaccess.thecvf.com/content/ICCV2021/papers/Gu_iNAS_Integral_NAS_for_Device-Aware_Salient_Object_Detection_ICCV_2021_paper.pdf)<br>:house:[project](https://mmcheng.net/inas/)
  * RGB-Dæ˜¾è‘—ç›®æ ‡æ£€æµ‹
    * [RGB-D Saliency Detection via Cascaded Mutual Information Minimization](https://arxiv.org/abs/2109.07246)<br>:star:[code](https://github.com/JingZhang617/cascaded_rgbd_sod)
  * co-saliency detection
    * [Summarize and Search: Learning Consensus-aware Dynamic Convolution for Co-Saliency Detection](https://arxiv.org/abs/2110.00338)<br>:star:[code](https://github.com/nnizhang/CADC)
* è¿ç¦ç‰©å“æ£€æµ‹
  * [Towards Real-World Prohibited Item Detection: A Large-Scale X-ray Benchmark](https://arxiv.org/abs/2108.07020)<br>:sunflower:[dataset](https://github.com/bywang2018/security-dataset)
  * [Towards Real-World X-Ray Security Inspection: A High-Quality Benchmark and Lateral Inhibition Module for Prohibited Items Detection](https://openaccess.thecvf.com/content/ICCV2021/papers/Tao_Towards_Real-World_X-Ray_Security_Inspection_A_High-Quality_Benchmark_and_Lateral_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/HiXray-author/HiXray)
* å°æ ·æœ¬ç›®æ ‡æ£€æµ‹
  * [DeFRCN: Decoupled Faster R-CNN for Few-Shot Object Detection](https://arxiv.org/abs/2108.09017)<br>:star:[code](https://github.com/er-muyue/DeFRCN)
  * [Query Adaptive Few-Shot Object Detection With Heterogeneous Graph Convolutional Networks](https://openaccess.thecvf.com/content/ICCV2021/papers/Han_Query_Adaptive_Few-Shot_Object_Detection_With_Heterogeneous_Graph_Convolutional_Networks_ICCV_2021_paper.pdf)
  * [Universal-Prototype Enhancing for Few-Shot Object Detection](https://arxiv.org/abs/2103.01077)<br>:star:[code](https://github.com/AmingWu/UP-FSOD)
* è§†è§‰å…³ç³»ååŒå®šä½
  * [Few-shot Visual Relationship Co-localization](https://arxiv.org/abs/2108.11618)<br>:star:[code](https://github.com/vl2g/VRC):house:[project](https://vl2g.github.io/projects/vrc/)
* å¯†é›†ç›®æ ‡æ£€æµ‹
  * [Mutual Supervision for Dense Object Detection](https://arxiv.org/abs/2109.05986)<br>:star:[code](https://github.com/MCG-NJU)
* åŸŸé€‚åº”ç›®æ ‡æ£€æµ‹
  * [Seeking Similarities over Differences: Similarity-based Domain Alignment for Adaptive Object Detection](https://arxiv.org/abs/2110.01428)
  * [Knowledge Mining and Transferring for Domain Adaptive Object Detection](https://openaccess.thecvf.com/content/ICCV2021/papers/Tian_Knowledge_Mining_and_Transferring_for_Domain_Adaptive_Object_Detection_ICCV_2021_paper.pdf)
* å›¾åƒç¯¡æ”¹æ£€æµ‹
  * [Image Manipulation Detection by Multi-View Multi-Scale Supervision](https://arxiv.org/abs/2104.06832)<br>:star:[code](https://github.com/dong03/MVSS-Net)
* Visual Relationship Detection(VRDè§†è§‰å…³ç³»æ£€æµ‹)
  * [Grounding Consistency: Distilling Spatial Common Sense for Precise Visual Relationship Detection](https://openaccess.thecvf.com/content/ICCV2021/papers/Diomataris_Grounding_Consistency_Distilling_Spatial_Common_Sense_for_Precise_Visual_Relationship_ICCV_2021_paper.pdf)
* é•¿å°¾ç›®æ ‡æ£€æµ‹
  * [Exploring Classification Equilibrium in Long-Tailed Object Detection](http://arxiv.org/abs/2108.07507)<br>:star:[code](https://github.com/fcjian/LOCE)
  * [MosaicOS: A Simple and Effective Use of Object-Centric Images for Long-Tailed Object Detection](https://arxiv.org/abs/2102.08884)<br>:star:[code](https://github.com/czhang0528/MosaicOS/)
* Salient Object Ranking
  * [Salient Object Ranking with Position-Preserved Attention](https://arxiv.org/abs/2106.05047)<br>:star:[code](https://github.com/EricFH/SOR)
* å°ç›®æ ‡æ£€æµ‹
  * [Robust Small Object Detection on the Water Surface Through Fusion of Camera and Millimeter Wave Radar](https://openaccess.thecvf.com/content/ICCV2021/papers/Cheng_Robust_Small_Object_Detection_on_the_Water_Surface_Through_Fusion_ICCV_2021_paper.pdf)
* é»‘æš—ä¸­ç›®æ ‡æ£€æµ‹
  * [Multitask AET With Orthogonal Tangent Regularity for Dark Object Detection](https://openaccess.thecvf.com/content/ICCV2021/papers/Cui_Multitask_AET_With_Orthogonal_Tangent_Regularity_for_Dark_Object_Detection_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/cuiziteng/ICCV_MAET)
* 3D object prediction
  * [Holistic Pose Graph: Modeling Geometric Structure among Objects in a Scene using Graph Inference for 3D Object Prediction](https://openaccess.thecvf.com/content/ICCV2021/papers/Xiao_Holistic_Pose_Graph_Modeling_Geometric_Structure_Among_Objects_in_a_ICCV_2021_paper.pdf)<br>:star:[code](https://vipl.ict.ac.cn/view_database.php?id=6)
* å¤šç›®æ ‡æ£€æµ‹
  * [Training Multi-Object Detector by Estimating Bounding Box Distribution for Input Image](https://arxiv.org/abs/1911.12721)<br>:star:[code](https://github.com/yoojy31/MDOD)
* 3D object grounding
  * [Free-form Description Guided 3D Visual Graph Network for Object Grounding in Point Cloud](https://arxiv.org/abs/2103.16381)<br>:star:[code](https://github.com/PNXD/FFL-3DOG)
* ç»†ç²’åº¦è£‚çº¹æ£€æµ‹
  * [CrackFormer: Transformer Network for Fine-Grained Crack Detection](https://openaccess.thecvf.com/content/ICCV2021/papers/Liu_CrackFormer_Transformer_Network_for_Fine-Grained_Crack_Detection_ICCV_2021_paper.pdf)
* çº¿æ®µæ£€æµ‹
  * [ELSD: Efficient Line Segment Detector and Descriptor](https://arxiv.org/abs/2104.14205)
* ç»†èƒæ£€æµ‹ä¸åˆ†ç±»
  * [Multi-Class Cell Detection Using Spatial Context Representation](https://arxiv.org/abs/2110.04886)<br>:star:[code](https://github.com/TopoXLab/MCSpatNet)
* é˜´å½±æ£€æµ‹
  * [Mitigating Intensity Bias in Shadow Detection via Feature Decomposition and Reweighting](https://openaccess.thecvf.com/content/ICCV2021/papers/Zhu_Mitigating_Intensity_Bias_in_Shadow_Detection_via_Feature_Decomposition_and_ICCV_2021_paper.pdf)
* ç¤¾äº¤è·ç¦»æ£€æµ‹
  * [BEV-Net: Assessing Social Distancing Compliance by Joint People Localization and Geometric Reasoning](https://openaccess.thecvf.com/content/ICCV2021/papers/Dai_BEV-Net_Assessing_Social_Distancing_Compliance_by_Joint_People_Localization_and_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/daizhirui/BEVNet)
* ä¼ªè£…ç›®æ ‡æ£€æµ‹
  * [Uncertainty-Guided Transformer Reasoning for Camouflaged Object Detection](https://openaccess.thecvf.com/content/ICCV2021/papers/Yang_Uncertainty-Guided_Transformer_Reasoning_for_Camouflaged_Object_Detection_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/fanyang587/UGTR)

<a name="13"/>

## 13.Image Segmentation(å›¾åƒåˆ†å‰²)
* [Standardized Max Logits: A Simple yet Effective Approach for Identifying Unexpected Road Obstacles in Urban-Scene Segmentation](https://arxiv.org/abs/2107.11264)<br>:open_mouth:oral:star:[code](https://github.com/shjung13/Standardized-max-logits):tv:[video](https://www.youtube.com/watch?v=leBJZHzX6xM)
* [TransForensics: Image Forgery Localization with Dense Self-Attention](https://arxiv.org/abs/2108.03871)
* [From Contexts to Locality: Ultra-high Resolution Image Segmentation via Locality-aware Contextual Correlation](https://arxiv.org/abs/2109.02580)<br>:star:[code](https://github.com/liqiokkk/FCtL)
* [Labels4Free: Unsupervised Segmentation using StyleGAN](https://arxiv.org/abs/2103.14968)<br>:house:[project](https://rameenabdal.github.io/Labels4Free/):tv:[video](https://www.youtube.com/watch?v=_pHunGpvLVk)
* [Warp-Refine Propagation: Semi-Supervised Auto-labeling via Cycle-consistency](https://arxiv.org/abs/2109.13432)
* [Scaling up instance annotation via label propagation](https://arxiv.org/abs/2110.02277)<br>:star:[code](https://github.com/ethanweber/scaling-anno):house:[project](http://scaling-anno.csail.mit.edu/)
* [Robust Trust Region for Weakly Supervised Segmentation](https://arxiv.org/abs/2104.01948)<br>:star:[code](https://github.com/dmitrii-marin/robust_trust_region):tv:[video](https://drive.google.com/file/d/1MLd3c-fpm2K3hgYyWYFFxW3Ve8FznfD2/view)
* [HPNet: Deep Primitive Segmentation Using Hybrid Representations](https://arxiv.org/abs/2105.10620)<br>:star:[code](https://github.com/SimingYan/HPNet)
* [Weakly Supervised Segmentation of Small Buildings With Point Labels](https://openaccess.thecvf.com/content/ICCV2021/papers/Lee_Weakly_Supervised_Segmentation_of_Small_Buildings_With_Point_Labels_ICCV_2021_paper.pdf)
* [BAPA-Net: Boundary Adaptation and Prototype Alignment for Cross-Domain Semantic Segmentation](https://openaccess.thecvf.com/content/ICCV2021/papers/Liu_BAPA-Net_Boundary_Adaptation_and_Prototype_Alignment_for_Cross-Domain_Semantic_Segmentation_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/manmanjun/BAPA-Net)
* [Conditional Diffusion for Interactive Segmentation](https://openaccess.thecvf.com/content/ICCV2021/papers/Chen_Conditional_Diffusion_for_Interactive_Segmentation_ICCV_2021_paper.pdf)
* [Human Detection and Segmentation via Multi-view Consensus](https://arxiv.org/abs/2012.05119)<br>:star:[code](https://github.com/isinsukatircioglu/mvc)
* [Unidentified Video Objects: A Benchmark for Dense, Open-World Segmentation](https://arxiv.org/abs/2104.04691)
* [Enhanced Boundary Learning for Glass-Like Object Segmentation](https://arxiv.org/abs/2103.15734)<br>:star:[code](https://github.com/hehao13/EBLNet)
* [PARTS: Unsupervised segmentation with slots, attention and independence maximization](https://openaccess.thecvf.com/content/ICCV2021/papers/Zoran_PARTS_Unsupervised_Segmentation_With_Slots_Attention_and_Independence_Maximization_ICCV_2021_paper.pdf)
* [Predictive Feature Learning for Future Segmentation Prediction](https://openaccess.thecvf.com/content/ICCV2021/papers/Lin_Predictive_Feature_Learning_for_Future_Segmentation_Prediction_ICCV_2021_paper.pdf)
* [Perception-Aware Multi-Sensor Fusion for 3D LiDAR Semantic Segmentation](https://arxiv.org/abs/2106.15277)<br>:star:[code](https://github.com/ICEORY/PMF)
* [Segmenter: Transformer for Semantic Segmentation](https://arxiv.org/abs/2105.05633)<br>:star:[code](https://github.com/rstrudel/segmenter)
* [C3-SemiSeg: Contrastive Semi-Supervised Segmentation via Cross-Set Learning and Dynamic Class-Balancing](https://openaccess.thecvf.com/content/ICCV2021/papers/Zhou_C3-SemiSeg_Contrastive_Semi-Supervised_Segmentation_via_Cross-Set_Learning_and_Dynamic_Class-Balancing_ICCV_2021_paper.pdf)
* å…¨æ™¯åˆ†å‰²
  * [Panoptic Narrative Grounding](https://arxiv.org/abs/2109.04988)
* è¯­ä¹‰åˆ†å‰²
  * [Re-distributing Biased Pseudo Labels for Semi-supervised Semantic Segmentation: A Baseline Investigation](https://arxiv.org/abs/2107.11279)<br>:open_mouth:oral:star:[code](https://github.com/CVMI-Lab/DARS)
  * [Personalized Image Semantic Segmentation](https://arxiv.org/abs/2107.13978)<br>:star:[code](https://github.com/zhangyuygss/PIS)
  * [RECALL: Replay-based Continual Learning in Semantic Segmentation](https://arxiv.org/abs/2108.03673)<br>:star:[code](https://github.com/LTTM/RECALL)
  * [Deep Metric Learning for Open World Semantic Segmentation](https://arxiv.org/abs/2108.04562)
  * [LabOR: Labeling Only if Required for Domain Adaptive Semantic Segmentation](https://arxiv.org/abs/2108.05570)<br>:open_mouth:oral
  * [Dual Path Learning for Domain Adaptation of Semantic Segmentation](https://arxiv.org/abs/2108.06337)<br>:star:[code](https://github.com/royee182/DPL)
  * [Multi-Target Adversarial Frameworks for Domain Adaptation in Semantic Segmentation](https://arxiv.org/abs/2108.06962)
  * [Exploiting a Joint Embedding Space for Generalized Zero-Shot Semantic Segmentation](https://arxiv.org/abs/2108.06536)<br>:star:[code](https://github.com/cvlab-yonsei/JoEm):house:[project](https://cvlab.yonsei.ac.kr/projects/JoEm/)
  * [Multi-Anchor Active Domain Adaptation for Semantic Segmentation](https://arxiv.org/abs/2108.08012)<br>:open_mouth:oral:star:[code](https://github.com/munanning/MADA)
  * [Pixel Contrastive-Consistent Semi-Supervised Semantic Segmentation](https://arxiv.org/abs/2108.09025)
  * [Self-Regulation for Semantic Segmentation](https://arxiv.org/abs/2108.09702)<br>:star:[code](https://github.com/dongzhang89/SR-SS)
  * [ShapeConv: Shape-aware Convolutional Layer for Indoor RGB-D Semantic Segmentation](https://arxiv.org/abs/2108.10528)<br>:star:[code](https://github.com/hanchaoleng/ShapeConv)
  * [Generalize then Adapt: Source-Free Domain Adaptive Semantic Segmentation](https://arxiv.org/abs/2108.11249)<br>:house:[project](https://sites.google.com/view/sfdaseg)
  * [Mining Contextual Information Beyond Image for Semantic Segmentation](https://arxiv.org/abs/2108.11819)<br>:star:[code](https://github.com/CharlesPikachu/mcibi)
  * [ISNet: Integrate Image-Level and Semantic-Level Context for Semantic Segmentation](https://arxiv.org/abs/2108.12382)<br>:star:[code](https://github.com/SegmentationBLWX/sssegmentation)
  * [Pseudo-mask Matters in Weakly-supervised Semantic Segmentation](https://arxiv.org/abs/2108.12995)<br>:star:[code](https://github.com/Eli-YiLi/PMM)
  * [SIGN: Spatial-information Incorporated Generative Network for Generalized Zero-shot Semantic Segmentation](https://arxiv.org/abs/2108.12517)
  * [Region-Aware Contrastive Learning for Semantic Segmentation](https://openaccess.thecvf.com/content/ICCV2021/papers/Hu_Region-Aware_Contrastive_Learning_for_Semantic_Segmentation_ICCV_2021_paper.pdf)
  * [GP-S3Net: Graph-Based Panoptic Sparse Semantic Segmentation Network](https://openaccess.thecvf.com/content/ICCV2021/papers/Razani_GP-S3Net_Graph-Based_Panoptic_Sparse_Semantic_Segmentation_Network_ICCV_2021_paper.pdf)
  * [Domain Adaptive Semantic Segmentation With Self-Supervised Depth Estimation](https://openaccess.thecvf.com/content/ICCV2021/papers/Wang_Domain_Adaptive_Semantic_Segmentation_With_Self-Supervised_Depth_Estimation_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/qinenergy/corda)
  * [Scribble-Supervised Semantic Segmentation by Uncertainty Reduction on Neural Representation and Self-Supervision on Neural Eigenspace](https://arxiv.org/abs/2102.09896)
  * [Exploring Cross-Image Pixel Contrast for Semantic Segmentation](https://arxiv.org/abs/2101.11939)<br>:open_mouth:oral:star:[code](https://github.com/tfzhou/ContrastiveSeg)
  * [Dynamic Divide-and-Conquer Adversarial Training for Robust Semantic Segmentation](https://arxiv.org/abs/2003.06555)<br>:star:[code](https://github.com/dvlab-research/Robust-Semantic-Segmentation)
  * [Uncertainty-Aware Pseudo Label Refinery for Domain Adaptive Semantic Segmentation](https://openaccess.thecvf.com/content/ICCV2021/papers/Wang_Uncertainty-Aware_Pseudo_Label_Refinery_for_Domain_Adaptive_Semantic_Segmentation_ICCV_2021_paper.pdf)
  * [Contrastive Learning for Label Efficient Semantic Segmentation](https://arxiv.org/abs/2012.06985)
  * [Scaling Semantic Segmentation Beyond 1K Classes on a Single GPU](http://arxiv.org/abs/2012.07489):star:[code](https://github.com/shipra25jain/ESSNet)
  * [Prototypical Matching and Open Set Rejection for Zero-Shot Semantic Segmentation](https://openaccess.thecvf.com/content/ICCV2021/papers/Zhang_Prototypical_Matching_and_Open_Set_Rejection_for_Zero-Shot_Semantic_Segmentation_ICCV_2021_paper.pdf)
  * [Geometric Unsupervised Domain Adaptation for Semantic Segmentation](https://arxiv.org/abs/2103.16694)
  * [Calibrated Adversarial Refinement for Stochastic Semantic Segmentation](https://arxiv.org/abs/2006.13144)<br>:star:[code](https://github.com/EliasKassapis/CARSSS)
  * [Multi-View Radar Semantic Segmentation](https://openaccess.thecvf.com/content/ICCV2021/papers/Ouaknine_Multi-View_Radar_Semantic_Segmentation_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/valeoai/MVRSS)
  * [Exploring Robustness of Unsupervised Domain Adaptation in Semantic Segmentation](https://arxiv.org/abs/2105.10843)<br>:open_mouth:oral:star:[code](https://github.com/uta-smile/ASSUDA)
  * [Specialize and Fuse: Pyramidal Output Representation for Semantic Segmentation](https://arxiv.org/abs/2108.01866)
  * [Unsupervised Semantic Segmentation by Contrasting Object Mask Proposals](https://arxiv.org/abs/2102.06191)<br>:star:[code](https://github.com/wvangansbeke/Unsupervised-Semantic-Segmentation)
  * [Scribble-Supervised Semantic Segmentation Inference](https://openaccess.thecvf.com/content/ICCV2021/papers/Xu_Scribble-Supervised_Semantic_Segmentation_Inference_ICCV_2021_paper.pdf)
  * [Semi-Supervised Semantic Segmentation With Pixel-Level Contrastive Learning From a Class-Wise Memory Bank](https://arxiv.org/abs/2104.13415)<br>:star:[code](https://github.com/Shathe/SemiSeg-Contrastive)
  * å°æ ·æœ¬è¯­ä¹‰åˆ†å‰²
    * [Simpler is Better: Few-shot Semantic Segmentation with Classifier Weight Transformer](https://arxiv.org/abs/2108.03032)<br>:star:[code](https://github.com/zhiheLu/CWT-for-FSS)
    * [Learning Meta-class Memory for Few-Shot Semantic Segmentation](https://arxiv.org/abs/2108.02958)
    * [Few-Shot Semantic Segmentation With Cyclic Memory Network](https://openaccess.thecvf.com/content/ICCV2021/papers/Xie_Few-Shot_Semantic_Segmentation_With_Cyclic_Memory_Network_ICCV_2021_paper.pdf)
  * 3Dè¯­ä¹‰åˆ†å‰²
    * [VMNet: Voxel-Mesh Network for Geodesic-Aware 3D Semantic Segmentation](https://arxiv.org/abs/2107.13824)<br>:open_mouth:oral:star:[code](https://github.com/hzykent/VMNet)
    * [Sparse-to-Dense Feature Matching: Intra and Inter Domain Cross-Modal Learning in Domain Adaptation for 3D Semantic Segmentation](https://arxiv.org/abs/2107.14724)<br>:star:[code](https://github.com/leolyj/DsCML)
    * [Weakly Supervised 3D Semantic Segmentation Using Cross-Image Consensus and Inter-Voxel Affinity Relations](https://openaccess.thecvf.com/content/ICCV2021/papers/Zhu_Weakly_Supervised_3D_Semantic_Segmentation_Using_Cross-Image_Consensus_and_Inter-Voxel_ICCV_2021_paper.pdf)
  * è§†é¢‘è¯­ä¹‰åˆ†å‰²
    * [Domain Adaptive Video Segmentation via Temporal Consistency Regularization](https://arxiv.org/abs/2107.11004)<br>:star:[code](https://github.com/Dayan-Guan/DA-VSN)
  * å¼±ç›‘ç£è¯­ä¹‰åˆ†å‰²
    * [Leveraging Auxiliary Tasks with Affinity Learning for Weakly Supervised Semantic Segmentation](https://arxiv.org/abs/2107.11787)<br>:star:[code](https://github.com/xulianuwa/AuxSegNet)
    * [Complementary Patch for Weakly Supervised Semantic Segmentation](https://arxiv.org/abs/2108.03852)
    * [ECS-Net: Improving Weakly Supervised Semantic Segmentation by Using Connections Between Class Activation Maps](https://openaccess.thecvf.com/content/ICCV2021/papers/Sun_ECS-Net_Improving_Weakly_Supervised_Semantic_Segmentation_by_Using_Connections_Between_ICCV_2021_paper.pdf)
    * [Unlocking the Potential of Ordinary Classifier: Class-Specific Adversarial Erasing Framework for Weakly Supervised Semantic Segmentation](https://openaccess.thecvf.com/content/ICCV2021/papers/Kweon_Unlocking_the_Potential_of_Ordinary_Classifier_Class-Specific_Adversarial_Erasing_Framework_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/KAIST-vilab/OC-CSE)
    * [Context Decoupling Augmentation for Weakly Supervised Semantic Segmentation](https://arxiv.org/abs/2103.01795)<br>:star:[code](https://github.com/suyukun666/CDA)
    * [Seminar Learning for Click-Level Weakly Supervised Semantic Segmentation](https://arxiv.org/abs/2108.13393)
  * ç‚¹äº‘è¯­ä¹‰åˆ†å‰²
    * [ReDAL: Region-based and Diversity-aware Active Learning for Point Cloud Semantic Segmentation](https://arxiv.org/abs/2107.11769)<br>:tv:[video](https://www.youtube.com/watch?v=XJeb9kMxs5E)
    * [Perturbed Self-Distillation: Weakly Supervised Large-Scale Point Cloud Semantic Segmentation](https://openaccess.thecvf.com/content/ICCV2021/papers/Zhang_Perturbed_Self-Distillation_Weakly_Supervised_Large-Scale_Point_Cloud_Semantic_Segmentation_ICCV_2021_paper.pdf)
    * [TempNet: Online Semantic Segmentation on Large-Scale Point Cloud Series](https://openaccess.thecvf.com/content/ICCV2021/papers/Zhou_TempNet_Online_Semantic_Segmentation_on_Large-Scale_Point_Cloud_Series_ICCV_2021_paper.pdf)
    * [Guided Point Contrastive Learning for Semi-Supervised Point Cloud Semantic Segmentation](https://openaccess.thecvf.com/content/ICCV2021/papers/Jiang_Guided_Point_Contrastive_Learning_for_Semi-Supervised_Point_Cloud_Semantic_Segmentation_ICCV_2021_paper.pdf)
    * [Learning With Noisy Labels for Robust Point Cloud Segmentation](https://arxiv.org/abs/2107.14230)<br>:star:[code](https://github.com/pleaseconnectwifi/PNAL):house:[project](https://shuquanye.com/PNAL_website/)
  * OOD
    * [Entropy Maximization and Meta Classification for Out-of-Distribution Detection in Semantic Segmentation](https://arxiv.org/abs/2012.06575)<br>:star:[code](https://github.com/robin-chan/meta-ood)
* å®ä¾‹åˆ†å‰²
  * [Rank & Sort Loss for Object Detection and Instance Segmentation](https://arxiv.org/abs/2107.11669)<br>:open_mouth:oral:star:[code](https://github.com/kemaloksuz/RankSortLoss)
  * [SOTR: Segmenting Objects with Transformers](https://arxiv.org/abs/2108.06747)<br>:star:[code](https://github.com/easton-cau/SOTR)
  * [A Weakly Supervised Amodal Segmenter with Boundary Uncertainty Estimation](https://arxiv.org/abs/2108.09897)
  * [Instances as Queries](https://arxiv.org/abs/2105.01928)<br>:star:[code](https://github.com/hustvl/QueryInst):tv:[video](https://www.youtube.com/watch?v=3Fqwvn6_oUQ)
  * [CrossVIS: Crossover Learning for Fast Online Video Instance Segmentation](https://arxiv.org/abs/2104.05970)<br>:star:[code](https://github.com/hustvl/CrossVIS):tv:[video](https://www.youtube.com/watch?v=tPvYYjTgaNs)
  * [CDNet: Centripetal Direction Network for Nuclear Instance Segmentation](https://openaccess.thecvf.com/content/ICCV2021/papers/He_CDNet_Centripetal_Direction_Network_for_Nuclear_Instance_Segmentation_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/honglianghe/CDNet)
  * [PrimitiveNet: Primitive Instance Segmentation With Local Primitive Embedding Under Adversarial Metric](https://openaccess.thecvf.com/content/ICCV2021/papers/Huang_PrimitiveNet_Primitive_Instance_Segmentation_With_Local_Primitive_Embedding_Under_Adversarial_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/hjwdzh/PrimitiveNet)
  * [FASA: Feature Augmentation and Sampling Adaptation for Long-Tailed Instance Segmentation](https://arxiv.org/abs/2102.12867)<br>:star:[code](https://github.com/yuhangzang/FASA):house:[project](https://www.mmlab-ntu.com/project/fasa/index.html)
  * [Prior to Segment: Foreground Cues for Weakly Annotated Classes in Partially Supervised Instance Segmentation](http://arxiv.org/abs/2011.11787)<br>:star:[code](https://github.com/dbtmpl/OPMask)
  * [DiscoBox: Weakly Supervised Instance Segmentation and Semantic Correspondence From Box Supervision](https://arxiv.org/abs/2105.06464)
  * [End-to-End Video Instance Segmentation via Spatial-Temporal Graph Neural Networks](https://openaccess.thecvf.com/content/ICCV2021/papers/Wang_End-to-End_Video_Instance_Segmentation_via_Spatial-Temporal_Graph_Neural_Networks_ICCV_2021_paper.pdf)
  * [The Surprising Impact of Mask-Head Architecture on Novel Class Segmentation](https://arxiv.org/abs/2104.00613)<br>:house:[project](https://google.github.io/deepmac/)
  * [How Shift Equivariance Impacts Metric Learning for Instance Segmentation](https://arxiv.org/abs/2101.05846)<br>:star:[code](https://github.com/Kainmueller-Lab/shift_equivariance_unet)
  * [Parallel Detection-and-Segmentation Learning for Weakly Supervised Instance Segmentation](https://openaccess.thecvf.com/content/ICCV2021/papers/Shen_Parallel_Detection-and-Segmentation_Learning_for_Weakly_Supervised_Instance_Segmentation_ICCV_2021_paper.pdf)
  * [Real-Time Instance Segmentation With Discriminative Orientation Maps](https://arxiv.org/abs/2106.12204)<br>:star:[code](https://github.com/duwt/OrienMask)
  * è§†é¢‘å®ä¾‹åˆ†å‰²
    * [Video Instance Segmentation with a Propose-Reduce Paradigm](https://arxiv.org/abs/2103.13746)<br>:star:[code](https://github.com/dvlab-research/ProposeReduce)
  * 3Då®ä¾‹åˆ†å‰²
    * [Hierarchical Aggregation for 3D Instance Segmentation](https://arxiv.org/abs/2108.02350)<br>:star:[code](https://github.com/hustvl/HAIS)
* å°æ ·æœ¬åˆ†å‰²
  * [Mining Latent Classes for Few-shot Segmentation](https://arxiv.org/abs/2103.15402)<br>:open_mouth:oral:star:[code](https://github.com/LiheYoung/MiningFSS)
* Human Motion Segmentation(äººä½“è¿åŠ¨åˆ†å‰²)
  * [Graph Constrained Data Representation Learning for Human Motion Segmentation](https://arxiv.org/abs/2107.13362)<br>:star:[code](https://github.com/mdimiccoli/GCRL-for-HMS/)
  * [Hypercorrelation Squeeze for Few-Shot Segmenation](https://openaccess.thecvf.com/content/ICCV2021/papers/Min_Hypercorrelation_Squeeze_for_Few-Shot_Segmenation_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/juhongm999/hsnet):house:[project](http://cvlab.postech.ac.kr/research/HSNet/)
* ç‚¹äº‘åˆ†å‰²
  * [Learning with Noisy Labels for Robust Point Cloud Segmentation](https://arxiv.org/abs/2107.14230)<br>:open_mouth:oral:star:[code](https://github.com/pleaseconnectwifi/PNAL):house:[project](https://shuquanye.com/PNAL_website/)
  * [DRINet: A Dual-Representation Iterative Learning Network for Point Cloud Segmentation](https://arxiv.org/abs/2108.04023)
  * [RPVNet: A Deep and Efficient Range-Point-Voxel Fusion Network for LiDAR Point Cloud Segmentation](https://arxiv.org/abs/2103.12978)
* è§†é¢‘ç›®æ ‡åˆ†å‰²(VOS)
  * [Full-Duplex Strategy for Video Object Segmentation](https://arxiv.org/abs/2108.03151)<br>:house:[project](http://dpfan.net/FSNet/)
  * [Joint Inductive and Transductive Learning for Video Object Segmentation](https://arxiv.org/abs/2108.03679)<br>:star:[code](https://github.com/maoyunyao/JOINT)
  * [Hierarchical Memory Matching Network for Video Object Segmentation](https://arxiv.org/abs/2109.11404)<br>:star:[code](https://github.com/Hongje/HMMN)
  * [Self-supervised Video Object Segmentation by Motion Grouping](https://arxiv.org/abs/2104.07658)<br>:star:[code](https://github.com/charigyang/motiongrouping):house:[project](https://charigyang.github.io/motiongroup/):tv:[video](https://www.youtube.com/watch?v=Q0dLExLXZIw)
  * [Deep Transport Network for Unsupervised Video Object Segmentation](https://openaccess.thecvf.com/content/ICCV2021/papers/Zhang_Deep_Transport_Network_for_Unsupervised_Video_Object_Segmentation_ICCV_2021_paper.pdf)
  * [Generating Masks From Boxes by Mining Spatio-Temporal Consistencies in Videos](https://arxiv.org/abs/2101.02196)<br>:star:[code](https://github.com/visionml/pytracking)
  * [Learning Motion-Appearance Co-Attention for Zero-Shot Video Object Segmentation](https://openaccess.thecvf.com/content/ICCV2021/papers/Yang_Learning_Motion-Appearance_Co-Attention_for_Zero-Shot_Video_Object_Segmentation_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/isyangshu/AMC-Net)
  * [Video Object Segmentation With Dynamic Memory Networks and Adaptive Object Alignment](https://openaccess.thecvf.com/content/ICCV2021/papers/Liang_Video_Object_Segmentation_With_Dynamic_Memory_Networks_and_Adaptive_Object_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/liang4sx/DMN-AOA)
* è¯­ä¹‰åœºæ™¯åˆ†å‰² 
  * [BiMaL: Bijective Maximum Likelihood Approach to Domain Adaptation in Semantic Scene Segmentation](https://arxiv.org/abs/2108.03267)<br>:star:[code](https://github.com/uark-cviu/BiMaL)
* Referring Segmentation(åŸºäºæ–‡æœ¬çš„åˆ†å‰²) 
  * [Vision-Language Transformer and Query Generation for Referring Segmentation](https://arxiv.org/abs/2108.05565)<br>:star:[code](https://github.com/henghuiding/Vision-Language-Transformer)
* åœºæ™¯ç†è§£
  * [DeepPanoContext: Panoramic 3D Scene Understanding with Holistic Scene Context Graph and Relation-based Optimization](https://arxiv.org/abs/2108.10743)<br>:open_mouth:oral:star:[code](https://github.com/chengzhag/DeepPanoContext):house:[project](https://chengzhag.github.io/publication/dpc/):tv:[video](https://youtu.be/mO1EtUHnX4w)
  * [ACDC: The Adverse Conditions Dataset With Correspondences for Semantic Driving Scene Understanding](https://openaccess.thecvf.com/content/ICCV2021/papers/Sakaridis_ACDC_The_Adverse_Conditions_Dataset_With_Correspondences_for_Semantic_Driving_ICCV_2021_paper.pdf)<br>:house:[project](https://acdc.vision.ee.ethz.ch/)
  * [Hypersim: A Photorealistic Synthetic Dataset for Holistic Indoor Scene Understanding](https://arxiv.org/abs/2011.02523)<br>:star:[code](https://github.com/apple/ml-hypersim)
* CMA  
  * [Towards Better Explanations of Class Activation Mapping](https://arxiv.org/abs/2102.05228)
  * [Keep CALM and Improve Visual Feature Attribution](https://arxiv.org/abs/2106.07861)<br>:star:[code](https://github.com/naver-ai/calm)
  * [LFI-CAM: Learning Feature Importance for Better Visual Explanation](https://openaccess.thecvf.com/content/ICCV2021/papers/Lee_LFI-CAM_Learning_Feature_Importance_for_Better_Visual_Explanation_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/TrustworthyAI-kr/LFI-CAM)
* å¤šç›®æ ‡åˆ†å‰²
  * [Faster Multi-Object Segmentation Using Parallel Quadratic Pseudo-Boolean Optimization](https://openaccess.thecvf.com/content/ICCV2021/papers/Jeppesen_Faster_Multi-Object_Segmentation_Using_Parallel_Quadratic_Pseudo-Boolean_Optimization_ICCV_2021_paper.pdf)
* åŠ¨ä½œåˆ†å‰²
  * [Weakly-Supervised Action Segmentation and Alignment via Transcript-Aware Union-of-Subspaces Learning](https://openaccess.thecvf.com/content/ICCV2021/papers/Lu_Weakly-Supervised_Action_Segmentation_and_Alignment_via_Transcript-Aware_Union-of-Subspaces_Learning_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/ZijiaLewisLu/ICCV21-TASL)
  * [Refining Action Segmentation with Hierarchical Video Representations](https://openaccess.thecvf.com/content/ICCV2021/papers/Ahn_Refining_Action_Segmentation_With_Hierarchical_Video_Representations_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/cotton-ahn/HASR_iccv2021)
* åœºæ™¯è§£æ
  * [Interaction via Bi-Directional Graph of Semantic Region Affinity for Scene Parsing](https://openaccess.thecvf.com/content/ICCV2021/papers/Ding_Interaction_via_Bi-Directional_Graph_of_Semantic_Region_Affinity_for_Scene_ICCV_2021_paper.pdf)
* æŠ å›¾
  * [Cascade Image Matting With Deformable Graph Refinement](https://arxiv.org/abs/2105.02646)
  * [Tripartite Information Mining and Integration for Image Matting](https://openaccess.thecvf.com/content/ICCV2021/papers/Liu_Tripartite_Information_Mining_and_Integration_for_Image_Matting_ICCV_2021_paper.pdf)
* è¿åŠ¨åˆ†å‰²
  * [SLIM: Self-Supervised LiDAR Scene Flow and Motion Segmentation](https://openaccess.thecvf.com/content/ICCV2021/papers/Baur_SLIM_Self-Supervised_LiDAR_Scene_Flow_and_Motion_Segmentation_ICCV_2021_paper.pdf)

<a name="12"/>

## 12.Image/Fine-Grained Classification(å›¾åƒ/ç»†ç²’åº¦åˆ†ç±») 
* [DiagViB-6: A Diagnostic Benchmark Suite for Vision Models in the Presence of Shortcut and Generalization Opportunities](https://arxiv.org/abs/2108.05779)
* [Online Continual Learning For Visual Food Classification](https://arxiv.org/abs/2108.06781)
* [A Unified Objective for Novel Class Discovery](https://arxiv.org/abs/2108.08536)<br>:open_mouth:oral:star:[code](https://github.com/DonkeyShot21/UNO):house:[project](https://ncd-uno.github.io/)<br>:newspaper:è§£è¯»:[ICCV2021 Oral | UNOï¼šç”¨äºâ€œæ–°ç±»å‘ç°â€çš„ç»Ÿä¸€ç›®æ ‡å‡½æ•°ï¼Œç®€åŒ–è®­ç»ƒæµç¨‹ï¼å·²å¼€æºï¼](https://mp.weixin.qq.com/s/3aQ5AUKOAnO7kDtsxhRJ3Q)
* [Improving Generalization of Batch Whitening by Convolutional Unit Optimization](https://arxiv.org/abs/2108.10629)<br>:star:[code](https://github.com/YooshinCho/pytorch_ConvUnitOptimization)
* [Towards Learning Spatially Discriminative Feature Representations](https://arxiv.org/abs/2109.01359)
* [CrossViT: Cross-Attention Multi-Scale Vision Transformer for Image Classification](https://arxiv.org/abs/2103.14899)<br>:star:[code](https://github.com/IBM/CrossViT)<br>:newspaper:è§£è¯»:[ICCV2021 MIT-IBMæ²ƒæ£®å¼€æºCrossViTï¼šTransformerèµ°å‘å¤šåˆ†æ”¯ã€å¤šå°ºåº¦](https://mp.weixin.qq.com/s/aqDaF4Iy96Nx1pvX__6vHg)
* [SCOUTER: Slot Attention-based Classifier for Explainable Image Recognition](https://arxiv.org/abs/2009.06138)<br>:star:[code](https://github.com/wbw520/scouter)
* [Influence-Balanced Loss for Imbalanced Visual Classification](https://arxiv.org/abs/2110.02444)<br>:star:[code](https://github.com/pseulki/IB-Loss) 
* [Explanations for Occluded Images](https://arxiv.org/abs/2103.03622)<br>:star:[code](https://github.com/theyoucheng/deepcover):house:[project](https://www.cprover.org/deepcover/iccv2021/):tv:[video](https://www.cprover.org/deepcover/iccv2021/iccv2021-talk-compatible.mp4)
* [Understanding Robustness of Transformers for Image Classification](http://arxiv.org/abs/2103.14586)
* [Learning Rare Category Classifiers on a Tight Labeling Budget](https://openaccess.thecvf.com/content/ICCV2021/papers/Mullapudi_Learning_Rare_Category_Classifiers_on_a_Tight_Labeling_Budget_ICCV_2021_paper.pdf)
* [Discover the Unknown Biased Attribute of an Image Classifier](https://arxiv.org/abs/2104.14556)<br>:star:[code](https://github.com/hubertlee915/discover_unknown_biases)
* [Co-Scale Conv-Attentional Image Transformers](https://arxiv.org/abs/2104.06399)<br>:open_mouth:oral:star:[code](https://github.com/mlpc-ucsd/CoaT)
* [Benchmark Platform for Ultra-Fine-Grained Visual Categorization Beyond Human Performance](https://openaccess.thecvf.com/content/ICCV2021/papers/Yu_Benchmark_Platform_for_Ultra-Fine-Grained_Visual_Categorization_Beyond_Human_Performance_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/XiaohanYu-GU/Ultra-FGVC)
* [Do Image Classifiers Generalize Across Time?](https://arxiv.org/abs/1906.02168)<br>:house:[project](https://modestyachts.github.io/natural-perturbations-website/)
* [Interpretable Image Recognition by Constructing Transparent Embedding Space](https://openaccess.thecvf.com/content/ICCV2021/papers/Wang_Interpretable_Image_Recognition_by_Constructing_Transparent_Embedding_Space_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/JackeyWang96/TesNet)
* [The Pursuit of Knowledge: Discovering and Localizing Novel Categories using Dual Memory](https://arxiv.org/abs/2105.01652)
* é•¿å°¾è¯†åˆ«
  * [Parametric Contrastive Learning](https://arxiv.org/abs/2107.12028)<br>:star:[code](https://github.com/jiequancui/Parametric-Contrastive-Learning)
  * [ACE: Ally Complementary Experts for Solving Long-Tailed Recognition in One-Shot](https://arxiv.org/abs/2108.02385)<br>:open_mouth:oral:star:[code](https://github.com/jrcai/ACE)
  * [Self Supervision to Distillation for Long-Tailed Visual Recognition](https://arxiv.org/abs/2109.04075)<br>:star:[code](https://github.com/MCG-NJU)
  * [Distilling Virtual Examples for Long-Tailed Recognition](https://arxiv.org/abs/2103.15042)
  * [Distributional Robustness Loss for Long-Tail Learning](https://arxiv.org/abs/2104.03066)
  * [GistNet: A Geometric Structure Transfer Network for Long-Tailed Recognition](https://arxiv.org/abs/2105.00131)
  * é•¿å°¾è§†è§‰å…³ç³»è¯†åˆ«
    * [Exploring Long Tail Visual Relationship Recognition With Large Vocabulary](https://arxiv.org/abs/2004.00436)<br>:star:[code](https://github.com/Vision-CAIR/LTVRR)
* ç»†ç²’åº¦
  * [Webly Supervised Fine-Grained Recognition: Benchmark Datasets and An Approach](https://arxiv.org/abs/2108.02399)<br>:star:[code](https://github.com/NUST-Machine-Intelligence-Laboratory/weblyFG-dataset)
  * [Learning Canonical 3D Object Representation for Fine-Grained Recognition](https://arxiv.org/abs/2108.04628)
  * [Counterfactual Attention Learning for Fine-Grained Visual Categorization and Re-identification](https://arxiv.org/abs/2108.08728)<br>:star:[code](https://github.com/raoyongming/CAL)
  * [N-ImageNet: Towards Robust, Fine-Grained Object Recognition With Event Cameras](https://openaccess.thecvf.com/content/ICCV2021/papers/Kim_N-ImageNet_Towards_Robust_Fine-Grained_Object_Recognition_With_Event_Cameras_ICCV_2021_paper.pdf)
  * [Grafit: Learning fine-grained image representations with coarse labels](https://arxiv.org/abs/2011.12982)
  * [Stochastic Partial Swap: Enhanced Model Generalization and Interpretability for Fine-Grained Recognition](https://openaccess.thecvf.com/content/ICCV2021/papers/Huang_Stochastic_Partial_Swap_Enhanced_Model_Generalization_and_Interpretability_for_Fine-Grained_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/Shaoli-Huang/SPS)
* å°æ ·æœ¬åˆ†ç±»
  * [Transductive Few-Shot Classification on the Oblique Manifold](https://arxiv.org/abs/2108.04009)
  * [Relational Embedding for Few-Shot Classification](https://arxiv.org/abs/2108.09666)<br>:star:[code](https://github.com/dahyun-kang/renet):house:[project](http://cvlab.postech.ac.kr/research/RENet)
  * [Binocular Mutual Learning for Improving Few-shot Classification](https://arxiv.org/abs/2108.12104)<br>:star:[code](https://github.com/ZZQzzq/BML)
  * [Partner-Assisted Learning for Few-Shot Image Classification](https://arxiv.org/abs/2109.07607)
  * [On the Importance of Distractors for Few-Shot Classification](https://arxiv.org/abs/2109.09883)<br>:star:[code](https://github.com/quantacode/Contrastive-Finetuning)
  * [Few-Shot Image Classification: Just Use a Library of Pre-Trained Feature Extractors and a Simple Classifier](https://arxiv.org/abs/2101.00562)
  * [Universal Representation Learning From Multiple Domains for Few-Shot Classification](https://arxiv.org/abs/2103.13841)<br>:star:[code](https://github.com/VICO-UoE/URL)
  * [A Multi-Mode Modulator for Multi-Domain Few-Shot Classification](https://openaccess.thecvf.com/content/ICCV2021/papers/Liu_A_Multi-Mode_Modulator_for_Multi-Domain_Few-Shot_Classification_ICCV_2021_paper.pdf)
  * [Variational Feature Disentangling for Fine-Grained Few-Shot Classification](https://arxiv.org/abs/2010.03255)<br>:star:[code](https://github.com/cvlab-stonybrook/vfd-iccv21)
  * [Mixture-Based Feature Space Learning for Few-Shot Image Classification](https://openaccess.thecvf.com/content/ICCV2021/papers/Afrasiyabi_Mixture-Based_Feature_Space_Learning_for_Few-Shot_Image_Classification_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/ArmanAfrasiyabi/MixtFSL-fs):house:[project](https://lvsn.github.io/MixtFSL/):tv:[video](https://www.youtube.com/embed/DxMCHCQKDCQ)
* å¤šæ ‡ç­¾åˆ†ç±»
  * [Asymmetric Loss For Multi-Label Classification](https://arxiv.org/abs/2009.14119)<br>:star:[code](https://github.com/Alibaba-MIIL/ASL)
  * [Semantic Diversity Learning for Zero-Shot Multi-label Classification](https://arxiv.org/abs/2105.05926)<br>:star:[code](https://github.com/Alibaba-MIIL/ZS_SDL)

  
<a name="11"/>

## 11.Visual Question Answering(è§†è§‰é—®ç­”)
* [Greedy Gradient Ensemble for Robust Visual Question Answering](https://arxiv.org/abs/2107.12651)<br>:star:[code](https://github.com/GeraldHan/GGE)
* [Weakly Supervised Relative Spatial Reasoning for Visual Question Answering](https://arxiv.org/abs/2109.01934)<br>:star:[code](https://github.com/pratyay-banerjee/weak_sup_vqa)
* [Calibrating Concepts and Operations: Towards Symbolic Reasoning on Real Images](https://arxiv.org/abs/2110.00519)<br>:star:[code](https://github.com/Lizw14/CaliCO)
* [Unshuffling Data for Improved Generalization in Visual Question Answering](https://openaccess.thecvf.com/content/ICCV2021/papers/Teney_Unshuffling_Data_for_Improved_Generalization_in_Visual_Question_Answering_ICCV_2021_paper.pdf)
* [TRAR: Routing the Attention Spans in Transformer for Visual Question Answering](https://openaccess.thecvf.com/content/ICCV2021/papers/Zhou_TRAR_Routing_the_Attention_Spans_in_Transformer_for_Visual_Question_ICCV_2021_paper.pdf)(https://github.com/rentainhe/TRAR-VQA/)
* [Contrast and Classify: Training Robust VQA Models](https://arxiv.org/abs/2010.06087)
* [Linguistically Routing Capsule Network for Out-of-Distribution Visual Question Answering](https://openaccess.thecvf.com/content/ICCV2021/papers/Cao_Linguistically_Routing_Capsule_Network_for_Out-of-Distribution_Visual_Question_Answering_ICCV_2021_paper.pdf)
* [Beyond Question-Based Biases: Assessing Multimodal Shortcut Learning in Visual Question Answering](https://arxiv.org/abs/2104.03149)<br>:star:[code](https://github.com/cdancette/detect-shortcuts)
* [Auto-Parsing Network for Image Captioning and Visual Question Answering](https://arxiv.org/abs/2108.10568)
* video question answering
  * [Just Ask: Learning to Answer Questions from Millions of Narrated Videos](https://arxiv.org/abs/2012.00451)<br>:open_mouth:oral:star:[code](https://github.com/antoyang/just-ask):house:[project](https://antoyang.github.io/just-ask.html)
  * [Adversarial VQA: A New Benchmark for Evaluating the Robustness of VQA Models](https://arxiv.org/abs/2106.00245)<br>:house:[project](https://adversarialvqa.github.io/)
  * [Env-QA: A Video Question Answering Benchmark for Comprehensive Understanding of Dynamic Environments](https://openaccess.thecvf.com/content/ICCV2021/papers/Gao_Env-QA_A_Video_Question_Answering_Benchmark_for_Comprehensive_Understanding_of_ICCV_2021_paper.pdf)<br>:sunflower:[dataset](http://vipl.ict.ac.cn/resources/envqa)
  * [On the Hidden Treasure of Dialog in Video Question Answering](https://arxiv.org/abs/2103.14517)<br>:star:[code](https://github.com/InterDigitalInc/DialogSummary-VideoQA):house:[project](https://engindeniz.github.io/dialogsummary-videoqa)
  * [HAIR: Hierarchical Visual-Semantic Relational Reasoning for Video Question Answering](https://openaccess.thecvf.com/content/ICCV2021/papers/Liu_HAIR_Hierarchical_Visual-Semantic_Relational_Reasoning_for_Video_Question_Answering_ICCV_2021_paper.pdf)
  * [Video Question Answering Using Language-Guided Deep Compressed-Domain Video Feature](https://openaccess.thecvf.com/content/ICCV2021/papers/Kim_Video_Question_Answering_Using_Language-Guided_Deep_Compressed-Domain_Video_Feature_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/Nayoung-Kim-ICP/VQAC)
* A-VQA
  * [Pano-AVQA: Grounded Audio-Visual Question Answering on 360âˆ˜ Videos](https://arxiv.org/abs/2110.05122)

<a name="10"/>

## 10.OCR
* [Joint Visual Semantic Reasoning: Multi-Stage Decoder for Text Recognition](https://arxiv.org/abs/2107.12090)<br>:tv:[video](https://www.youtube.com/watch?v=GPk-O3ZqIoI)
* [Text is Text, No Matter What: Unifying Text Recognition using Knowledge Distillation](https://arxiv.org/abs/2107.12087)<br>:tv:[video](https://www.youtube.com/watch?v=8VLkaf_hGdQ)
* [Towards the Unseen: Iterative Text Recognition by Distilling from Errors](https://arxiv.org/abs/2107.12081)<br>:tv:[video](https://www.youtube.com/watch?v=ywaGXFZIiDI)
* ä»»æ„å½¢çŠ¶æ–‡æœ¬æ£€æµ‹
  * [Adaptive Boundary Proposal Network for Arbitrary Shape Text Detection](https://arxiv.org/abs/2107.12664)<br>:star:[code](https://github.com/GXYM/TextBPN)
* åœºæ™¯æ–‡æœ¬è¯†åˆ«
  * [From Two to One: A New Scene Text Recognizer with Visual Language Modeling Network](https://arxiv.org/abs/2108.09661)<br>:star:[code](https://github.com/wangyuxin87/VisionLAN)
* åœºæ™¯æ–‡æœ¬æ›¿æ¢
  * [STRIVE: Scene Text Replacement In Videos](https://arxiv.org/abs/2109.02762)<br>:house:[project](https://striveiccv2021.github.io/STRIVE-ICCV2021/)  
* æå–æ–‡æ¡£å›¾åƒ
  * [End-to-End Piece-Wise Unwarping of Document Images](https://openaccess.thecvf.com/content/ICCV2021/papers/Das_End-to-End_Piece-Wise_Unwarping_of_Document_Images_ICCV_2021_paper.pdf)<br>:house:[project](https://sagniklp.github.io/PiecewiseUnwarp/)
* æ‰‹å†™æ–‡æœ¬ç”Ÿæˆ
  * [Handwriting Transformers](https://arxiv.org/abs/2104.03964)<br>:star:[code](https://github.com/ankanbhunia/Handwriting-Transformers)
* Table Structure Recognition(è¡¨æ ¼ç»“æ„è¯†åˆ«)
  * [TGRNet: A Table Graph Reconstruction Network for Table Structure Recognition](https://arxiv.org/abs/2106.10598)<br>:star:[code](https://github.com/xuewenyuan/TGRNet)
  
<a name="9"/>

## 9.Video
* Action Detection and Recognition(äººä½“åŠ¨ä½œæ£€æµ‹ä¸è¯†åˆ«)
  * [Channel-wise Topology Refinement Graph Convolution for Skeleton-Based Action Recognition](https://arxiv.org/abs/2107.12213)<br>:star:[code](https://github.com/Uason-Chen/CTR-GCN)
  * [MGSampler: An Explainable Sampling Strategy for Video Action Recognition](https://arxiv.org/abs/2104.09952)<br>:star:[code](https://github.com/MCG-NJU/MGSampler)
  * [Skeleton Cloud Colorization for Unsupervised 3D Action Representation Learning](https://arxiv.org/abs/2108.01959)
  * [Video Pose Distillation for Few-Shot, Fine-Grained Sports Action Recognition](https://arxiv.org/abs/2109.01305)
  * [Class Semantics-based Attention for Action Detection](https://arxiv.org/abs/2109.02613)
  * [MultiSports: A Multi-Person Video Dataset of Spatio-Temporally Localized Sports Actions](https://arxiv.org/pdf/2105.07404.pdf)<br>:star:[code](https://github.com/MCG-NJU/MultiSports)
  * [AdaSGN: Adapting Joint Number and Model Size for Efficient Skeleton-Based Action Recognition](https://arxiv.org/abs/2103.11770)<br>:star:[code](https://github.com/lshiwjx/AdaSGN)
  * [OadTR: Online Action Detection With Transformers](https://arxiv.org/abs/2106.11149)<br>:star:[code](https://github.com/wangxiang1230/OadTR)
  * [Self-Supervised 3D Skeleton Action Representation Learning With Motion Consistency and Continuity](https://openaccess.thecvf.com/content/ICCV2021/papers/Su_Self-Supervised_3D_Skeleton_Action_Representation_Learning_With_Motion_Consistency_and_ICCV_2021_paper.pdf)
  * [Interactive Prototype Learning for Egocentric Action Recognition](https://openaccess.thecvf.com/content/ICCV2021/papers/Wang_Interactive_Prototype_Learning_for_Egocentric_Action_Recognition_ICCV_2021_paper.pdf)
  * [Efficient Action Recognition via Dynamic Knowledge Propagation](https://openaccess.thecvf.com/content/ICCV2021/papers/Kim_Efficient_Action_Recognition_via_Dynamic_Knowledge_Propagation_ICCV_2021_paper.pdf)
  * [Else-Net: Elastic Semantic Network for Continual Action Recognition From Skeleton Data](https://openaccess.thecvf.com/content/ICCV2021/papers/Li_Else-Net_Elastic_Semantic_Network_for_Continual_Action_Recognition_From_Skeleton_ICCV_2021_paper.pdf)
  * [Learning Self-Similarity in Space and Time As Generalized Motion for Video Action Recognition](https://openaccess.thecvf.com/content/ICCV2021/papers/Kwon_Learning_Self-Similarity_in_Space_and_Time_As_Generalized_Motion_for_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/arunos728/SELFY):house:[project](http://cvlab.postech.ac.kr/research/SELFY/)
  * [Temporal Action Detection With Multi-Level Supervision](https://arxiv.org/abs/2011.11893)<br>:star:[code](https://github.com/bfshi/SSAD_OSAD)
  * [Watch Only Once: An End-to-End Video Action Detection Framework](https://openaccess.thecvf.com/content/ICCV2021/papers/Chen_Watch_Only_Once_An_End-to-End_Video_Action_Detection_Framework_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/ShoufaChen/WOO)
  * [Unsupervised Few-Shot Action Recognition via Action-Appearance Aligned Meta-Adaptation](https://arxiv.org/abs/2109.15317)<br>:open_mouth:oral
  * [Geometric Deep Neural Network Using Rigid and Non-Rigid Transformations for Human Action Recognition](https://openaccess.thecvf.com/content/ICCV2021/papers/Friji_Geometric_Deep_Neural_Network_Using_Rigid_and_Non-Rigid_Transformations_for_ICCV_2021_paper.pdf)
  * [Just One Moment: Structural Vulnerability of Deep Action Recognition Against One Frame Attack](https://arxiv.org/abs/2011.14585)
  * [Evidential Deep Learning for Open Set Action Recognition](https://arxiv.org/abs/2107.10161)<br>:star:[code](https://github.com/Cogito2012/DEAR):house:[project](https://www.rit.edu/actionlab/dear):tv:[video](https://youtu.be/5rdKkr9kCG4)
  * [Learning an Augmented RGB Representation With Cross-Modal Knowledge Distillation for Action Detection](https://arxiv.org/abs/2108.03619)
  * [Class-Incremental Learning for Action Recognition in Videos](https://openaccess.thecvf.com/content/ICCV2021/papers/Park_Class-Incremental_Learning_for_Action_Recognition_in_Videos_ICCV_2021_paper.pdf)
  * [D2-Net: Weakly-Supervised Action Localization via Discriminative Embeddings and Denoised Activations](https://openaccess.thecvf.com/content/ICCV2021/papers/Narayan_D2-Net_Weakly-Supervised_Action_Localization_via_Discriminative_Embeddings_and_Denoised_Activations_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/naraysa/D2-Net)
  * é›¶æ ·æœ¬åŠ¨ä½œè¯†åˆ«
    * [Elaborative Rehearsal for Zero-shot Action Recognition](https://arxiv.org/abs/2108.02833)<br>:star:[code](https://github.com/DeLightCMU/ElaborativeRehearsal)
  * Temporal Action Localization(æ—¶åºåŠ¨ä½œå®šä½)
    * [Enriching Local and Global Contexts for Temporal Action Localization](https://arxiv.org/abs/2107.12960)<br>:star:[code](https://github.com/buxiangzhiren/ContextLoc)
    * [Learning Action Completeness from Points for Weakly-supervised Temporal Action Localization](https://arxiv.org/abs/2108.05029)<br>:open_mouth:oral:star:[code](https://github.com/Pilhyeon)
    * [Foreground-Action Consistency Network for Weakly Supervised Temporal Action Localization](https://arxiv.org/abs/2108.06524)<br>:star:[code](https://github.com/LeonHLJ/FAC-Net)
    * [Video Self-Stitching Graph Network for Temporal Action Localization](http://arxiv.org/abs/2011.14598)
    * [Divide and Conquer for Single-Frame Temporal Action Localization](https://openaccess.thecvf.com/content/ICCV2021/papers/Ju_Divide_and_Conquer_for_Single-Frame_Temporal_Action_Localization_ICCV_2021_paper.pdf)
    * [CAG-QIL: Context-Aware Actionness Grouping via Q Imitation Learning for Online Temporal Action Localization](https://openaccess.thecvf.com/content/ICCV2021/papers/Kang_CAG-QIL_Context-Aware_Actionness_Grouping_via_Q_Imitation_Learning_for_Online_ICCV_2021_paper.pdf)
  * Temporal Action Proposal Generation(æ—¶åºåŠ¨ä½œææ¡ˆç”Ÿæˆ)
    * [Relaxed Transformer Decoders for Direct Action Proposal Generation](https://arxiv.org/abs/2102.01894)<br>:star:[code](https://github.com/MCG-NJU/RTD-Action)
* Action Quality Assessment(è¡ŒåŠ¨è´¨é‡è¯„ä¼°)
  * [Group-aware Contrastive Regression for Action Quality Assessment](https://arxiv.org/abs/2108.07797)
* Video Rescaling
  * [Self-Conditioned Probabilistic Learning of Video Rescaling](https://arxiv.org/abs/2107.11639)
* Video activity localisation
  * [Cross-Sentence Temporal and Semantic Relations in Video Activity Localisation](https://arxiv.org/abs/2107.11443)<br>:newspaper:è§£è¯»:[ICCV2021 | å¦‚ä½•é«˜æ•ˆè§†é¢‘å®šä½ï¼ŸQMUL&åŒ—å¤§&Adobeå¼ºå¼ºè”æ‰‹æå‡ºå¼±ç›‘ç£CRMï¼Œæ€§èƒ½SOTA](https://mp.weixin.qq.com/s/tlGzpUU56HWjVqDtVOkdWg)
* è§†é¢‘ä¿®å¤
  * [Internal Video Inpainting by Implicit Long-range Propagation](https://arxiv.org/abs/2108.01912)<br>:star:[code](https://github.com/Tengfei-Wang/Implicit-Internal-Video-Inpainting):house:[project](https://tengfei-wang.github.io/Implicit-Internal-Video-Inpainting/)
  * [Occlusion-Aware Video Object Inpainting](https://arxiv.org/abs/2108.06765)<br>:house:[project](http://www.kelei.site/voin/)
  * [FuseFormer: Fusing Fine-Grained Information in Transformers for Video Inpainting](https://arxiv.org/abs/2109.02974)<br>:star:[code](https://github.com/ruiliu-ai/FuseFormer)
  * [Flow-Guided Video Inpainting with Scene Templates](https://openaccess.thecvf.com/content/ICCV2021/papers/Lao_Flow-Guided_Video_Inpainting_With_Scene_Templates_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/donglao/videoinpainting)
  * [Frequency-Aware Spatiotemporal Transformers for Video Inpainting Detection](https://openaccess.thecvf.com/content/ICCV2021/papers/Yu_Frequency-Aware_Spatiotemporal_Transformers_for_Video_Inpainting_Detection_ICCV_2021_paper.pdf)
* è§†é¢‘åˆ†æ
  * è§†é¢‘è¡¨å¾å­¦ä¹ 
    * [Enhancing Self-supervised Video Representation Learning via Multi-level Feature Optimization](https://arxiv.org/abs/2108.02183)<br>:star:[code](https://github.com/shvdiwnkozbw/Video-Representation-via-Multi-level-Optimization)
    * [Self-Supervised Video Representation Learning with Meta-Contrastive Network](https://arxiv.org/abs/2108.08426)
    * [Long Short View Feature Decomposition via Contrastive Video Representation Learning](https://arxiv.org/abs/2109.11593)
* è§†é¢‘å‰ªè¾‘
  * [Learning to Cut by Watching Movies](https://arxiv.org/abs/2108.04294)<br>:star:[code](https://github.com/PardoAlejo/LearningToCut):house:[project](https://www.alejandropardo.net/publication/learning-to-cut/) 
* è§†é¢‘å­—å¹•
  * [Motion Guided Region Message Passing for Video Captioning](https://openaccess.thecvf.com/content/ICCV2021/papers/Chen_Motion_Guided_Region_Message_Passing_for_Video_Captioning_ICCV_2021_paper.pdf)
  * [Aligning Subtitles in Sign Language Videos](https://arxiv.org/abs/2105.02877)<br>:house:[project](https://www.robots.ox.ac.uk/~vgg/research/bslalign/):tv:[video](https://youtu.be/WF-I4nP8SMM)
  * Dense Video Captioning
    * [End-to-End Dense Video Captioning with Parallel Decoding](https://arxiv.org/abs/2108.07781)<br>:star:[code](https://github.com/ttengwang/PDVC)
* è§†é¢‘ç¼–ç 
  * [Overfitting the Data: Compact Neural Video Delivery via Content-aware Feature Modulation](https://arxiv.org/abs/2108.08202)<br>:star:[code](https://github.com/Neural-video-delivery/CaFM-Pytorch-ICCV2021)<br>:newspaper:è§£è¯»:[ICCV2021â€”å·¥ä¸šç•Œä¸­çš„ç¥ç»ç½‘ç»œè§†é¢‘ä¼ è¾“è¶…åˆ†ç®—æ³•](https://mp.weixin.qq.com/s/dQCZaZNz0oCMHQQEdV79aA)
* è§†é¢‘ç”Ÿæˆ
  * [Click to Move: Controlling Video Generation with Sparse Motion](https://arxiv.org/abs/2108.08815)<br>:star:[code](https://github.com/PierfrancescoArdino/C2M)
* Video Relation Detection(è§†é¢‘å…³ç³»æ£€æµ‹)
  * [Social Fabric: Tubelet Compositions for Video Relation Detection](https://arxiv.org/abs/2108.08363)<br>:star:[code](https://github.com/shanshuo/Social-Fabric)
* Video Grounding
  * [Support-Set Based Cross-Supervision for Video Grounding](https://arxiv.org/abs/2108.10576)
* è§†é¢‘ç²¾å½©ç‰‡æ®µæ£€æµ‹
  * [Cross-category Video Highlight Detection via Set-based Learning](https://arxiv.org/abs/2108.11770)<br>:star:[code](https://github.com/ChrisAllenMing/Cross_Category_Video_Highlight)
  * [PR-Net: Preference Reasoning for Personalized Video Highlight Detection](https://arxiv.org/abs/2109.01799)
  * [HighlightMe: Detecting Highlights from Human-Centric Videos](https://arxiv.org/abs/2110.01774)
  * [Temporal Cue Guided Video Highlight Detection With Low-Rank Audio-Visual Fusion](https://openaccess.thecvf.com/content/ICCV2021/papers/Ye_Temporal_Cue_Guided_Video_Highlight_Detection_With_Low-Rank_Audio-Visual_Fusion_ICCV_2021_paper.pdf)
  * [Joint Visual and Audio Learning for Video Highlight Detection](https://openaccess.thecvf.com/content/ICCV2021/papers/Badamdorj_Joint_Visual_and_Audio_Learning_for_Video_Highlight_Detection_ICCV_2021_paper.pdf)
* è§†é¢‘è¯†åˆ«
  * [Searching for Two-Stream Models in Multivariate Space for Video Recognition](https://arxiv.org/abs/2108.12957)
  * [Adaptive Focus for Efficient Video Recognition](https://arxiv.org/abs/2105.03245)<br>:open_mouth:oral:star:[code](https://github.com/blackfeather-wang/AdaFocus)
  * [AdaMML: Adaptive Multi-Modal Learning for Efficient Video Recognition](https://arxiv.org/abs/2105.05165)<br>:star:[code](https://github.com/IBM/AdaMML):house:[project](https://rpand002.github.io/adamml.html)
  * [TAM: Temporal Adaptive Module for Video Recognition](https://arxiv.org/abs/2005.06803)<br>:star:[code](https://github.com/liu-zhy/temporal-adaptive-module)
  * [Condensing a Sequence to One Informative Frame for Video Recognition](https://openaccess.thecvf.com/content/ICCV2021/papers/Qiu_Condensing_a_Sequence_to_One_Informative_Frame_for_Video_Recognition_ICCV_2021_paper.pdf)
  * [VideoLT: Large-Scale Long-Tailed Video Recognition](https://arxiv.org/abs/2105.02668)<br>:star:[code](https://github.com/17Skye17/VideoLT)
  * [Motion-Augmented Self-Training for Video Recognition at Smaller Scale](https://arxiv.org/abs/2105.01646
  * [Multi-Modal Multi-Action Video Recognition](https://openaccess.thecvf.com/content/ICCV2021/papers/Shi_Multi-Modal_Multi-Action_Video_Recognition_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/zhenglab/multi-action-video)
* Motion Retargeting(è¿åŠ¨é‡å®šä½)
  * [Contact-Aware Retargeting of Skinned Motion](https://arxiv.org/abs/2109.07431)<br>:tv:[video](https://www.youtube.com/watch?v=qQ4HO2Hibsk)
* è§†é¢‘é¢„æµ‹
  * [A Hierarchical Variational Neural Uncertainty Model for Stochastic Video Prediction](https://arxiv.org/abs/2110.03446)<br>:open_mouth:oral
* è§†é¢‘åˆæˆ
  * [iPOKE: Poking a Still Image for Controlled Stochastic Video Synthesis](https://arxiv.org/abs/2107.02790)<br>:star:[code](https://github.com/CompVis/ipoke):house:[project](https://bit.ly/3dJN4Lf)
* è§†é¢‘å¸§æ’å€¼
  * [Training Weakly Supervised Video Frame Interpolation With Events](https://openaccess.thecvf.com/content/ICCV2021/papers/Yu_Training_Weakly_Supervised_Video_Frame_Interpolation_With_Events_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/YU-Zhiyang/WEVI)
  * [Asymmetric Bilateral Motion Estimation for Video Frame Interpolation](https://arxiv.org/abs/2108.06815)<br>:star:[code](https://arxiv.org/abs/2108.06815)
  * [XVFI: eXtreme Video Frame Interpolation](https://arxiv.org/abs/2103.16206)<br>:open_mouth:oral:star:[code](https://github.com/JihyongOh/XVFI):tv:[video](https://www.youtube.com/watch?v=5qAiffYFJh8)
* Deepfake è§†é¢‘æ£€æµ‹
  * [ID-Reveal: Identity-aware DeepFake Video Detection](https://openaccess.thecvf.com/content/ICCV2021/papers/Cozzolino_ID-Reveal_Identity-Aware_DeepFake_Video_Detection_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/grip-unina/id-reveal)
* è§†é¢‘ç¨³å®š
  * [Hybrid Neural Fusion for Full-Frame Video Stabilization](https://arxiv.org/abs/2102.06205)<br>:star:[code](https://github.com/alex04072000/FuSta):house:[project](https://alex04072000.github.io/FuSta/):tv:[video](https://youtu.be/KO3sULs4hso)
* Video Frame-level Similarity(è§†é¢‘å¸§çº§ç›¸ä¼¼åº¦å­¦ä¹ )
  * [Rethinking Self-supervised Correspondence Learning: A Video Frame-level Similarity Perspective](https://arxiv.org/abs/2103.17263)<br>:open_mouth:oral:star:[code](https://github.com/xvjiarui/VFS/):house:[project](https://jerryxu.net/VFS/):tv:[video](https://youtu.be/H6cwwdf1p4I)
* è§†é¢‘å‹ç¼©
  * [Online-Trained Upsampler for Deep Low Complexity Video Compression](https://openaccess.thecvf.com/content/ICCV2021/papers/Klopp_Online-Trained_Upsampler_for_Deep_Low_Complexity_Video_Compression_ICCV_2021_paper.pdf)
* è§†é¢‘æ—¶åˆ»æ£€ç´¢
  * [Fast Video Moment Retrieval](https://openaccess.thecvf.com/content/ICCV2021/papers/Gao_Fast_Video_Moment_Retrieval_ICCV_2021_paper.pdf)
* è§†é¢‘æ‘˜è¦
  * [Multiple Pairwise Ranking Networks for Personalized Video Summarization](https://openaccess.thecvf.com/content/ICCV2021/papers/Saquil_Multiple_Pairwise_Ranking_Networks_for_Personalized_Video_Summarization_ICCV_2021_paper.pdf)
* è§†é¢‘è´¨é‡è¯„ä¼°
  * [Unsupervised Curriculum Domain Adaptation for No-Reference Video Quality Assessment](https://openaccess.thecvf.com/content/ICCV2021/papers/Chen_Unsupervised_Curriculum_Domain_Adaptation_for_No-Reference_Video_Quality_Assessment_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/cpf0079/UCDA)
* Video Grounding
  * [STVGBert: A Visual-Linguistic Transformer Based Framework for Spatio-Temporal Video Grounding](https://openaccess.thecvf.com/content/ICCV2021/papers/Su_STVGBert_A_Visual-Linguistic_Transformer_Based_Framework_for_Spatio-Temporal_Video_Grounding_ICCV_2021_paper.pdf)
* è§†é¢‘å®šä½
  * [Zero-Shot Natural Language Video Localization](https://arxiv.org/abs/2110.00428)<br>:open_mouth:oral:star:[code](https://github.com/gistvision/PSVL)
* è§†é¢‘æ¨ç†
  * [Real-Time Video Inference on Edge Devices via Adaptive Model Streaming](https://openaccess.thecvf.com/content/ICCV2021/papers/Khani_Real-Time_Video_Inference_on_Edge_Devices_via_Adaptive_Model_Streaming_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/modelstreaming/ams)
* è§†é¢‘ç›¸å…³
  * [Anonymizing Egocentric Videos](https://openaccess.thecvf.com/content/ICCV2021/papers/Thapar_Anonymizing_Egocentric_Videos_ICCV_2021_paper.pdf)
* è§†é¢‘å¼‚å¸¸æ£€æµ‹
  * [Dance With Self-Attention: A New Look of Conditional Random Fields on Anomaly Detection in Videos](https://openaccess.thecvf.com/content/ICCV2021/papers/Purwanto_Dance_With_Self-Attention_A_New_Look_of_Conditional_Random_Fields_ICCV_2021_paper.pdf)
  * [A Hybrid Video Anomaly Detection Framework via Memory-Augmented Flow Reconstruction and Flow-Guided Frame Prediction](https://arxiv.org/abs/2108.06852)<br>:star:[code](https://github.com/LiUzHiAn/hf2vad)
  * [Weakly-Supervised Video Anomaly Detection With Robust Temporal Feature Magnitude Learning](https://arxiv.org/abs/2101.10030)<br>:star:[code](https://github.com/tianyu0207/RTFM)
* è§†é¢‘å»å™ª
  * [Unsupervised Deep Video Denoising](https://arxiv.org/abs/2011.15045)<br>:star:[code](https://github.com/sreyas-mohan/udvd):house:[project](https://sreyas-mohan.github.io/udvd/)
* Video Portrait Relighting(äººåƒè§†é¢‘é‡ç…§æ˜)
  * [Neural Video Portrait Relighting in Real-time via Consistency Modeling](https://arxiv.org/abs/2104.00484)
* è§†é¢‘æ—¶åºå®šä½
  * [Boundary-Sensitive Pre-Training for Temporal Localization in Videos](https://openaccess.thecvf.com/content/ICCV2021/papers/Xu_Boundary-Sensitive_Pre-Training_for_Temporal_Localization_in_Videos_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/frostinassiky/bsp):house:[project](https://frostinassiky.github.io/bsp/)
* è§†é¢‘å…³è”æ€§
  * [Explainable Video Entailment With Grounded Visual Evidence](https://openaccess.thecvf.com/content/ICCV2021/papers/Chen_Explainable_Video_Entailment_With_Grounded_Visual_Evidence_ICCV_2021_paper.pdf)
* è§†é¢‘æŠ å›¾
  * [Video Matting via Consistency-Regularized Graph Neural Networks](https://openaccess.thecvf.com/content/ICCV2021/papers/Wang_Video_Matting_via_Consistency-Regularized_Graph_Neural_Networks_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/TiantianWang/VideoMatting-CRGNN)
* è§†é¢‘ç¼–ç 
  * [ELF-VC: Efficient Learned Flexible-Rate Video Coding](https://openaccess.thecvf.com/content/ICCV2021/papers/Rippel_ELF-VC_Efficient_Learned_Flexible-Rate_Video_Coding_ICCV_2021_paper.pdf)
* è¯†åˆ«è§†é¢‘ä¸­äº’åŠ¨å…³ç³»
  * [Motion Guided Attention Fusion To Recognize Interactions From Videos](https://arxiv.org/abs/2104.00646)
* è§†é¢‘å»æ¨¡ç³Š
  * [Multi-Scale Separable Network for Ultra-High-Definition Video Deblurring](https://openaccess.thecvf.com/content/ICCV2021/papers/Deng_Multi-Scale_Separable_Network_for_Ultra-High-Definition_Video_Deblurring_ICCV_2021_paper.pdf)
* è§†é¢‘ç†è§£
  * [Unified Graph Structured Models for Video Understanding](https://arxiv.org/abs/2103.15662)
* è§†é¢‘é‡å»º
  * [HDR Video Reconstruction: A Coarse-to-fine Network and A Real-world Benchmark Dataset](https://arxiv.org/abs/2103.14943)<br>:star:[code](https://github.com/guanyingc/DeepHDRVideo):house:[project](https://guanyingc.github.io/DeepHDRVideo/):tv:[video](https://youtu.be/uqfILpoWNco):sunflower:[dataset](https://guanyingc.github.io/DeepHDRVideo-Dataset/)


<a name="8"/>

## 8.Human Pose Estimation(äººä½“å§¿æ€ä¼°è®¡)
* [Human Pose Regression with Residual Log-likelihood Estimation](https://arxiv.org/abs/2107.11291)<br>:open_mouth:oral:star:[code](https://github.com/Jeff-sjtu/res-loglikelihood-regression)
* [Online Knowledge Distillation for Efficient Pose Estimation](https://arxiv.org/abs/2108.02092)
* [DECA: Deep viewpoint-Equivariant human pose estimation using Capsule Autoencoders](https://arxiv.org/abs/2108.08557)<br>:open_mouth:oral:star:[code](https://github.com/mmlab-cv/DECA)
* [Estimating and Exploiting the Aleatoric Uncertainty in Surface Normal Estimation](https://arxiv.org/abs/2109.09881)<br>:open_mouth:oral:star:[code](https://github.com/baegwangbin/surface_normal_uncertainty)
* [Dynamical Pose Estimation](https://arxiv.org/abs/2103.06182)<br>:star:[code](https://github.com/hankyang94/DAMP):tv:[video](https://www.youtube.com/watch?v=S6L0h-d0IYM)
* [Multi-Instance Pose Networks: Rethinking Top-Down Pose Estimation](https://arxiv.org/abs/2101.11223)<br>:star:[code](https://github.com/rawalkhirodkar/MIPNet):house:[project](https://rawalkhirodkar.github.io/mipnet/)
* [Egocentric Pose Estimation From Human Vision Span](https://arxiv.org/abs/2104.05167)
* [Learning Privacy-Preserving Optics for Human Pose Estimation](https://openaccess.thecvf.com/content/ICCV2021/papers/Hinojosa_Learning_Privacy-Preserving_Optics_for_Human_Pose_Estimation_ICCV_2021_paper.pdf)<br>:open_mouth:oral:star:[code](https://github.com/carlosh93/privacy-optics-hpe):house:[project](https://carloshinojosa.me/project/privacy-hpe/):tv:[video](https://www.youtube.com/watch?v=Lg7I1QR3W9Y)
* [TokenPose: Learning Keypoint Tokens for Human Pose Estimation](https://arxiv.org/abs/2104.03516)<br>:star:[code](https://github.com/leeyegy/TokenPose)
* [Motion Adaptive Pose Estimation from Compressed Videos](https://openaccess.thecvf.com/content/ICCV2021/papers/Fan_Motion_Adaptive_Pose_Estimation_From_Compressed_Videos_ICCV_2021_paper.pdf)
* 3D äººä½“å§¿æ€ä¼°è®¡
  * [PyMAF: 3D Human Pose and Shape Regression with Pyramidal Mesh Alignment Feedback Loop](https://arxiv.org/abs/2103.16507)<br>:open_mouth:oral:star:[code](https://github.com/HongwenZhang/PyMAF):house:[project](https://hongwenzhang.github.io/pymaf/)
  * [HuMoR: 3D Human Motion Model for Robust Pose Estimation](https://arxiv.org/abs/2105.04668)<br>:open_mouth:oral:house:[project](https://geometry.stanford.edu/projects/humor/):tv:[video](https://youtu.be/5VWirxUHG0Y)
  * [Probabilistic Monocular 3D Human Pose Estimation with Normalizing Flows](https://arxiv.org/abs/2107.13788)<br>:star:[code](https://github.com/twehrbein/Probabilistic-Monocular-3D-Human-Pose-Estimation-with-Normalizing-Flows):tv:[video](https://www.youtube.com/watch?v=gaNX5CIl1L8)
  * [Learning Skeletal Graph Neural Networks for Hard 3D Pose Estimation](https://arxiv.org/abs/2108.07181)<br>:star:[code](https://github.com/ailingzengzzz/Skeletal-GNN)
  * [EventHPE: Event-based 3D Human Pose and Shape Estimation](https://arxiv.org/abs/2108.06819)<br>:star:[code](https://github.com/JimmyZou/EventHPE)
  * [imGHUM: Implicit Generative Models of 3D Human Shape and Articulated Pose](https://arxiv.org/abs/2108.10842)
  * [Graph-Based 3D Multi-Person Pose Estimation Using Multi-View Images](https://arxiv.org/abs/2109.05885)
  * [Unsupervised 3D Pose Estimation for Hierarchical Dance Video Recognition](https://arxiv.org/abs/2109.09166)
  * [Learning to Regress Bodies from Images using Differentiable Semantic Rendering](https://arxiv.org/abs/2110.03480)<br>:house:[project](https://dsr.is.tue.mpg.de/)
  * [Hierarchical Kinematic Probability Distributions for 3D Human Shape and Pose Estimation from Images in the Wild](https://arxiv.org/abs/2110.00990)<br>:star:[code](https://github.com/akashsengupta1997/HierarchicalProbabilistic3DHuman)
  * [3D Human Pose Estimation With Spatial and Temporal Transformers](https://arxiv.org/abs/2103.10455)<br>:star:[code](https://github.com/zczcwh/PoseFormer):tv:[video](https://www.youtube.com/watch?v=z8HWOdXjGR8)
  * [PARE: Part Attention Regressor for 3D Human Body Estimation](http://arxiv.org/abs/2104.08527)<br>:star:[code](https://github.com/mkocabas/PARE):house:[project](https://pare.is.tue.mpg.de/):tv:[video](https://youtu.be/3C9hdFajO3k)
  * [Learning Causal Representation for Training Cross-Domain Pose Estimator via Generative Interventions](https://openaccess.thecvf.com/content/ICCV2021/papers/Zhang_Learning_Causal_Representation_for_Training_Cross-Domain_Pose_Estimator_via_Generative_ICCV_2021_paper.pdf)
  * [UltraPose: Synthesizing Dense Pose With 1 Billion Points by Human-Body Decoupling 3D Model](https://openaccess.thecvf.com/content/ICCV2021/papers/Yan_UltraPose_Synthesizing_Dense_Pose_With_1_Billion_Points_by_Human-Body_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/MomoAILab/ultrapose)
  * [Modulated Graph Convolutional Network for 3D Human Pose Estimation](https://openaccess.thecvf.com/content/ICCV2021/papers/Zou_Modulated_Graph_Convolutional_Network_for_3D_Human_Pose_Estimation_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/ZhimingZo/Modulated-GCN)
  * [Revitalizing Optimization for 3D Human Pose and Shape Estimation: A Sparse Constrained Formulation](https://arxiv.org/abs/2105.13965)<br>:star:[code](https://github.com/fantaosha/SCOPE):house:[project](https://sites.google.com/view/scope-human/):tv:[video](https://sites.google.com/view/scope-human/home#h.j4u4ho36ixe8)
  * [Estimating Egocentric 3D Human Pose in Global Space](https://arxiv.org/abs/2104.13454)<br>:house:[project](https://vcai.mpi-inf.mpg.de/projects/globalegomocap/):tv:[video](https://vcai.mpi-inf.mpg.de/projects/globalegomocap/data/global_egomocap.mp4)
  * [Camera Distortion-Aware 3D Human Pose Estimation in Video With Optimization-Based Meta-Learning](https://openaccess.thecvf.com/content/ICCV2021/papers/Cho_Camera_Distortion-Aware_3D_Human_Pose_Estimation_in_Video_With_Optimization-Based_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/hanbyel0105/CamDistHumanPose3D)
  * [EM-POSE: 3D Human Pose Estimation From Sparse Electromagnetic Trackers](https://openaccess.thecvf.com/content/ICCV2021/papers/Kaufmann_EM-POSE_3D_Human_Pose_Estimation_From_Sparse_Electromagnetic_Trackers_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/facebookresearch/em-pose):house:[project](https://ait.ethz.ch/projects/2021/em-pose/):tv:[video](https://youtu.be/PqSL4fhIilM)
  * [Towards Alleviating the Modeling Ambiguity of Unsupervised Monocular 3D Human Pose Estimation](https://openaccess.thecvf.com/content/ICCV2021/papers/Yu_Towards_Alleviating_the_Modeling_Ambiguity_of_Unsupervised_Monocular_3D_Human_ICCV_2021_paper.pdf)<br>:house:[project](https://sites.google.com/view/ambiguity-aware-hpe)
* [SPEC: Seeing People in the Wild with an Estimated Camera](https://arxiv.org/abs/2110.00620)<br>:star:[code](https://github.com/mkocabas/SPEC):house:[project](https://spec.is.tue.mpg.de/):tv:[video](https://youtu.be/0nV6NoxbqUM)
* [Encoder-Decoder With Multi-Level Attention for 3D Human Shape and Pose Estimation](https://arxiv.org/abs/2109.02303)<br>:star:[code](https://github.com/ziniuwan/maed)
* 3Då§¿åŠ¿è¿ç§»
  * [Unsupervised Geodesic-preserved Generative Adversarial Networks for Unconstrained 3D Pose Transfer](https://arxiv.org/abs/2108.07520)<br>:star:[code](https://github.com/mikecheninoulu/Unsupervised_IEPGAN)
* æ‰‹éƒ¨å§¿åŠ¿
  * æ‰‹åŠ¿åˆæˆ
    * [Speech Drives Templates: Co-Speech Gesture Synthesis with Learned Templates](https://arxiv.org/abs/2108.08020)<br>:star:[code](https://github.com/ShenhanQian/SpeechDrivesTemplates)
  * æ‰‹åŠ¿è¯†åˆ«
    * [Hand Image Understanding via Deep Multi-Task Learning](https://arxiv.org/abs/2107.11646)<br>:star:[code](https://github.com/MandyMo/HIU-DMTL)
    * [SemiHand: Semi-Supervised Hand Pose Estimation With Consistency](https://openaccess.thecvf.com/content/ICCV2021/papers/Yang_SemiHand_Semi-Supervised_Hand_Pose_Estimation_With_Consistency_ICCV_2021_paper.pdf)
  * 3D æ‰‹éƒ¨å§¿æ€
    * [HandFoldingNet: A 3D Hand Pose Estimation Network Using Multiscale-Feature Guided Folding of a 2D Hand Skeleton](https://arxiv.org/abs/2108.05545)<br>:star:[code](https://github.com/cwc1260/HandFold)
    * [EventHands: Real-Time Neural 3D Hand Pose Estimation From an Event Stream](https://openaccess.thecvf.com/content/ICCV2021/papers/Rudnev_EventHands_Real-Time_Neural_3D_Hand_Pose_Estimation_From_an_Event_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/r00tman/EventHands):house:[project](https://4dqv.mpi-inf.mpg.de/EventHands/):tv:[video](https://youtu.be/jB1nkSYtblU)
    * [Self-Supervised 3D Hand Pose Estimation from monocular RGB via Contrastive Learning](https://arxiv.org/abs/2106.05953)<br>:star:[code](https://ait.ethz.ch/projects/2021/PeCLR/):house:[project](https://ait.ethz.ch/projects/2021/PeCLR/):tv:[video](https://youtu.be/DQtWIGrL54g)
  * æ‰‹éƒ¨äº¤äº’å§¿åŠ¿ä¼°è®¡
    * [End-to-End Detection and Pose Estimation of Two Interacting Hands](https://openaccess.thecvf.com/content/ICCV2021/papers/Kim_End-to-End_Detection_and_Pose_Estimation_of_Two_Interacting_Hands_ICCV_2021_paper.pdf)
  * 3Dæ‰‹ç½‘æ ¼å»ºæ¨¡
    * [I2UV-HandNet: Image-to-UV Prediction Network for Accurate and High-Fidelity 3D Hand Mesh Modeling](https://openaccess.thecvf.com/content/ICCV2021/papers/Chen_I2UV-HandNet_Image-to-UV_Prediction_Network_for_Accurate_and_High-Fidelity_3D_Hand_ICCV_2021_paper.pdf)
  * [Towards Accurate Alignment in Real-Time 3D Hand-Mesh Reconstruction](https://arxiv.org/abs/2109.01723)
  * æ‰‹éƒ¨ç½‘æ ¼æ¢å¤
    * [Self-Supervised Transfer Learning for Hand Mesh Recovery From Binocular Images](https://openaccess.thecvf.com/content/ICCV2021/papers/Chen_Self-Supervised_Transfer_Learning_for_Hand_Mesh_Recovery_From_Binocular_Images_ICCV_2021_paper.pdf)
  * æ‰‹åŠ¿å­¦ä¹ 
    * [TravelNet: Self-Supervised Physically Plausible Hand Motion Learning From Monocular Color Images](https://openaccess.thecvf.com/content/ICCV2021/papers/Zhao_TravelNet_Self-Supervised_Physically_Plausible_Hand_Motion_Learning_From_Monocular_Color_ICCV_2021_paper.pdf)<br>:house:[project](https://www.yangangwang.com/#publication)
  * æ‰‹åŠ¿é‡å»º
    * [Interacting Two-Hand 3D Pose and Shape Reconstruction from Single Color Image](https://www.yangangwang.com/papers/ZHANG-ITH-2021-08.pdf)<br>:star:[code](https://github.com/BaowenZ/Intershape)
* ä¸‰ç»´ç½‘æ ¼åˆæˆ
  * [Deep Hybrid Self-Prior for Full 3D Mesh Generation](https://arxiv.org/abs/2108.08017)<br>:house:[project](https://yqdch.github.io/DHSP3D/)
  * [Mesh Graphormer](https://arxiv.org/abs/2104.00272)<br>:star:[code](https://github.com/microsoft/MeshGraphormer)
* äººä½“é‡å»º
  * [ARCH++: Animation-Ready Clothed Human Reconstruction Revisited](https://arxiv.org/abs/2108.07845)<br>:tv:[video](https://www.youtube.com/watch?v=kNtlheGLSR8)
  * 3D äººä½“é‡å»º
    * [Probabilistic Modeling for Human Mesh Recovery](https://arxiv.org/abs/2108.11944)<br>:star:[code](https://github.com/nkolot/ProHMR):house:[project](https://www.seas.upenn.edu/~nkolot/projects/prohmr/)
    * [Gravity-Aware Monocular 3D Human-Object Reconstruction](https://arxiv.org/abs/2108.08844)<br>:house:[project](http://4dqv.mpi-inf.mpg.de/GraviCap/)
    * [THUNDR: Transformer-Based 3D Human Reconstruction With Markers](https://arxiv.org/abs/2106.09336)
    * [NPMs: Neural Parametric Models for 3D Deformable Shapes](https://arxiv.org/abs/2104.00702)<br>:star:[code](https://github.com/pablopalafox/npms):house:[project](https://pablopalafox.github.io/npms/):tv:[video](https://www.youtube.com/watch?v=muZXXgkkMPY)
* 4Däººä½“æ•æ‰
  * [Learning Motion Priors for 4D Human Body Capture in 3D Scenes](https://arxiv.org/abs/2108.10399)<br>:star:[code](https://github.com/sanweiliti/LEMO):house:[project](https://sanweiliti.github.io/LEMO/LEMO.html):tv:[video](https://youtu.be/ly8UaeFqFhw) 
* äººä½“å§¿æ€ä¼°è®¡ä¸åˆæˆ
  * [Physics-based Human Motion Estimation and Synthesis from Videos](https://arxiv.org/abs/2109.09913)
* å¤šäººå§¿æ€ä¼°è®¡ 
  * [Shape-aware Multi-Person Pose Estimation from Multi-View Images](https://arxiv.org/abs/2110.02330)<br>:star:[code](https://github.com/zj-dong/Multi-Person-Pose-Estimation):house:[project](https://ait.ethz.ch/projects/2021/multi-human-pose/):tv:[video](https://www.youtube.com/watch?v=KE5Jpnyqmh4)<br>è®ºæ–‡å…¬å¼€
  * [The Center of Attention: Center-Keypoint Grouping via Attention for Multi-Person Pose Estimation](https://arxiv.org/abs/2110.05132)<br>:star:[code](https://github.com/dvl-tum)
* äºº/ç‰©ä½“å§¿æ€å…³é”®ç‚¹æ£€æµ‹
  * [Keypoint Communities](https://arxiv.org/abs/2110.00988)<br>:star:[code](https://github.com/DuncanZauss/Keypoint_Communities)
* äººä½“è¿åŠ¨æ•æ‰
  * [SOMA: Solving Optical Marker-Based MoCap Automatically](https://arxiv.org/abs/2110.04431)<br>:star:[code](https://github.com/nghorbani/soma):house:[project](https://soma.is.tue.mpg.de/):tv:[video](https://youtu.be/BEFCqIefLA8)
  * [DeepMultiCap: Performance Capture of Multiple Characters Using Sparse Multiview Cameras](https://arxiv.org/abs/2105.00261)<br>:house:[project](http://liuyebin.com/dmc/dmc.html)
  * [Lightweight Multi-person Total Motion Capture Using Sparse Multi-view Cameras](https://arxiv.org/abs/2108.10378)
* 2Däººä½“å§¿åŠ¿ä¼°è®¡
  * [An Empirical Study of the Collapsing Problem in Semi-Supervised 2D Human Pose Estimation](https://arxiv.org/abs/2011.12498)<br>:star:[code](https://github.com/xierc/Semi_Human_Pose)
* Human Action Video Alignment
  * [Normalized Human Pose Features for Human Action Video Alignment](https://openaccess.thecvf.com/content/ICCV2021/papers/Liu_Normalized_Human_Pose_Features_for_Human_Action_Video_Alignment_ICCV_2021_paper.pdf)
* 3Då§¿æ€è¿ç§»
  * [Intrinsic-Extrinsic Preserved GANs for Unsupervised 3D Pose Transfer](https://arxiv.org/abs/2108.07520)<br>:star:[code](https://github.com/mikecheninoulu/Unsupervised_IEPGAN)
* äººä½“ç½‘æ ¼æ¢å¤
  * [Skeleton2Mesh: Kinematics Prior Injected Unsupervised Human Mesh Recovery](https://openaccess.thecvf.com/content/ICCV2021/papers/Yu_Skeleton2Mesh_Kinematics_Prior_Injected_Unsupervised_Human_Mesh_Recovery_ICCV_2021_paper.pdf)<br>:house:[project](https://sites.google.com/view/skeleton2mesh)
  * [Uncertainty-Aware Human Mesh Recovery From Video by Learning Part-Based 3D Dynamics](https://openaccess.thecvf.com/content/ICCV2021/papers/Lee_Uncertainty-Aware_Human_Mesh_Recovery_From_Video_by_Learning_Part-Based_3D_ICCV_2021_paper.pdf)
* æ ¹æ®äººä½“å§¿åŠ¿ä¼°è®¡è·ç¦»
  * [Single View Physical Distance Estimation using Human Pose](https://arxiv.org/abs/2106.10335)<br>:house:[project](https://feixh.github.io/projects/physical_distance/)
* 3Däººä½“
  * [SNARF: Differentiable Forward Skinning for Animating Non-Rigid Neural Implicit Shapes](https://arxiv.org/abs/2104.03953)<br>:star:[code](https://github.com/xuchen-ethz/snarf):house:[project](https://xuchen-ethz.github.io/snarf):tv:[video](https://www.youtube.com/watch?v=rCEpFTKjFHE)
  * [Monocular, One-stage, Regression of Multiple 3D People](https://arxiv.org/abs/2008.12272)<br>:star:[code](https://github.com/Arthur151/ROMP)
* è¿åŠ¨åˆæˆ
  * [Synthesis of Compositional Animations from Textual Descriptions](https://arxiv.org/abs/2103.14675)<br>:star:[code](https://github.com/anindita127/Complextext2animation)
  * 3Däººä½“è¿åŠ¨åˆæˆ
    * [Action-Conditioned 3D Human Motion Synthesis With Transformer VAE](https://openaccess.thecvf.com/content/ICCV2021/papers/Petrovich_Action-Conditioned_3D_Human_Motion_Synthesis_With_Transformer_VAE_ICCV_2021_paper.pdf)
    * [A Unified 3D Human Motion Synthesis Model via Conditional Variational Auto-Encoder](https://openaccess.thecvf.com/content/ICCV2021/papers/Cai_A_Unified_3D_Human_Motion_Synthesis_Model_via_Conditional_Variational_ICCV_2021_paper.pdf)
* 3DåŠ¨ç”»
  * [DeePSD: Automatic Deep Skinning and Pose Space Deformation for 3D Garment Animation](https://arxiv.org/abs/2009.02715)
* æœè£…ç±»åˆ«çº§å§¿åŠ¿ä¼°è®¡
  * [GarmentNets: Category-Level Pose Estimation for Garments via Canonical Space Shape Completion](https://arxiv.org/abs/2104.05177)<br>:star:[code](https://github.com/columbia-ai-robotics/garmentnets):house:[project](https://garmentnets.cs.columbia.edu/)
* æœè£…äººä½“å»ºæ¨¡
  * [Point-Based Modeling of Human Clothing](https://arxiv.org/abs/2104.08230)<br>:star:[code](https://github.com/saic-vul/point_based_clothing):house:[project](https://saic-violet.github.io/point-based-clothing/):tv:[video](https://www.youtube.com/watch?v=kFrAu415kDU&feature=youtu.be)
* å…³é”®ç‚¹å®šä½
  * [TransPose: Keypoint Localization via Transformer](https://arxiv.org/abs/2012.14214)<br>:star:[code](https://github.com/yangsenius/TransPose)

<a name="7"/>

## 7.Scene Graph Generation(åœºæ™¯å›¾ç”Ÿæˆ)
* [Spatial-Temporal Transformer for Dynamic Scene Graph Generation](https://arxiv.org/abs/2107.12309)<br>:star:[code](https://github.com/yrcong/STTran):tv:[video](https://www.youtube.com/watch?v=6D3ExjQpbjQ&feature=youtu.be)
* [Unconditional Scene Graph Generation](https://arxiv.org/abs/2108.05884)<br>:house:[project](https://scenegraphgen.github.io/)
* [Target Adaptive Context Aggregation for Video Scene Graph Generation](https://arxiv.org/abs/2108.08121)<br>:star:[code](https://github.com/MCG-NJU/TRACE)
* [Learning to Generate Scene Graph from Natural Language Supervision](https://arxiv.org/pdf/2109.02227)<br>:star:[code](https://github.com/YiwuZhong/SGG_from_NLS)
* [Segmentation-Grounded Scene Graph Generation](https://arxiv.org/abs/2104.14207)<br>:star:[code](https://github.com/ubc-vision/segmentation-sg)
* [Context-aware Scene Graph Generation with Seq2Seq Transformer](https://openaccess.thecvf.com/content/ICCV2021/papers/Lu_Context-Aware_Scene_Graph_Generation_With_Seq2Seq_Transformers_ICCV_2021_paper.pdf)<br>:star:[code]( https://github.com/layer6ai-labs/SGG-Seq2Seq)
* [A Simple Baseline for Weakly-Supervised Scene Graph Generation](https://openaccess.thecvf.com/content/ICCV2021/papers/Shi_A_Simple_Baseline_for_Weakly-Supervised_Scene_Graph_Generation_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/jshi31/WS-SGG)
* [Generative Compositional Augmentations for Scene Graph Prediction](https://arxiv.org/abs/2007.05756)<br>:star:[code](https://github.com/bknyaz/sgg)
* [From General to Specific: Informative Scene Graph Generation via Balance Adjustment](https://arxiv.org/abs/2108.13129)<br>:star:[code](https://github.com/ZhuGeKongKong/SGG-G2S)
* åœºæ™¯åˆæˆ
  * [Self-Supervised Real-to-Sim Scene Generation](https://arxiv.org/abs/2011.14488)<br>:house:[project](https://research.nvidia.com/publication/2021-08_Sim2SG)
  * [Unconstrained Scene Generation With Locally Conditioned Radiance Fields](https://arxiv.org/abs/2104.00670)<br>:star:[code](https://apple.github.io/ml-gsn/):house:[project](https://github.com/apple/ml-gsn)
  * [Visual Distant Supervision for Scene Graph Generation](https://arxiv.org/abs/2103.15365)<br>:star:[code](https://github.com/thunlp/VisualDS)

<a name="6"/>

## 6.Point Cloud(ç‚¹äº‘)
* [AdaFit: Rethinking Learning-based Normal Estimation on Point Clouds](https://arxiv.org/abs/2108.05836)<br>:star:[code](https://github.com/Runsong123/AdaFit):house:[project](https://runsong123.github.io/AdaFit/)
* [Adaptive Graph Convolution for Point Cloud Analysis](https://arxiv.org/abs/2108.08035)<br>:star:[code](https://github.com/hrzhou2/AdaptConv-master)
* [Learning Inner-Group Relations on Point Clouds](https://arxiv.org/abs/2108.12468)
* [CPFN: Cascaded Primitive Fitting Networks for High-Resolution Point Clouds](https://arxiv.org/abs/2109.00113)<br>:star:[code](https://github.com/erictuanle/CPFN)
* [Cloud Transformers: A Universal Approach To Point Cloud Processing Tasks](https://arxiv.org/abs/2007.11679)<br>:star:[code](https://github.com/saic-vul/cloud_transformers):tv:[video](https://www.youtube.com/watch?v=lYTzLhy-ybw) 
* [PCAM: Product of Cross-Attention Matrices for Rigid Registration of Point Clouds](https://arxiv.org/abs/2110.01269)<br>:star:[code](https://github.com/valeoai/PCAM)
* [3DVG-Transformer: Relation Modeling for Visual Grounding on Point Clouds](https://openaccess.thecvf.com/content/ICCV2021/papers/Zhao_3DVG-Transformer_Relation_Modeling_for_Visual_Grounding_on_Point_Clouds_ICCV_2021_paper.pdf)
* [Differentiable Convolution Search for Point Cloud Processing](http://arxiv.org/abs/2108.12856)
* [Superpoint Network for Point Cloud Oversegmentation](https://openaccess.thecvf.com/content/ICCV2021/papers/Hui_Superpoint_Network_for_Point_Cloud_Oversegmentation_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/fpthink/SPNet)
* [PU-EVA: An Edge-Vector Based Approximation Solution for Flexible-Scale Point Cloud Upsampling](https://openaccess.thecvf.com/content/ICCV2021/papers/Luo_PU-EVA_An_Edge-Vector_Based_Approximation_Solution_for_Flexible-Scale_Point_Cloud_ICCV_2021_paper.pdf)
* [SGMNet: Learning Rotation-Invariant Point Cloud Representations via Sorted Gram Matrix](https://openaccess.thecvf.com/content/ICCV2021/papers/Xu_SGMNet_Learning_Rotation-Invariant_Point_Cloud_Representations_via_Sorted_Gram_Matrix_ICCV_2021_paper.pdf)
* [DWKS: A Local Descriptor of Deformations Between Meshes and Point Clouds](https://openaccess.thecvf.com/content/ICCV2021/papers/Magnet_DWKS_A_Local_Descriptor_of_Deformations_Between_Meshes_and_Point_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/RobinMagnet/DWKS)
* [Robustness Certification for Point Cloud Models](https://arxiv.org/abs/2103.16652)<br>:star:[code](https://github.com/eth-sri/3dcertify)
* [Vector Neurons: A General Framework for SO(3)-Equivariant Networks](https://openaccess.thecvf.com/content/ICCV2021/papers/Deng_Vector_Neurons_A_General_Framework_for_SO3-Equivariant_Networks_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/FlyingGiraffe/vnn)
* [Unsupervised Point Cloud Pre-Training via Occlusion Completion](https://arxiv.org/abs/2010.01089)<br>:star:[code](https://github.com/hansen7/OcCo)
* [Towards Efficient Graph Convolutional Networks for Point Cloud Handling](https://arxiv.org/abs/2104.05706)<br>:star:[code](https://github.com/ofsoundof/EfficientGCN)
* [Progressive Seed Generation Auto-Encoder for Unsupervised Point Cloud Learning](https://openaccess.thecvf.com/content/ICCV2021/papers/Yang_Progressive_Seed_Generation_Auto-Encoder_for_Unsupervised_Point_Cloud_Learning_ICCV_2021_paper.pdf)
* ç‚¹äº‘å»å™ª
  * [Score-Based Point Cloud Denoising](https://arxiv.org/abs/2107.10981)<br>:star:[code](https://github.com/luost26/score-denoise)
* ç‚¹äº‘é…å‡†
  * [HRegNet: A Hierarchical Network for Large-scale Outdoor LiDAR Point Cloud Registration](https://arxiv.org/abs/2107.11992)<br>:star:[code](https://github.com/ispc-lab/HRegNet):house:[project](https://ispc-group.github.io/hregnet)
  * [(Just) A Spoonful of Refinements Helps the Registration Error Go Down](https://arxiv.org/abs/2108.03257https://arxiv.org/abs/2108.03257)<br>:open_mouth:oral:star:[code](https://github.com/SergioRAgostinho/just-a-spoonful)
  * [A Robust Loss for Point Cloud Registration](https://arxiv.org/abs/2108.11682)
  * [Deep Hough Voting for Robust Global Registration](https://arxiv.org/abs/2109.04310)
  * [Sampling Network Guided Cross-Entropy Method for Unsupervised Point Cloud Registration](https://arxiv.org/abs/2109.06619)<br>:star:[code](https://github.com/Jiang-HB/CEMNet)
  * [Feature Interactive Representation for Point Cloud Registration](https://openaccess.thecvf.com/content/ICCV2021/papers/Wu_Feature_Interactive_Representation_for_Point_Cloud_Registration_ICCV_2021_paper.pdf)
  * [LSG-CPD: Coherent Point Drift With Local Surface Geometry for Point Cloud Registration](https://openaccess.thecvf.com/content/ICCV2021/papers/Liu_LSG-CPD_Coherent_Point_Drift_With_Local_Surface_Geometry_for_Point_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/ChirikjianLab/LSG-CPD):tv:[video](https://www.youtube.com/watch?v=1lxz9Uu-GXI)
  * [OMNet: Learning Overlapping Mask for Partial-to-Partial Point Cloud Registration](https://arxiv.org/abs/2103.00937)<br>:star:[code](https://github.com/megvii-research/OMNet)
  * [DeepPRO: Deep Partial Point Cloud Registration of Objects](https://openaccess.thecvf.com/content/ICCV2021/papers/Lee_DeepPRO_Deep_Partial_Point_Cloud_Registration_of_Objects_ICCV_2021_paper.pdf)
  * [Provably Approximated Point Cloud Registration](https://openaccess.thecvf.com/content/ICCV2021/papers/Jubran_Provably_Approximated_Point_Cloud_Registration_ICCV_2021_paper.pdf)
  * [Bootstrap Your Own Correspondences](https://arxiv.org/abs/2106.00677)ç‚¹äº‘é…å‡†
  * [Distinctiveness Oriented Positional Equilibrium for Point Cloud Registration](https://openaccess.thecvf.com/content/ICCV2021/papers/Min_Distinctiveness_Oriented_Positional_Equilibrium_for_Point_Cloud_Registration_ICCV_2021_paper.pdf)
* 3Dç‚¹äº‘
  * [Unsupervised Learning of Fine Structure Generation for 3D Point Clouds by 2D Projection Matching](https://arxiv.org/abs/2108.03746)<br>:star:[code](https://github.com/chenchao15/2D_projection_matching)
  * [Spatio-temporal Self-Supervised Representation Learning for 3D Point Clouds](https://arxiv.org/abs/2109.00179)<br>:star:[code](https://github.com/yichen928/STRL):house:[project](https://siyuanhuang.com/STRL/)
  * [Unsupervised Learning of Fine Structure Generation for 3D Point Clouds by 2D Projections Matching](https://arxiv.org/abs/2108.03746)<br>:star:[code](https://github.com/chenchao15/2D_projection_matching)
  * [Point Transformer](https://arxiv.org/abs/2012.09164)
  * [Point-Set Distances for Learning Representations of 3D Point Clouds](https://arxiv.org/abs/2102.04014)
  * [PointBA: Towards Backdoor Attacks in 3D Point Cloud](https://arxiv.org/abs/2103.16074)
  * [Minimal Adversarial Examples for Deep Learning on 3D Point Clouds](https://arxiv.org/abs/2008.12066)
  * 3Dç‚¹äº‘é‡å»º
    * [MonteFloor: Extending MCTS for Reconstructing Accurate Large-Scale Floor Plans](https://arxiv.org/abs/2103.11161)<br>:open_mouth:oral:house:[project](https://www.tugraz.at/index.php?id=52770):tv:[video](https://youtu.be/RJi4v5nQnfE)
* ç‚¹äº‘è¡¥å…¨
  * [SnowflakeNet: Point Cloud Completion by Snowflake Point Deconvolution with Skip-Transformer](https://arxiv.org/abs/2108.04444)<br>:open_mouth:oral:star:[code](https://github.com/AllenXiangX/SnowflakeNet)
  * [ME-PCN: Point Completion Conditioned on Mask Emptiness](https://arxiv.org/abs/2108.08187)
  * [PoinTr: Diverse Point Cloud Completion with Geometry-Aware Transformers](https://arxiv.org/abs/2108.08839)<br>:open_mouth:oral:star:[code](https://github.com/yuxumin/PoinTr)
  * [Voxel-based Network for Shape Completion by Leveraging Edge Generation](https://arxiv.org/abs/2108.09936)<br>:star:[code](https://github.com/xiaogangw/VE-PCN)
  * [RFNet: Recurrent Forward Network for Dense Point Cloud Completion](https://arxiv.org/abs/2104.00820)
* ç‚¹äº‘å¢å¼º
  * [Point Cloud Augmentation with Weighted Local Transformations](https://arxiv.org/abs/2110.05379)<br>:star:[code](https://github.com/mlvlab/PointWOLF)
* ç‚¹äº‘å½¢çŠ¶åˆ†æ
  * [Walk in the Cloud: Learning Curves for Point Clouds Shape Analysis](https://arxiv.org/abs/2105.01288)<br>:star:[code](https://github.com/tiangexiang/CurveNet):house:[project](https://curvenet.github.io/)
* ç‚¹äº‘åˆ†æ
  * [A Closer Look at Rotation-Invariant Deep Point Cloud Analysis](https://openaccess.thecvf.com/content/ICCV2021/papers/Li_A_Closer_Look_at_Rotation-Invariant_Deep_Point_Cloud_Analysis_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/SILI1994/rotation-invariant-pointcloud-analysis)
* 3Dç‚¹äº‘åˆ†ç±»
  * [A Backdoor Attack Against 3D Point Cloud Classifiers](http://arxiv.org/abs/2104.05808)<br>:star:[code](https://github.com/zhenxianglance/PCBA)
* 3Dç‚¹äº‘ç”Ÿæˆä¸è¡¥å…¨
  * [3D Shape Generation and Completion through Point-Voxel Diffusion](https://arxiv.org/abs/2104.03670)<br>:house:[project](https://alexzhou907.github.io/pvd):tv:[video](https://youtu.be/64jl79i6HNY)
* point cloud object co-segmentation
  * [Unsupervised Point Cloud Object Co-Segmentation by Co-Contrastive Learning and Mutual Attention Sampling](https://openaccess.thecvf.com/content/ICCV2021/papers/Yang_Unsupervised_Point_Cloud_Object_Co-Segmentation_by_Co-Contrastive_Learning_and_Mutual_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/jimmy15923/unsup_point_coseg)
* ç‚¹äº‘ç†è§£
  * [Shape Self-Correction for Unsupervised Point Cloud Understanding](https://openaccess.thecvf.com/content/ICCV2021/papers/Chen_Shape_Self-Correction_for_Unsupervised_Point_Cloud_Understanding_ICCV_2021_paper.pdf)


<a name="5"/>

## 5.Few-Shot/Zero-Shot Learning;Domain Generalization/Adaptation(å°/é›¶æ ·æœ¬å­¦ä¹ ;åŸŸé€‚åº”/æ³›åŒ–)
* åŸŸé€‚åº”
  * [Transporting Causal Mechanisms for Unsupervised Domain Adaptation](https://arxiv.org/abs/2107.11055)<br>:open_mouth:oral<br>:star:[code](https://github.com/yue-zhongqi/tcm)
  * [Generalized Source-free Domain Adaptation](https://arxiv.org/abs/2108.01614)<br>:star:[code](https://github.com/Albert0147/G-SFDA)
  * [Semantic Concentration for Domain Adaptation](https://arxiv.org/abs/2108.05720)<br>:star:[code](https://github.com/BIT-DA)
  * [PIT: Position-Invariant Transform for Cross-FoV Domain Adaptation](https://arxiv.org/abs/2108.07142)<br>:star:[code](https://github.com/sheepooo/PIT-Position-Invariant-Transform)
  * [Learning Cross-modal Contrastive Features for Video Domain Adaptation](https://arxiv.org/abs/2108.11974)
  * [Zero-Shot Day-Night Domain Adaptation With a Physics Prior](https://openaccess.thecvf.com/content/ICCV2021/papers/Lengyel_Zero-Shot_Day-Night_Domain_Adaptation_With_a_Physics_Prior_ICCV_2021_paper.pdf)<br>:open_mouth:oral:star:[code](https://github.com/Attila94/CIConv)
  * [Active Universal Domain Adaptation](https://openaccess.thecvf.com/content/ICCV2021/papers/Ma_Active_Universal_Domain_Adaptation_ICCV_2021_paper.pdf)
  * [Re-energizing Domain Discriminator with Sample Relabeling for Adversarial Domain Adaptation](https://arxiv.org/abs/2103.11661)
  * [OVANet: One-vs-All Network for Universal Domain Adaptation](https://arxiv.org/abs/2104.03344)<br>:star:[code](https://github.com/VisionLearningGroup/OVANet)
  * [Collaborative Optimization and Aggregation for Decentralized Domain Generalization and Adaptation](https://openaccess.thecvf.com/content/ICCV2021/papers/Wu_Collaborative_Optimization_and_Aggregation_for_Decentralized_Domain_Generalization_and_Adaptation_ICCV_2021_paper.pdf)
  * [Partial Video Domain Adaptation with Partial Adversarial Temporal Attentive Network](https://arxiv.org/abs/2107.04941)<br>:star:[code](https://xuyu0010.github.io/pvda.html)
  * [Information-Theoretic Regularization for Multi-Source Domain Adaptation](https://arxiv.org/abs/2104.01568)
  * [Gradient Distribution Alignment Certificates Better Adversarial Domain Adaptation](https://openaccess.thecvf.com/content/ICCV2021/papers/Gao_Gradient_Distribution_Alignment_Certificates_Better_Adversarial_Domain_Adaptation_ICCV_2021_paper.pdf)
  * [Adaptive Adversarial Network for Source-Free Domain Adaptation](https://openaccess.thecvf.com/content/ICCV2021/papers/Xia_Adaptive_Adversarial_Network_for_Source-Free_Domain_Adaptation_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/HaifengXia/SFDA)
  * [T-SVDNet: Exploring High-Order Prototypical Correlations for Multi-Source Domain Adaptation](https://openaccess.thecvf.com/content/ICCV2021/papers/Li_T-SVDNet_Exploring_High-Order_Prototypical_Correlations_for_Multi-Source_Domain_Adaptation_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/lslrh/T-SVDNet)
  * [Self-Supervised Domain Adaptation for Forgery Localization of JPEG Compressed Images](https://openaccess.thecvf.com/content/ICCV2021/papers/Rao_Self-Supervised_Domain_Adaptation_for_Forgery_Localization_of_JPEG_Compressed_Images_ICCV_2021_paper.pdf)
  * [ECACL: A Holistic Framework for Semi-Supervised Domain Adaptation](https://arxiv.org/abs/2104.09136)<br>:star:[code](https://github.com/kailigo/pacl)
  * [STEM: An approach to Multi-source Domain Adaptation with Guarantees](https://openaccess.thecvf.com/content/ICCV2021/papers/Nguyen_STEM_An_Approach_to_Multi-Source_Domain_Adaptation_With_Guarantees_ICCV_2021_paper.pdf)
  * [Towards Novel Target Discovery Through Open-Set Domain Adaptation](https://arxiv.org/abs/2105.02432)<br>:star:[code](https://github.com/scottjingtt/SROSDA)
  * [Deep Co-Training With Task Decomposition for Semi-Supervised Domain Adaptation](https://arxiv.org/abs/2007.12684)<br>:star:[code](https://github.com/LoyoYang/DeCoTa)
  * [mDALU: Multi-Source Domain Adaptation and Label Unification with Partial Datasets](https://arxiv.org/abs/2012.08385)
  * [Geometry-Aware Self-Training for Unsupervised Domain Adaptation on Object Point Clouds](https://openaccess.thecvf.com/content/ICCV2021/papers/Zou_Geometry-Aware_Self-Training_for_Unsupervised_Domain_Adaptation_on_Object_Point_Clouds_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/zou-longkun/GAST)
  * æ— ç›‘ç£åŸŸé€‚åº”
    * [Adversarial Unsupervised Domain Adaptation with Conditional and Label Shift: Infer, Align and Iterate](https://arxiv.org/abs/2107.13469)
    * [Recursively Conditional Gaussian for Ordinal Unsupervised Domain Adaptation](https://arxiv.org/abs/2107.13467)<br>:open_mouth:oral
    * [Tune it the Right Way: Unsupervised Validation of Domain Adaptation via Soft Neighborhood Density](https://arxiv.org/abs/2108.10860)<br>:star:[code](https://github.com/VisionLearningGroup/SND)
    * [Adversarial Robustness for Unsupervised Domain Adaptation](https://arxiv.org/abs/2109.00946)<br>:house:[project](http://deepawais.com/robust_uda/)
    * [SENTRY: Selective Entropy Optimization via Committee Consistency for Unsupervised Domain Adaptation](https://arxiv.org/abs/2012.11460)<br>:star:[code](https://github.com/virajprabhu/SENTRY)
  * é›¶æ ·æœ¬åŸŸé€‚åº”
    * [Zero-Shot Domain Adaptation with a Physics Prior](https://arxiv.org/abs/2108.05137)<br>:open_mouth:oral:star:[code](https://github.com/Attila94/CIConv)
    * [Collaborative Learning with Disentangled Features for Zero-shot Domain Adaptation](https://openaccess.thecvf.com/content/ICCV2021/papers/Jhoo_Collaborative_Learning_With_Disentangled_Features_for_Zero-Shot_Domain_Adaptation_ICCV_2021_paper.pdf)
* åŸŸæ³›åŒ–
  * [Domain Generalization via Gradient Surgery](https://arxiv.org/abs/2108.01621)<br>:star:[code](https://github.com/lucasmansilla/DGvGS)
  * [Learning to Diversify for Single Domain Generalization](https://arxiv.org/abs/2108.11726)<br>:star:[code](https://github.com/BUserName/Learning_to_diversify)
  * [Shape-Biased Domain Generalization via Shock Graph Embeddings](https://arxiv.org/abs/2109.05671)
  * [SelfReg: Self-Supervised Contrastive Regularization for Domain Generalization](https://openaccess.thecvf.com/content/ICCV2021/papers/Kim_SelfReg_Self-Supervised_Contrastive_Regularization_for_Domain_Generalization_ICCV_2021_paper.pdf)
  * [A Style and Semantic Memory Mechanism for Domain Generalization](https://openaccess.thecvf.com/content/ICCV2021/papers/Chen_A_Style_and_Semantic_Memory_Mechanism_for_Domain_Generalization_ICCV_2021_paper.pdf)
  * [Confidence Calibration for Domain Generalization Under Covariate Shift](https://arxiv.org/abs/2104.00742)
  * [A Simple Feature Augmentation for Domain Generalization](https://openaccess.thecvf.com/content/ICCV2021/papers/Li_A_Simple_Feature_Augmentation_for_Domain_Generalization_ICCV_2021_paper.pdf)
* å°æ ·æœ¬
  * [Boosting the Generalization Capability in Cross-Domain Few-shot Learning via Noise-enhanced Supervised Autoencoder](https://arxiv.org/abs/2108.05028)
  * [Generalized and Incremental Few-Shot Learning by Explicit Learning and Calibration without Forgetting](https://arxiv.org/abs/2108.08165)<br>:star:[code](https://github.com/annusha/LCwoF)
  * [Meta Navigator: Search for a Good Adaptation Policy for Few-shot Learning](https://arxiv.org/abs/2109.05749)
  * [Meta-Learning with Task-Adaptive Loss Function for Few-Shot Learning](https://arxiv.org/abs/2110.03909)<br>:open_mouth:oral:star:[code](https://github.com/baiksung)
  * [Z-Score Normalization, Hubness, and Few-Shot Learning](https://openaccess.thecvf.com/content/ICCV2021/papers/Fei_Z-Score_Normalization_Hubness_and_Few-Shot_Learning_ICCV_2021_paper.pdf)
  * [Pseudo-Loss Confidence Metric for Semi-Supervised Few-Shot Learning](https://openaccess.thecvf.com/content/ICCV2021/papers/Huang_Pseudo-Loss_Confidence_Metric_for_Semi-Supervised_Few-Shot_Learning_ICCV_2021_paper.pdf)
  * [Curvature Generation in Curved Spaces for Few-Shot Learning](https://openaccess.thecvf.com/content/ICCV2021/papers/Gao_Curvature_Generation_in_Curved_Spaces_for_Few-Shot_Learning_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/ZhiGaomcislab/CurvatureGeneration_FSL)
  * [Task-Aware Part Mining Network for Few-Shot Learning](https://openaccess.thecvf.com/content/ICCV2021/papers/Wu_Task-Aware_Part_Mining_Network_for_Few-Shot_Learning_ICCV_2021_paper.pdf)
  * [Meta-Baseline: Exploring Simple Meta-Learning for Few-Shot Learning](https://openaccess.thecvf.com/content/ICCV2021/papers/Chen_Meta-Baseline_Exploring_Simple_Meta-Learning_for_Few-Shot_Learning_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/yinboc/few-shot-meta-baseline)
  * [UVStyle-Net: Unsupervised Few-Shot Learning of 3D Style Similarity Measure for B-Reps](https://openaccess.thecvf.com/content/ICCV2021/papers/Meltzer_UVStyle-Net_Unsupervised_Few-Shot_Learning_of_3D_Style_Similarity_Measure_for_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/AutodeskAILab/UVStyle-Net)
  * [Shallow Bayesian Meta Learning for Real-World Few-Shot Recognition](https://arxiv.org/abs/2101.02833)<br>:star:[code](https://github.com/Open-Debin/Bayesian_MQDA)
  * [Iterative Label Cleaning for Transductive and Semi-Supervised Few-Shot Learning](https://arxiv.org/abs/2012.07962)<br>:star:[code](https://github.com/MichalisLazarou/iLPC)
  * [Coarsely-labeled Data for Better Few-shot Transfer](https://openaccess.thecvf.com/content/ICCV2021/papers/Phoo_Coarsely-Labeled_Data_for_Better_Few-Shot_Transfer_ICCV_2021_paper.pdf)br>:star:[code](https://github.com/cpphoo/PAS)
  * å°æ ·æœ¬å¼‚å¸¸æ£€æµ‹
    * [A Hierarchical Transformation-Discriminating Generative Model for Few Shot Anomaly Detection](https://arxiv.org/abs/2104.14535)
* Zero-Shot Learning(é›¶æ ·æœ¬å­¦ä¹ )
  * [Discriminative Region-based Multi-Label Zero-Shot Learning](https://arxiv.org/abs/2108.09301)<br>:star:[code](https://github.com/akshitac8/BiAM)
  * [Field-Guide-Inspired Zero-Shot Learning](https://arxiv.org/abs/2108.10967)
  * Generalized Zero-Shot Learning(å¹¿ä¹‰é›¶æ ·æœ¬å­¦ä¹ )
    * [FREE: Feature Refinement for Generalized Zero-Shot Learning](https://arxiv.org/abs/2107.13807)<br>:star:[code](https://github.com/shiming-chen/FREE)
    * [Semantics Disentangling for Generalized Zero-Shot Learning](https://arxiv.org/abs/2101.07978)<br>:star:[code](https://github.com/uqzhichen/SDGZSL)
 
<a name="4"/>

## 4.Neural rendering(ç¥ç»æ¸²æŸ“)
* [In-Place Scene Labelling and Understanding with Implicit Scene Representation](https://arxiv.org/abs/2103.15875)<br>:open_mouth:oral:house:[project](https://shuaifengzhi.com/Semantic-NeRF/):tv:[video](https://www.youtube.com/watch?v=FpShWO7LVbM)
* [Differentiable Surface Rendering via Non-Differentiable Sampling](https://arxiv.org/abs/2108.04886)
* [Self-Calibrating Neural Radiance Fields](https://arxiv.org/abs/2108.13826)<br>:star:[code](https://github.com/POSTECH-CVLab/SCNeRF)
* [NerfingMVS: Guided Optimization of Neural Radiance Fields for Indoor Multi-view Stereo](https://arxiv.org/abs/2109.01129)<br>:open_mouth:oral:star:[code](https://github.com/weiyithu/NerfingMVS):house:[project](https://weiyithu.github.io/NerfingMVS/)
* [Learning Object-Compositional Neural Radiance Field for Editable Scene Rendering](https://arxiv.org/abs/2109.01847)<br>:star:[code](https://github.com/zju3dv/object_nerf):house:[project](https://zju3dv.github.io/object_nerf/)
* [CodeNeRF: Disentangled Neural Radiance Fields for Object Categories](https://arxiv.org/abs/2109.01750)<br>:star:[code](https://github.com/wayne1123/code-nerf)
* [MVSNeRF: Fast Generalizable Radiance Field Reconstruction from Multi-View Stereo](https://arxiv.org/abs/2103.15595)<br>:star:[code](https://github.com/apchenstu/mvsnerf):house:[project](https://apchenstu.github.io/mvsnerf/):tv:[video](https://youtu.be/3M3edNiaGsA)
* [PlenOctrees for Real-Time Rendering of Neural Radiance Fields](https://arxiv.org/abs/2103.14024)<br>:open_mouth:oral:star:[Conversion Code](https://github.com/sxyu/plenoctree):star:[Viewer Code](https://github.com/sxyu/volrend):house:[project](https://alexyu.net/plenoctrees/):tv:[video](https://youtu.be/obrmH1T5mfI)
* [Neural Radiance Flow for 4D View Synthesis and Video Processing](https://arxiv.org/abs/2012.09790)<br>:star:[code](https://github.com/yilundu/nerflow):house:[project](https://yilundu.github.io/nerflow/)
* [Animatable Neural Radiance Fields for Modeling Dynamic Human Bodies](https://openaccess.thecvf.com/content/ICCV2021/papers/Peng_Animatable_Neural_Radiance_Fields_for_Modeling_Dynamic_Human_Bodies_ICCV_2021_paper.pdf)star:[code](https://github.com/zju3dv/animatable_nerf):house:[project](https://zju3dv.github.io/animatable_nerf/):tv:[video](https://zju3dv.github.io/animatable_nerf/)
* [GNeRF: GAN-Based Neural Radiance Field Without Posed Camera](https://arxiv.org/abs/2103.15606)<br>:open_mouth:oral
* [BARF: Bundle-Adjusting Neural Radiance Fields](https://arxiv.org/abs/2104.06405)<br>:open_mouth:oral:star:[code](https://github.com/chenhsuanlin/bundle-adjusting-NeRF):house:[project](https://chenhsuanlin.bitbucket.io/bundle-adjusting-NeRF/)
* [FastNeRF: High-Fidelity Neural Rendering at 200FPS](https://arxiv.org/abs/2103.10380)<br>:house:[project](https://microsoft.github.io/FastNeRF/):tv:[video](https://youtu.be/JS5H-Usiphg)
* [PIRenderer: Controllable Portrait Image Generation via Semantic Neural Rendering](https://arxiv.org/abs/2109.08379)<br>:star:[code](https://github.com/RenYurui/PIRender):tv:[video](https://www.youtube.com/watch?v=gDhcRcPI1JU&feature=youtu.be)
* [NeRD: Neural Reflectance Decomposition from Image Collections](https://arxiv.org/abs/2012.03918)<br>:star:[code](https://github.com/cgtuebingen/NeRD-Neural-Reflectance-Decomposition):house:[project](https://markboss.me/publication/2021-nerd/):tv:[video](https://www.youtube.com/watch?v=JL-qMTXw9VU)
* [Editing Conditional Radiance Fields](https://arxiv.org/abs/2105.06466)<br>:star:[code](https://github.com/stevliu/editnerf):house:[project](http://editnerf.csail.mit.edu/):tv:[video](https://www.youtube.com/watch?v=9qwRD4ejOpw)
* [GRF: Learning a General Radiance Field for 3D Representation and Rendering](https://arxiv.org/abs/2010.04595)<br>:star:[code](https://github.com/alextrevithick/GRF)
* [4DComplete: Non-Rigid Motion Estimation Beyond the Observable Surface](https://arxiv.org/abs/2105.01905)<br>:star:[code](https://github.com/rabbityl/DeformingThings4D):tv:[video](https://www.youtube.com/watch?v=QrSsVoTRpWk)
* [KiloNeRF: Speeding up Neural Radiance Fields with Thousands of Tiny MLPs](https://arxiv.org/abs/2103.13744)<br>:star:[code](https://github.com/creiser/kilonerf/)
* [Neural Articulated Radiance Field](https://arxiv.org/abs/2104.03110)<br>:star:[code](https://github.com/nogu-atsu/NARF)
* [Baking Neural Radiance Fields for Real-Time View Synthesis](https://arxiv.org/abs/2103.14645)<br>:house:[project](https://phog.github.io/snerg/):tv:[video](https://www.youtube.com/watch?v=5jKry8n5YO8)
* [Non-Rigid Neural Radiance Fields: Reconstruction and Novel View Synthesis of a Dynamic Scene From Monocular Video](https://arxiv.org/abs/2012.12247)<br>:star:[code](https://github.com/facebookresearch/nonrigid_nerf):house:[project](https://vcai.mpi-inf.mpg.de/projects/nonrigid_nerf/):tv:[video]
* [Nerfies: Deformable Neural Radiance Fields](https://arxiv.org/abs/2011.12948)<br>:star:[code](https://github.com/google/nerfies):house:[project](https://nerfies.github.io/):tv:[video](https://www.youtube.com/watch?v=MrKrnHhk8IA)
* [Mip-NeRF: A Multiscale Representation for Anti-Aliasing Neural Radiance Fields](https://openaccess.thecvf.com/content/ICCV2021/papers/Barron_Mip-NeRF_A_Multiscale_Representation_for_Anti-Aliasing_Neural_Radiance_Fields_ICCV_2021_paper.pdf)
* [UNISURF: Unifying Neural Implicit Surfaces and Radiance Fields for Multi-View Reconstruction](https://arxiv.org/abs/2104.10078)<br>:open_mouth:oral:star:[code](https://github.com/autonomousvision/unisurf):house:[project](https://moechsle.github.io/unisurf/):tv:[video](https://www.youtube.com/watch?v=WXUfHvZge0E)
* 3Dæ¸²æŸ“
  * [GANcraft: Unsupervised 3D Neural Rendering of Minecraft Worlds](https://arxiv.org/abs/2104.07659)<br>:open_mouth:oral:star:[code](https://github.com/NVlabs/imaginaire):house:[project](https://nvlabs.github.io/GANcraft/):tv:[video](https://youtu.be/5K-AgDmCtt0)
* 3D photography(3D ç›¸ç‰‡)
  * [SLIDE: Single Image 3D Photography with Soft Layering and Depth-aware Inpainting](https://arxiv.org/abs/2109.01068)<br>:open_mouth:oral:house:[project](https://varunjampani.github.io/slide/):tv:[video](https://www.youtube.com/watch?v=RQio7q-ueY8)
* æ¸²æŸ“
  * [EgoRenderer: Rendering Human Avatars from Egocentric Camera Images](https://openaccess.thecvf.com/content/ICCV2021/papers/Hu_EgoRenderer_Rendering_Human_Avatars_From_Egocentric_Camera_Images_ICCV_2021_paper.pdf)

<a name="3"/>

## 3.Image Clustering(å›¾åƒèšç±»)
* [Clustering by Maximizing Mutual Information Across Views](https://arxiv.org/abs/2107.11635)
* [Learning Hierarchical Graph Neural Networks for Image Clustering](https://arxiv.org/abs/2107.01319)<br>:star:[code](https://github.com/dmlc/dgl/tree/master/examples/pytorch/hilander)
* [One-Pass Multi-View Clustering for Large-Scale Data](https://openaccess.thecvf.com/content/ICCV2021/papers/Liu_One-Pass_Multi-View_Clustering_for_Large-Scale_Data_ICCV_2021_paper.pdf)
* [End-to-End Robust Joint Unsupervised Image Alignment and Clustering](https://openaccess.thecvf.com/content/ICCV2021/papers/Zeng_End-to-End_Robust_Joint_Unsupervised_Image_Alignment_and_Clustering_ICCV_2021_paper.pdf)
* [Graph Contrastive Clustering](https://arxiv.org/abs/2104.01429)<br>:star:[code](https://github.com/mynameischaos/GCC)
* äººè„¸èšç±»
  * [Learn To Cluster Faces via Pairwise Classification](https://openaccess.thecvf.com/content/ICCV2021/papers/Liu_Learn_To_Cluster_Faces_via_Pairwise_Classification_ICCV_2021_paper.pdf)

<a name="2"/>

## 2.Sign Language(æ‰‹è¯­è¯†åˆ«)
* [Mixed SIGNals: Sign Language Production via a Mixture of Motion Primitives](https://arxiv.org/abs/2107.11317)
* [SignBERT: Pre-Training of Hand-Model-Aware Representation for Sign Language Recognition](https://arxiv.org/abs/2110.05382)
* [Self-Mutual Distillation Learning for Continuous Sign Language Recognition](https://openaccess.thecvf.com/content/ICCV2021/papers/Hao_Self-Mutual_Distillation_Learning_for_Continuous_Sign_Language_Recognition_ICCV_2021_paper.pdf)
* [Visual Alignment Constraint for Continuous Sign Language Recognition](https://arxiv.org/abs/2104.02330)<br>:star:[code](https://github.com/ycmin95/VAC_CSLR)
* æ‰‹è¯­ç¿»è¯‘
  * [Stochastic Transformer Networks With Linear Competing Units: Application To End-to-End SL Translation](https://arxiv.org/abs/2109.13318)

<a name="1"/>

## 1.Other(å…¶å®ƒ)
* [Bias Loss for Mobile Neural Networks](https://arxiv.org/abs/2107.11170)<br>:star:[code](https://github.com/lusinlu/biasloss_skipblocknet)
* [Improve Unsupervised Pretraining for Few-label Transfer](https://arxiv.org/abs/2107.12369)
* [Temporal-wise Attention Spiking Neural Networks for Event Streams Classification](https://arxiv.org/abs/2107.11711)
* [Accelerating Atmospheric Turbulence Simulation via Learned Phase-to-Space Transform](https://arxiv.org/abs/2107.11627)
* [Energy-Based Open-World Uncertainty Modeling for Confidence Calibration](https://arxiv.org/abs/2107.12628)
* [Robustness via Cross-Domain Ensembles](https://arxiv.org/abs/2103.10919)<br>:open_mouth:oral:star:[code](https://github.com/EPFL-VILAB/XDEnsembles):house:[project](https://crossdomain-ensembles.epfl.ch/):tv:[video](https://youtu.be/h0FI5Sp7y7g)
* [Warp Consistency for Unsupervised Learning of Dense Correspondences](https://arxiv.org/abs/2104.03308)<br>:open_mouth:oral:star:[code](https://github.com/PruneTruong/DenseMatching)
* [Few-Shot and Continual Learning with Attentive Independent Mechanisms](https://arxiv.org/abs/2107.14053)<br>:star:[code](https://github.com/huang50213/AIM-Fewshot-Continual)
* [Out-of-Core Surface Reconstruction via Global TGV Minimization](https://arxiv.org/abs/2107.14790)
* [ELLIPSDF: Joint Object Pose and Shape Optimization with a Bi-level Ellipsoid and Signed Distance Function Description](https://arxiv.org/abs/2108.00355)
* [Multi-scale Matching Networks for Semantic Correspondence](https://arxiv.org/abs/2108.00211)<br>:star:[code](https://github.com/wintersun661/MMNet)
* [Learning with Noisy Labels via Sparse Regularization](https://arxiv.org/abs/2108.00192)<br>:star:[code](https://github.com/hitcszx/lnl_sr)
* [CanvasVAE: Learning to Generate Vector Graphic Documents](https://arxiv.org/abs/2108.01249)
* [Toward Spatially Unbiased Generative Models](https://arxiv.org/abs/2108.01285)<br>:star:[code](https://github.com/jychoi118/toward_spatial_unbiased)
* [Learning Compatible Embeddings](https://arxiv.org/abs/2108.01958)<br>:star:[code](https://github.com/IrvingMeng/LCE)
* [Instance Similarity Learning for Unsupervised Feature Representation](https://arxiv.org/abs/2108.02721)<br>:star:[code](https://github.com/ZiweiWangTHU/ISL)
* [Generalizable Mixed-Precision Quantization via Attribution Rank Preservation](https://arxiv.org/abs/2108.02720)<br>:star:[code](https://github.com/ZiweiWangTHU/GMPQ)
* [Unifying Nonlocal Blocks for Neural Networks](https://arxiv.org/abs/2108.02451)<br>:star:[code](https://github.com/zh460045050/SNL_ICCV2021)
* [Impact of Aliasing on Generalization in Deep Convolutional Networks](https://arxiv.org/abs/2108.03489)
* [NASOA: Towards Faster Task-oriented Online Fine-tuning with a Zoo of Models](https://arxiv.org/abs/2108.03434)<br>:star:[code](https://github.com/NAS-OA/NASOA)
* [ProAI: An Efficient Embedded AI Hardware for Automotive Applications - a Benchmark Study](https://arxiv.org/abs/2108.05170)
* [m-RevNet: Deep Reversible Neural Networks with Momentum](https://arxiv.org/abs/2108.05862) [æ¶‰å«Œå­¦æœ¯ä¸ç«¯ï¼Œå·²ç”³è¯·æ’¤ç¨¿](https://www.zhihu.com/question/480075870/answer/2064860328)
* [Continual Neural Mapping: Learning An Implicit Scene Representation from Sequential Observations](https://arxiv.org/abs/2108.05851)
* [MT-ORL: Multi-Task Occlusion Relationship Learning](https://arxiv.org/abs/2108.05722)<br>:star:[code](https://github.com/fengpanhe/MT-ORL) 
* [Finding Representative Interpretations on Convolutional Neural Networks](https://arxiv.org/abs/2108.06384)
* [Orthogonal Jacobian Regularization for Unsupervised Disentanglement in Image Generation](https://arxiv.org/abs/2108.07668)<br>:star:[code](https://github.com/csyxwei/OroJaR)
* [PR-RRN: Pairwise-Regularized Residual-Recursive Networks for Non-rigid Structure-from-Motion](https://arxiv.org/abs/2108.07506)
* [Instance Segmentation in 3D Scenes using Semantic Superpoint Tree Networks](https://arxiv.org/abs/2108.07478)<br>:star:[code](https://github.com/Gorilla-Lab-SCUT)
* [Learning RAW-to-sRGB Mappings with Inaccurately Aligned Supervision](https://arxiv.org/abs/2108.08119)<br>:star:[code](https://github.com/cszhilu1998/RAW-to-sRGB)
* [Structured Outdoor Architecture Reconstruction by Exploration and Classification](https://arxiv.org/abs/2108.07990)
* [Global Pooling, More than Meets the Eye: Position Information is Encoded Channel-Wise in CNNs](https://arxiv.org/abs/2108.07884)<br>:star:[code](https://github.com/islamamirul/PermuteNet)
* [A New Journey from SDRTV to HDRTV](https://arxiv.org/abs/2108.07978)<br>:star:[code](https://github.com/chxy95/HDRTVNet)
* [A Simple Framework for 3D Lensless Imaging with Programmable Masks](https://arxiv.org/abs/2108.07966)<br>:star:[code](https://github.com/CSIPlab/Programmable3Dcam)
* [Causal Attention for Unbiased Visual Recognition](https://arxiv.org/abs/2108.08782)<br>:star:[code](https://github.com/Wangt-CN/CaaM)
* [Learning to Match Features with Seeded Graph Matching Network](https://arxiv.org/abs/2108.08771)<br>:star:[code](https://github.com/vdvchen/SGMNet)
* [Amplitude-Phase Recombination: Rethinking Robustness of Convolutional Neural Networks in Frequency Domain](https://arxiv.org/abs/2108.08487)
* [PatchMatch-RL: Deep MVS with Pixelwise Depth, Normal, and Visibility](https://arxiv.org/abs/2108.08943)<br>:open_mouth:oral
* [Towards Understanding the Generative Capability of Adversarially Robust Classifiers](https://arxiv.org/abs/2108.09093)<br>:open_mouth:oral
* [Ranking Models in Unlabeled New Environments](https://arxiv.org/abs/2108.10310)<br>:star:[code](https://github.com/sxzrt/Proxy-Set)
* [Learning of Visual Relations: The Devil is in the Tails](https://arxiv.org/abs/2108.09668)<br>:house:[project](http://www.svcl.ucsd.edu/projects/DT2-ACBS/)
* [BlockCopy: High-Resolution Video Processing with Block-Sparse Feature Propagation and Online Policies](https://arxiv.org/abs/2108.09376)<br>:star:[code](https://github.com/thomasverelst/blockcopy-video-processing-pytorch)
* [Patch2CAD: Patchwise Embedding Learning for In-the-Wild Shape Retrieval from a Single Image](https://arxiv.org/abs/2108.09368)
* å»åå·®
  * [BiaSwap: Removing dataset bias with bias-tailored swapping augmentation](https://arxiv.org/abs/2108.10008)
* [Full-Velocity Radar Returns by Radar-Camera Fusion](https://arxiv.org/abs/2108.10637)
* [CSG-Stump: A Learning Friendly CSG-Like Representation for Interpretable Shape Parsing](https://arxiv.org/abs/2108.11305)<br>:star:[code](https://github.com/kimren227/CSGStumpNet):house:[project](https://kimren227.github.io/projects/CSGStump/)
* [NGC: A Unified Framework for Learning with Open-World Noisy Data](https://arxiv.org/abs/2108.11035)
* [LocTex: Learning Data-Efficient Visual Representations from Localized Textual Supervision](https://arxiv.org/abs/2108.11950)<br>:house:[project](https://loctex.mit.edu/)
* [Unsupervised Dense Deformation Embedding Network for Template-Free Shape Correspondence](https://arxiv.org/abs/2108.11609)
* [Lifelong Infinite Mixture Model Based on Knowledge-Driven Dirichlet Process](https://arxiv.org/abs/2108.12278)<br>:star:[code](https://github.com/dtuzi123/Lifelong-infinite-mixture-model)
* [Digging into Uncertainty in Self-supervised Multi-view Stereo](https://arxiv.org/abs/2108.12966)
* [Learning to Discover Reflection Symmetry via Polar Matching Convolution](https://arxiv.org/abs/2108.12952)<br>:star:[code](https://github.com/ahyunSeo/PMCNet):house:[project](http://cvlab.postech.ac.kr/research/PMCNet/)
* [A Dual Adversarial Calibration Framework for Automatic Fetal Brain Biometry](https://arxiv.org/abs/2108.12719)
* [The Functional Correspondence Problem](https://arxiv.org/abs/2109.01097)
* [The Animation Transformer: Visual Correspondence via Segment Matching](https://arxiv.org/abs/2109.02614)
* [Parsing Table Structures in the Wild](https://arxiv.org/abs/2109.02199)<br>:star:[code](https://github.com/wangwen-whu/WTW-Dataset)
* [Square Root Marginalization for Sliding-Window Bundle Adjustment](https://arxiv.org/abs/2109.02182)<br>:star:[code](https://gitlab.com/VladyslavUsenko/basalt):house:[project](https://vision.in.tum.de/research/vslam/rootvo):tv:[video](https://youtu.be/5JCwMSZr4IM)
* [Hierarchical Object-to-Zone Graph for Object Navigation](https://arxiv.org/abs/2109.02066)<br>:star:[code](https://github.com/sx-zhang/HOZ):tv:[video](https://drive.google.com/file/d/1UtTcFRhFZLkqgalKom6_9GpQmsJfXAZC/view)
* [Robustness and Generalization via Generative Adversarial Training](https://arxiv.org/abs/2109.02765)
* [Learning Fast Sample Re-weighting Without Reward Data](https://arxiv.org/abs/2109.03216)<br>:star:[code](https://github.com/google-research/google-research/tree/master/ieg)
* [ReconfigISP: Reconfigurable Camera Image Processing Pipeline](https://arxiv.org/abs/2109.04760)<br>:house:[project](https://www.mmlab-ntu.com/project/reconfigisp/)
* [Learning Indoor Inverse Rendering with 3D Spatially-Varying Lighting](https://arxiv.org/abs/2109.06061)<br>:open_mouth:oral
* [Low-Shot Validation: Active Importance Sampling for Estimating Classifier Performance on Rare Categories](https://arxiv.org/abs/2109.05720)
* [DisUnknown: Distilling Unknown Factors for Disentanglement Learning](https://arxiv.org/abs/2109.08090)<br>:star:[code](https://github.com/stormraiser/disunknown):house:[project](https://stormraiser.github.io/disunknown/)
* [S3VAADA: Submodular Subset Selection for Virtual Adversarial Active Domain Adaptation](https://arxiv.org/abs/2109.08901)<br>:house:[project](https://sites.google.com/iisc.ac.in/s3vaada-iccv2021)
* [ALADIN: All Layer Adaptive Instance Normalization for Fine-grained Style Similarity](https://arxiv.org/abs/2103.09776)<br>:tv:[video](https://www.youtube.com/watch?v=TxE1_juIHqY)
* [Photon-Starved Scene Inference using Single Photon Cameras](https://arxiv.org/abs/2107.11001)<br>:tv:[video](https://www.youtube.com/watch?v=r1YvHnGbi6k)
* [OSCAR-Net: Object-centric Scene Graph Attention for Image Attribution](https://arxiv.org/abs/2108.03541)<br>:star:[code](https://github.com/exnx/oscar):house:[project](https://exnx.github.io/oscar/)
* [Learning to Estimate Hidden Motions with Global Motion Aggregation](https://arxiv.org/abs/2104.02409)<br>:star:[code](https://github.com/zacjiang/GMA):tv:[video](https://www.youtube.com/watch?v=cBNSQ8ZFKSE)
* [Modelling Neighbor Relation in Joint Space-Time Graph for Video Correspondence Learning](https://arxiv.org/abs/2109.13499)
* [Meta Learning on a Sequence of Imbalanced Domains with Difficulty Awareness](https://arxiv.org/abs/2109.14120)<br>:star:[code](https://github.com/joey-wang123/Imbalancemeta) 
* [Procedure Planning in Instructional Videosvia Contextual Modeling and Model-based Policy Learning](https://arxiv.org/abs/2110.01770)<br>:open_mouth:oral
* [Extensions of Karger's Algorithm: Why They Fail in Theory and How They Are Useful in Practice](https://arxiv.org/abs/2110.02750)<br>:open_mouth:oral:star:[code](https://github.com/ejnnr)
* [Neural Strokes: Stylized Line Drawing of 3D Shapes](https://arxiv.org/abs/2110.03900)<br>:star:[code](https://github.com/DifanLiu/NeuralStrokes)
* [Learning Realistic Human Reposing using Cyclic Self-Supervision with 3D Shape, Pose, and Appearance Consistency](https://arxiv.org/abs/2110.05458)
* [Omnidata: A Scalable Pipeline for Making Multi-Task Mid-Level Vision Datasets from 3D Scans](https://arxiv.org/abs/2110.04994)<br>:house:[project](https://omnidata.vision/)
* [Cherry-Picking Gradients: Learning Low-Rank Embeddings of Visual Data via Differentiable Cross-Approximation](https://openaccess.thecvf.com/content/ICCV2021/papers/Usvyatsov_Cherry-Picking_Gradients_Learning_Low-Rank_Embeddings_of_Visual_Data_via_Differentiable_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/aelphy/c-pic)
* [Exploiting Explanations for Model Inversion Attacks](https://arxiv.org/abs/2104.12669)
* [Learning Bias-Invariant Representation by Cross-Sample Mutual Information Minimization](https://arxiv.org/abs/2108.05449)
* [RDI-Net: Relational Dynamic Inference Networks](https://openaccess.thecvf.com/content/ICCV2021/papers/Wang_RDI-Net_Relational_Dynamic_Inference_Networks_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/huanyuhello/RDI-Net)
* [ARAPReg: An As-Rigid-As Possible Regularization Loss for Learning Deformable Shape Generators](https://arxiv.org/abs/2108.09432)<br>:star:[code](https://github.com/GitBoSun/ARAPReg)
* [T-Net: Effective Permutation-Equivariant Network for Two-View Correspondence Learning](https://openaccess.thecvf.com/content/ICCV2021/papers/Zhong_T-Net_Effective_Permutation-Equivariant_Network_for_Two-View_Correspondence_Learning_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/x-gb/T-Net)
* [Learning To Stylize Novel Views](http://arxiv.org/abs/2105.13509)<br>:star:[code](https://github.com/hhsinping/stylescene):house:[project](https://hhsinping.github.io/3d_scene_stylization/)
* [A Lazy Approach to Long-Horizon Gradient-Based Meta-Learning](https://openaccess.thecvf.com/content/ICCV2021/papers/Zheng_Exploring_Temporal_Coherence_for_More_General_Video_Face_Forgery_Detection_ICCV_2021_paper.pdf)
* [Viewing Graph Solvability via Cycle Consistency](https://openaccess.thecvf.com/content/ICCV2021/papers/Arrigoni_Viewing_Graph_Solvability_via_Cycle_Consistency_ICCV_2021_paper.pdf)<br>:open_mouth:oral:star:[code](https://github.com/federica-arrigoni/solvability)<br>:trophy:Best paper honorable mention
* [SACoD: Sensor Algorithm Co-Design Towards Efficient CNN-Powered Intelligent PhlatCam](https://openaccess.thecvf.com/content/ICCV2021/papers/Fu_SACoD_Sensor_Algorithm_Co-Design_Towards_Efficient_CNN-Powered_Intelligent_PhlatCam_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/RICE-EIC/SACoD)
* [Rethinking 360Â° Image Visual Attention Modelling with Unsupervised Learning](https://openaccess.thecvf.com/content/ICCV2021/papers/Djilali_Rethinking_360deg_Image_Visual_Attention_Modelling_With_Unsupervised_Learning._ICCV_2021_paper.pdf)
* [Motion Basis Learning for Unsupervised Deep Homography Estimation with Subspace Projection](https://arxiv.org/abs/2103.15346)<br>:star:[code](https://github.com/megvii-research/BasesHomo)
* [Batch Normalization Increases Adversarial Vulnerability and Decreases Adversarial Transferability: A Non-Robust Feature Perspective](https://openaccess.thecvf.com/content/ICCV2021/papers/Benz_Batch_Normalization_Increases_Adversarial_Vulnerability_and_Decreases_Adversarial_Transferability_A_ICCV_2021_paper.pdf)
* [DeepCAD: A Deep Generative Network for Computer-Aided Design Models](https://arxiv.org/abs/2105.09492)<br>:house:[project](http://www.cs.columbia.edu/cg/deepcad/)
* [Better Aggregation in Test-Time Augmentation](https://openaccess.thecvf.com/content/ICCV2021/papers/Shanmugam_Better_Aggregation_in_Test-Time_Augmentation_ICCV_2021_paper.pdf)
* [Self-Born Wiring for Neural Trees](https://openaccess.thecvf.com/content/ICCV2021/papers/Chen_Self-Born_Wiring_for_Neural_Trees_ICCV_2021_paper.pdf)
* [Detector-Free Weakly Supervised Grounding by Separation](https://arxiv.org/abs/2104.09829)
* [Motion-Aware Dynamic Architecture for Efficient Frame Interpolation](https://openaccess.thecvf.com/content/ICCV2021/papers/Choi_Motion-Aware_Dynamic_Architecture_for_Efficient_Frame_Interpolation_ICCV_2021_paper.pdf)
* [Relating Adversarially Robust Generalization to Flat Minima](https://arxiv.org/abs/2104.04448)
* [Bit-Mixer: Mixed-Precision Networks With Runtime Bit-Width Selection](https://openaccess.thecvf.com/content/ICCV2021/papers/Bulat_Bit-Mixer_Mixed-Precision_Networks_With_Runtime_Bit-Width_Selection_ICCV_2021_paper.pdf)
* [AINet: Association Implantation for Superpixel Segmentation](https://arxiv.org/abs/2101.10696)<br>:star:[code](https://github.com/wangyxxjtu/AINet-ICCV2021)
* [Orthogonal Projection Loss](https://arxiv.org/abs/2103.14021)<br>:star:[code](https://github.com/kahnchana/opl)
* [Knowledge-Enriched Distributional Model Inversion Attacks](https://arxiv.org/abs/2010.04092)<br>:star:[code](https://github.com/SCccc21/Knowledge-Enriched-DMI)
* [Architecture Disentanglement for Deep Neural Networks](https://arxiv.org/abs/2003.13268)<br>:star:[code](https://github.com/hujiecpp/NAD)
* [On Equivariant and Invariant Learning of Object Landmark Representations](https://arxiv.org/abs/2006.14787)<br>:star:[code](https://github.com/cvl-umass/ContrastLandmark):house:[project](https://people.cs.umass.edu/~zezhoucheng/contrastive_landmark/)
* [Predicting with Confidence on Unseen Distributions](https://arxiv.org/abs/2107.03315)
* [Embed Me If You Can: A Geometric Perceptron](https://arxiv.org/abs/2006.06507)<br>:star:[code](https://github.com/pavlo-melnyk/mlgp-embedme)
* [Persistent Homology Based Graph Convolution Network for Fine-Grained 3D Shape Segmentation](https://openaccess.thecvf.com/content/ICCV2021/papers/Wong_Persistent_Homology_Based_Graph_Convolution_Network_for_Fine-Grained_3D_Shape_ICCV_2021_paper.pdf)
* [HIRE-SNN: Harnessing the Inherent Robustness of Energy-Efficient Deep Spiking Neural Networks by Training With Crafted Input Noise](https://openaccess.thecvf.com/content/ICCV2021/papers/Kundu_HIRE-SNN_Harnessing_the_Inherent_Robustness_of_Energy-Efficient_Deep_Spiking_Neural_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/ksouvik52/hiresnn2021)
* [Towards Memory-Efficient Neural Networks via Multi-Level In Situ Generation](https://arxiv.org/abs/2108.11430)
* [From Culture to Clothing: Discovering the World Events Behind a Century of Fashion Images](https://arxiv.org/abs/2102.01690)<br>:house:[project](http://vision.cs.utexas.edu/projects/CultureClothing/)
* [MBA-VO: Motion Blur Aware Visual Odometry](https://openaccess.thecvf.com/content/ICCV2021/papers/Liu_MBA-VO_Motion_Blur_Aware_Visual_Odometry_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/ethliup/MBA-VO)
* [STR-GQN: Scene Representation and Rendering for Unknown Cameras Based on Spatial Transformation Routing](https://openaccess.thecvf.com/content/ICCV2021/papers/Chen_STR-GQN_Scene_Representation_and_Rendering_for_Unknown_Cameras_Based_on_ICCV_2021_paper.pdf)
* [Explaining Local, Global, And Higher-Order Interactions In Deep Learning](https://arxiv.org/abs/2006.08601)
* [Beyond Trivial Counterfactual Explanations with Diverse Valuable Explanations](https://arxiv.org/abs/2103.10226)<br>:star:[code](https://github.com/ElementAI/beyond-trivial-explanations)
* [Homogeneous Architecture Augmentation for Neural Predictor](https://arxiv.org/abs/2107.13153)<br>:star:[code](https://github.com/lyq998/HAAP)
* [SS-IL: Separated Softmax for Incremental Learning](https://openaccess.thecvf.com/content/ICCV2021/papers/Ahn_SS-IL_Separated_Softmax_for_Incremental_Learning_ICCV_2021_paper.pdf)
* [VSAC: Efficient and Accurate Estimator for H and F](https://arxiv.org/abs/2106.10240)
* [Fusion Moves for Graph Matching](https://arxiv.org/abs/2101.12085)<br>:star:[code](https://github.com/vislearn/libmpopt):house:[project](https://vislearn.github.io/libmpopt/iccv2021/)
* [Geometric Granularity Aware Pixel-To-Mesh](https://openaccess.thecvf.com/content/ICCV2021/papers/Shi_Geometric_Granularity_Aware_Pixel-To-Mesh_ICCV_2021_paper.pdf)
* [Modulated Periodic Activations for Generalizable Local Functional Representations](https://arxiv.org/abs/2104.03960)<br>:house:[project](https://ishit.github.io/modsine/)
* [Interpretation of Emergent Communication in Heterogeneous Collaborative Embodied Agents](https://openaccess.thecvf.com/content/ICCV2021/papers/Patel_Interpretation_of_Emergent_Communication_in_Heterogeneous_Collaborative_Embodied_Agents_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/saimwani/CoMON):house:[project](https://shivanshpatel35.github.io/comon/):tv:[video](https://youtu.be/kLv2rxO9t0g)
* [A Dark Flash Normal Camera](https://arxiv.org/abs/2012.06125)<br>:house:[project](https://darkflashnormalpaper.github.io/):tv:[video](https://www.youtube.com/watch?v=RboGUBqYQec)
* [Pri3D: Can 3D Priors Help 2D Representation Learning?](https://arxiv.org/abs/2104.11225)<br>:star:[code](https://github.com/Sekunde/Pri3D):tv:[video](https://www.youtube.com/watch?v=S2VodtyfQbQ)
* [Membership Inference Attacks Are Easier on Difficult Problems](https://arxiv.org/abs/2102.07762)
* [Auxiliary Tasks and Exploration Enable ObjectGoal Navigation](https://openaccess.thecvf.com/content/ICCV2021/papers/Ye_Auxiliary_Tasks_and_Exploration_Enable_ObjectGoal_Navigation_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/joel99/objectnav):house:[project](https://joel99.github.io/objectnav/)
* [MixMo: Mixing Multiple Inputs for Multiple Outputs via Deep Subnetworks](https://arxiv.org/abs/2103.06132)
* [Act the Part: Learning Interaction Strategies for Articulated Object Part Discovery](https://arxiv.org/abs/2105.01047)<br>:house:[project](https://atp.cs.columbia.edu/)
* [DCT-SNN: Using DCT To Distribute Spatial Information Over Time for Low-Latency Spiking Neural Networks](https://openaccess.thecvf.com/content/ICCV2021/papers/Garg_DCT-SNN_Using_DCT_To_Distribute_Spatial_Information_Over_Time_for_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/SayeedChowdhury/dct-snn)
* [Learning To Resize Images for Computer Vision Tasks](https://arxiv.org/abs/2103.09950)
* [Field of Junctions: Extracting Boundary Structure at Low SNR](https://arxiv.org/abs/2011.13866)
* [DeepGaze IIE: Calibrated prediction in and out-of-domain for state-of-the-art saliency modeling](https://arxiv.org/abs/2105.12441)
* [Learning To Reduce Defocus Blur by Realistically Modeling Dual-Pixel Data](https://arxiv.org/abs/2012.03255)<br>:star:[code](https://github.com/Abdullah-Abuolaim/recurrent-defocus-deblurring-synth-dual-pixel)
* [Graph-based Asynchronous Event Processing for Rapid Object Recognitio](https://openaccess.thecvf.com/content/ICCV2021/papers/Li_Graph-Based_Asynchronous_Event_Processing_for_Rapid_Object_Recognition_ICCV_2021_paper.pdf)
* [Ranking Models in Unlabeled New Environments](https://openaccess.thecvf.com/content/ICCV2021/papers/Sun_Ranking_Models_in_Unlabeled_New_Environments_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/sxzrt/Proxy-Set)
* [A Hybrid Frequency-Spatial Domain Model for Sparse Image Reconstruction in Scanning Transmission Electron Microscopy](https://openaccess.thecvf.com/content/ICCV2021/papers/He_A_Hybrid_Frequency-Spatial_Domain_Model_for_Sparse_Image_Reconstruction_in_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/icthrm/Sparse-Sampling-Reconstruction)
* [MixMix: All You Need for Data-Free Compression Are Feature and Data Mixing](https://arxiv.org/abs/2011.09899)
* [Efficient Large Scale Inlier Voting for Geometric Vision Problems](https://arxiv.org/abs/2107.11810)<br>:star:[code](https://github.com/google-research/google-research/tree/master/large_scale_voting)
* [Aggregation With Feature Detection](https://openaccess.thecvf.com/content/ICCV2021/papers/Sun_Aggregation_With_Feature_Detection_ICCV_2021_paper.pdf)
* [ReCU: Reviving the Dead Weights in Binary Neural Networks](http://arxiv.org/abs/2103.12369)<br>:star:[code](https://github.com/z-hXu/ReCU)
* [Deep Halftoning With Reversible Binary Pattern](https://openaccess.thecvf.com/content/ICCV2021/papers/Xia_Deep_Halftoning_With_Reversible_Binary_Pattern_ICCV_2021_paper.pdf)
* [FFT-OT: A Fast Algorithm for Optimal Transportation](https://openaccess.thecvf.com/content/ICCV2021/papers/Lei_FFT-OT_A_Fast_Algorithm_for_Optimal_Transportation_ICCV_2021_paper.pdf)
* [Progressive Correspondence Pruning by Consensus Learning](https://arxiv.org/abs/2101.00591)<br>:star:[code](https://github.com/sailor-z/CLNet):house:[project](https://sailor-z.github.io/projects/CLNet)<br>:newspaper:è§£è¯»:[åŸºäºä¸€è‡´æ€§å­¦ä¹ çš„æ¸è¿›å¼åŒ¹é…ç­›é€‰ (ICCV 2021)](https://zhuanlan.zhihu.com/p/394483122)
* [Multispectral Illumination Estimation Using Deep Unrolling Network](https://openaccess.thecvf.com/content/ICCV2021/papers/Li_Multispectral_Illumination_Estimation_Using_Deep_Unrolling_Network_ICCV_2021_paper.pdf)
* [Distilling Global and Local Logits With Densely Connected Relations](https://openaccess.thecvf.com/content/ICCV2021/papers/Kim_Distilling_Global_and_Local_Logits_With_Densely_Connected_Relations_ICCV_2021_paper.pdf)
* [Learning specialized activation functions with the Piecewise Linear Unit](https://arxiv.org/abs/2104.03693)
* [Adaptive Convolutions With Per-Pixel Dynamic Filter Atom](https://arxiv.org/abs/2108.07895)
* [Deep Matching Prior: Test-Time Optimization for Dense Correspondence](https://arxiv.org/abs/2106.03090)<br>:star:[code](https://github.com/SunghwanHong/Deep-Matching-Prior)
* [Calibrated and Partially Calibrated Semi-Generalized Homographies](https://arxiv.org/abs/2103.06535)<br>:star:[code](https://github.com/snehalbhayani/SemiGeneralizedHomography)
* [The Spatio-Temporal Poisson Point Process: A Simple Model for the Alignment of Event Camera Data](https://openaccess.thecvf.com/content/ICCV2021/papers/Gu_The_Spatio-Temporal_Poisson_Point_Process_A_Simple_Model_for_the_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/pbideau/Event-ST-PPP)
* [EC-DARTS: Inducing Equalized and Consistent Optimization Into DARTS](https://openaccess.thecvf.com/content/ICCV2021/papers/Zhou_EC-DARTS_Inducing_Equalized_and_Consistent_Optimization_Into_DARTS_ICCV_2021_paper.pdf)
* [Refining activation downsampling with SoftPool](https://arxiv.org/abs/2101.00440)
* [FATNN: Fast and Accurate Ternary Neural Networks](https://arxiv.org/abs/2008.05101)<br>:star:[code](https://github.com/MonashAI/QTool)
* [GTT-Net: Learned Generalized Trajectory Triangulation](https://openaccess.thecvf.com/content/ICCV2021/papers/Xu_GTT-Net_Learned_Generalized_Trajectory_Triangulation_ICCV_2021_paper.pdf) 
* [Deep Permutation Equivariant Structure from Motion](https://arxiv.org/abs/2104.06703)<br>:star:[code](https://github.com/drormoran/Equivariant-SFM)
* [Extending Neural P-frame Codecs for B-frame Codin](https://openaccess.thecvf.com/content/ICCV2021/papers/Pourreza_Extending_Neural_P-Frame_Codecs_for_B-Frame_Coding_ICCV_2021_paper.pdf)
* [Hierarchical Graph Attention Network for Few-Shot Visual-Semantic Learning](https://openaccess.thecvf.com/content/ICCV2021/papers/Yin_Hierarchical_Graph_Attention_Network_for_Few-Shot_Visual-Semantic_Learning_ICCV_2021_paper.pdf)
* [SA-ConvONet: Sign-Agnostic Optimization of Convolutional Occupancy Networks](https://openaccess.thecvf.com/content/ICCV2021/papers/Tang_SA-ConvONet_Sign-Agnostic_Optimization_of_Convolutional_Occupancy_Networks_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/tangjiapeng/SA-ConvONet)
* [AA-RMVSNet: Adaptive Aggregation Recurrent Multi-View Stereo Network](https://openaccess.thecvf.com/content/ICCV2021/papers/Wei_AA-RMVSNet_Adaptive_Aggregation_Recurrent_Multi-View_Stereo_Network_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/QT-Zhu/AA-RMVSNet)
* [Rethinking the Backdoor Attacks' Triggers: A Frequency Perspective](https://arxiv.org/abs/2104.03413)<br>:star:[code](https://github.com/YiZeng623/frequency-backdoor)
* [Orthographic-Perspective Epipolar Geometry](https://openaccess.thecvf.com/content/ICCV2021/papers/Larsson_Orthographic-Perspective_Epipolar_Geometry_ICCV_2021_paper.pdf)
* [Why Approximate Matrix Square Root Outperforms Accurate SVD in Global Covariance Pooling?](https://arxiv.org/abs/2105.02498)<br>:star:[code](https://github.com/KingJamesSong/DifferentiableSVD)
* [PixelPyramids: Exact Inference Models From Lossless Image Pyramids](https://openaccess.thecvf.com/content/ICCV2021/papers/Mahajan_PixelPyramids_Exact_Inference_Models_From_Lossless_Image_Pyramids_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/visinf/pixelpyramids)
* [SurfaceNet: Adversarial SVBRDF Estimation from a Single Image](https://arxiv.org/abs/2107.11298)<br>:star:[code](https://github.com/perceivelab/surfacenet)
* [Adaptive Curriculum Learning](https://openaccess.thecvf.com/content/ICCV2021/papers/Kong_Adaptive_Curriculum_Learning_ICCV_2021_paper.pdf)
* [Sparse-Shot Learning With Exclusive Cross-Entropy for Extremely Many Localisations](https://arxiv.org/abs/2104.10425)
* [Graspness Discovery in Clutters for Fast and Accurate Grasp Detection](https://openaccess.thecvf.com/content/ICCV2021/papers/Wang_Graspness_Discovery_in_Clutters_for_Fast_and_Accurate_Grasp_Detection_ICCV_2021_paper.pdf)
* [RobustNav: Towards Benchmarking Robustness in Embodied Navigation](https://arxiv.org/abs/2106.04531)<br>:star:[code](https://github.com/allenai/robustnav)
* [Generating Attribution Maps With Disentangled Masked Backpropagation](https://openaccess.thecvf.com/content/ICCV2021/papers/Ruiz_Generating_Attribution_Maps_With_Disentangled_Masked_Backpropagation_ICCV_2021_paper.pdf)
* [Spectral Leakage and Rethinking the Kernel Size in CNNs](https://arxiv.org/abs/2101.10143)<br>:star:[code](https://github.com/ntomen/Windowed-Convolutions-for-CNNs)
* [What You Can Learn by Staring at a Blank Wall](https://arxiv.org/abs/2108.13027)
* [Neural TMDlayer: Modeling Instantaneous Flow of Features via SDE Generators](https://arxiv.org/abs/2108.08891)
* [CLEAR: Clean-up Sample-Targeted Backdoor in Neural Networks](https://openaccess.thecvf.com/content/ICCV2021/papers/Zhu_CLEAR_Clean-Up_Sample-Targeted_Backdoor_in_Neural_Networks_ICCV_2021_paper.pdf)
* [Learning To Hallucinate Examples From Extrinsic and Intrinsic Supervision](https://openaccess.thecvf.com/content/ICCV2021/papers/Gui_Learning_To_Hallucinate_Examples_From_Extrinsic_and_Intrinsic_Supervision_ICCV_2021_paper.pdf)
* [Single-shot Hyperspectral-Depth Imaging with Learned Diffractive Optics](https://arxiv.org/abs/2009.00463)
* [GridToPix: Training Embodied Agents With Minimal Supervision](https://arxiv.org/abs/2105.00931)<br>:house:[project](https://unnat.github.io/gridtopix/):tv:[video](https://youtu.be/CT6n32pa1EI)
* [Differentiable Dynamic Wirings for Neural Networks](https://openaccess.thecvf.com/content/ICCV2021/papers/Yuan_Differentiable_Dynamic_Wirings_for_Neural_Networks_ICCV_2021_paper.pdf)
* [JEM++: Improved Techniques for Training JEM](https://openaccess.thecvf.com/content/ICCV2021/papers/Yang_JEM_Improved_Techniques_for_Training_JEM_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/sndnyang/JEMPP)
* [X-World: Accessibility, Vision, and Autonomy Meet](https://openaccess.thecvf.com/content/ICCV2021/papers/Zhang_X-World_Accessibility_Vision_and_Autonomy_Meet_ICCV_2021_paper.pdf)
* [Memory-augmented Dynamic Neural Relational Inference](https://openaccess.thecvf.com/content/ICCV2021/papers/Gong_Memory-Augmented_Dynamic_Neural_Relational_Inference_ICCV_2021_paper.pdf)
* [Physics-based Differentiable Depth Sensor Simulation](https://arxiv.org/abs/2103.16563)
* [Hypergraph Neural Networks for Hypergraph Matching](https://openaccess.thecvf.com/content/ICCV2021/papers/Liao_Hypergraph_Neural_Networks_for_Hypergraph_Matching_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/xwliao/HNN-HM)
* Visual Grounding
  * [SAT: 2D Semantics Assisted Training for 3D Visual Grounding](https://arxiv.org/abs/2105.11450)
* [Cortical Surface Shape Analysis Based on Alexandrov Polyhedra](https://openaccess.thecvf.com/content/ICCV2021/papers/Zhang_Cortical_Surface_Shape_Analysis_Based_on_Alexandrov_Polyhedra_ICCV_2021_paper.pdf)
* [FcaNet: Frequency Channel Attention Networks](https://arxiv.org/abs/2012.11879)<br>:star:[code](https://github.com/cfzd/FcaNet)
* [Procedure Planning in Instructional Videos via Contextual Modeling and Model-Based Policy Learning](https://openaccess.thecvf.com/content/ICCV2021/papers/Bi_Procedure_Planning_in_Instructional_Videos_via_Contextual_Modeling_and_Model-Based_ICCV_2021_paper.pdf)
* [Structured Outdoor Architecture Reconstruction by Exploration and Classification](https://arxiv.org/abs/2108.07990)<br>:star:[code](https://github.com/zhangfuyang/search_evaluate)
* [ELLIPSDF: Joint Object Pose and Shape Optimization with a Bi-level Ellipsoid and Signed Distance Function Description](https://arxiv.org/abs/2108.00355)
* [Testing Using Privileged Information by Adapting Features With Statistical Dependence](https://openaccess.thecvf.com/content/ICCV2021/papers/Kim_Testing_Using_Privileged_Information_by_Adapting_Features_With_Statistical_Dependence_ICCV_2021_paper.pdf)
* [Virtual Light Transport Matrices for Non-Line-of-Sight Imaging](https://arxiv.org/abs/2103.12622)<br>:open_mouth:oral
* [DecentLaM: Decentralized Momentum SGD for Large-batch Deep Training](https://arxiv.org/abs/2104.11981)
* [Contrastive Multimodal Fusion with TupleInfoNCE](https://arxiv.org/abs/2107.02575)
* [Learning Better Visual Data Similarities via New Grouplet Non-Euclidean Embedding](https://openaccess.thecvf.com/content/ICCV2021/papers/Zhang_Learning_Better_Visual_Data_Similarities_via_New_Grouplet_Non-Euclidean_Embedding_ICCV_2021_paper.pdf)
* [An Elastica Geodesic Approach With Convexity Shape Prior](https://openaccess.thecvf.com/content/ICCV2021/papers/Chen_An_Elastica_Geodesic_Approach_With_Convexity_Shape_Prior_ICCV_2021_paper.pdf)
* [Inverting a Rolling Shutter Camera: Bring Rolling Shutter Images to High Framerate Global Shutter Video](https://openaccess.thecvf.com/content/ICCV2021/papers/Fan_Inverting_a_Rolling_Shutter_Camera_Bring_Rolling_Shutter_Images_to_ICCV_2021_paper.pdf)
* [Multimodal Knowledge Expansion](https://arxiv.org/abs/2103.14431)<br>:star:[code](https://github.com/zihuixue/MKE)
* [Direct Differentiable Augmentation Search](https://arxiv.org/abs/2104.04282)<br>:star:[code](https://github.com/zxcvfd13502/DDAS_code)
* [The Functional Correspondence Problem](https://arxiv.org/abs/2109.01097)<br>:house:[project](https://agi-labs.github.io/FuncCorr/)
* [Joint Topology-Preserving and Feature-Refinement Network for Curvilinear Structure Segmentation](https://openaccess.thecvf.com/content/ICCV2021/papers/Cheng_Joint_Topology-Preserving_and_Feature-Refinement_Network_for_Curvilinear_Structure_Segmentation_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/zkl20061823)
* [Generative Layout Modeling Using Constraint Graphs](https://arxiv.org/abs/2011.13417)
* [Self-Supervised Image Prior Learning with GMM from a Single Noisy Image](https://openaccess.thecvf.com/content/ICCV2021/papers/Liu_Self-Supervised_Image_Prior_Learning_With_GMM_From_a_Single_Noisy_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/HUST-Tan/SS-GMM)
* [Deep Implicit Surface Point Prediction Networks](https://arxiv.org/abs/2106.05779)<br>:star:[code](https://github.com/rahulvenkk/csp-net):house:[project](https://sites.google.com/view/cspnet):tv:[video](https://drive.google.com/file/d/1fferQxWo3ug14NMiV3UqwDGmFaWc5tjh/view)
* [Poly-NL: Linear Complexity Non-local Layers With 3rd Order Polynomials](https://openaccess.thecvf.com/content/ICCV2021/papers/Babiloni_Poly-NL_Linear_Complexity_Non-Local_Layers_With_3rd_Order_Polynomials_ICCV_2021_paper.pdf)
* [Factorizing Perception and Policy for Interactive Instruction Following](https://arxiv.org/abs/2012.03208)<br>:star:[code](https://github.com/gistvision/moca)
* [Group-Wise Inhibition Based Feature Regularization for Robust Classification](https://arxiv.org/abs/2103.02152)<br>:star:[code](https://github.com/LinusWu/TENET_Training)
* [Searching for Robustness: Loss Learning for Noisy Classification Tasks](https://arxiv.org/abs/2103.00243)
* [Statistically Consistent Saliency Estimation](https://arxiv.org/abs/2109.01068)
* [Practical Relative Order Attack in Deep Ranking](https://arxiv.org/abs/2103.05248)<br>:star:[code](https://github.com/cdluminate/advorder)
* [Q-Match: Iterative Shape Matching via Quantum Annealing](https://openaccess.thecvf.com/content/ICCV2021/papers/Benkner_Q-Match_Iterative_Shape_Matching_via_Quantum_Annealing_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/MSeelbach/Q-Match):house:[project](https://4dqv.mpi-inf.mpg.de/QMATCH/)
* [Learning To Better Segment Objects From Unseen Classes With Unlabeled Videos](https://arxiv.org/abs/2104.12276)<br>:house:[project](https://dulucas.github.io/Homepage/gbopt/):tv:[video](https://youtu.be/7q3jRZ_2PE4)
* [Globally Optimal and Efficient Manhattan Frame Estimation by Delimiting Rotation Search Space](https://openaccess.thecvf.com/content/ICCV2021/papers/Ge_Globally_Optimal_and_Efficient_Manhattan_Frame_Estimation_by_Delimiting_Rotation_ICCV_2021_paper.pdf)
* [Cross-Encoder for Unsupervised Gaze Representation Learning](https://openaccess.thecvf.com/content/ICCV2021/papers/Sun_Cross-Encoder_for_Unsupervised_Gaze_Representation_Learning_ICCV_2021_paper.pdf)
* [Hierarchical Disentangled Representation Learning for Outdoor Illumination Estimation and Editing](https://openaccess.thecvf.com/content/ICCV2021/papers/Yu_Hierarchical_Disentangled_Representation_Learning_for_Outdoor_Illumination_Estimation_and_Editing_ICCV_2021_paper.pdf)
* [NeuSpike-Net: High Speed Video Reconstruction via Bio-Inspired Neuromorphic Cameras](https://openaccess.thecvf.com/content/ICCV2021/papers/Zhu_NeuSpike-Net_High_Speed_Video_Reconstruction_via_Bio-Inspired_Neuromorphic_Cameras_ICCV_2021_paper.pdf)
* [Local Temperature Scaling for Probability Calibration](https://arxiv.org/abs/2008.05105)
* [LIRA: Learnable, Imperceptible and Robust Backdoor Attacks](https://openaccess.thecvf.com/content/ICCV2021/papers/Doan_LIRA_Learnable_Imperceptible_and_Robust_Backdoor_Attacks_ICCV_2021_paper.pdf)
* [Conformer: Local Features Coupling Global Representations for Visual Recognition](https://arxiv.org/abs/2105.03889)<br>:star:[code](https://github.com/pengzhiliang/Conformer)
* [Reliably fast adversarial training via latent adversarial perturbation](https://arxiv.org/abs/2104.01575)
* [PX-NET: Simple and Efficient Pixel-Wise Training of Photometric Stereo Networks](https://openaccess.thecvf.com/content/ICCV2021/papers/Logothetis_PX-NET_Simple_and_Efficient_Pixel-Wise_Training_of_Photometric_Stereo_Networks_ICCV_2021_paper.pdf)
* [A-SDF: Learning Disentangled Signed Distance Functions for Articulated Shape Representation](https://openaccess.thecvf.com/content/ICCV2021/papers/Mu_A-SDF_Learning_Disentangled_Signed_Distance_Functions_for_Articulated_Shape_Representation_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/JitengMu/A-SDF):house:[project](https://jitengmu.github.io/A-SDF/):tv:[video](https://youtu.be/P5WTcaXzC7A)
* [ICON: Learning Regular Maps Through Inverse Consistency](https://arxiv.org/abs/2105.04459)
* [Video Geo-Localization Employing Geo-Temporal Feature Learning and GPS Trajectory Smoothing](https://openaccess.thecvf.com/content/ICCV2021/papers/Regmi_Video_Geo-Localization_Employing_Geo-Temporal_Feature_Learning_and_GPS_Trajectory_Smoothing_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/kregmi/VTE)
* [Kernel Methods in Hyperbolic Spaces](https://openaccess.thecvf.com/content/ICCV2021/papers/Fang_Kernel_Methods_in_Hyperbolic_Spaces_ICCV_2021_paper.pdf)
* [Cross-Camera Convolutional Color Constancy](https://arxiv.org/abs/2011.11890)
* [BlockPlanner: City Block Generation with Vectorized Graph Representation](https://openaccess.thecvf.com/content/ICCV2021/papers/Xu_BlockPlanner_City_Block_Generation_With_Vectorized_Graph_Representation_ICCV_2021_paper.pdf)
* [A Machine Teaching Framework for Scalable Recognition](https://openaccess.thecvf.com/content/ICCV2021/papers/Wang_A_Machine_Teaching_Framework_for_Scalable_Recognition_ICCV_2021_paper.pdf)
* Clothed Human Bodies
  * [Dynamic Surface Function Networks for Clothed Human Bodies](https://arxiv.org/abs/2104.03978)<br>:star:[code](https://github.com/andreiburov/DSFN):house:[project](https://andreiburov.github.io/DSFN/):tv:[video](https://www.youtube.com/watch?v=4wbSi9Sqdm4)
* è¿ç§»å­¦ä¹ 
  * [Fast and Efficient DNN Deployment via Deep Gaussian Transfer Learning](https://openaccess.thecvf.com/content/ICCV2021/papers/Sun_Fast_and_Efficient_DNN_Deployment_via_Deep_Gaussian_Transfer_Learning_ICCV_2021_paper.pdf)
* Active Recognition(AR)
  * [FLAR: A Unified Prototype Framework for Few-Sample Lifelong Active Recognition](https://openaccess.thecvf.com/content/ICCV2021/papers/Fan_FLAR_A_Unified_Prototype_Framework_for_Few-Sample_Lifelong_Active_Recognition_ICCV_2021_paper.pdf)
* 3Dæ‘„å½±
  * [SLIDE: Single Image 3D Photography with Soft Layering and Depth-aware Inpainting](https://arxiv.org/abs/2109.01068)<br>:open_mouth:oral:house:[project](https://varunjampani.github.io/slide/):tv:[video](https://www.youtube.com/watch?v=RQio7q-ueY8)
* [Sub-Bit Neural Networks: Learning To Compress and Accelerate Binary Neural Networks](https://openaccess.thecvf.com/content/ICCV2021/papers/Wang_Sub-Bit_Neural_Networks_Learning_To_Compress_and_Accelerate_Binary_Neural_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/yikaiw/SNN)
* [When Pigs Fly: Contextual Reasoning in Synthetic and Natural Scenes](https://arxiv.org/abs/2104.02215)<br>:star:[code](https://github.com/kreimanlab/WhenPigsFlyContext)
* [Physics-Enhanced Machine Learning for Virtual Fluorescence Microscopy](https://arxiv.org/abs/2004.04306)<br>:star:[code](https://github.com/clvcooke/virtual-fluorescence)
* [Ground-truth or DAER: Selective Re-query of Secondary Information](https://arxiv.org/abs/2009.07414)<br>:star:[code](https://github.com/lemmersj/ground-truth-or-daer)
* [Can Shape Structure Features Improve Model Robustness Under Diverse Adversarial Settings?](https://openaccess.thecvf.com/content/ICCV2021/papers/Sun_Can_Shape_Structure_Features_Improve_Model_Robustness_Under_Diverse_Adversarial_ICCV_2021_paper.pdf)
* [Joint Representation Learning and Novel Category Discovery on Single- and Multi-Modal Data](https://openaccess.thecvf.com/content/ICCV2021/papers/Jia_Joint_Representation_Learning_and_Novel_Category_Discovery_on_Single-_and_ICCV_2021_paper.pdf)
* [Sparse Needlets for Lighting Estimation with Spherical Transport Loss](https://arxiv.org/abs/2106.13090)
* [Semantic Perturbations with Normalizing Flows for Improved Generalization](https://openaccess.thecvf.com/content/ICCV2021/papers/Yuksel_Semantic_Perturbations_With_Normalizing_Flows_for_Improved_Generalization_ICCV_2021_paper.pdf)
* [Differentiable Surface Rendering via Non-Differentiable Sampling](https://arxiv.org/abs/2108.04886)
* [Towards Robustness of Deep Neural Networks via Regularization](https://openaccess.thecvf.com/content/ICCV2021/papers/Li_Towards_Robustness_of_Deep_Neural_Networks_via_Regularization_ICCV_2021_paper.pdf)
* [Objects as Cameras: Estimating High-Frequency Illumination from Shadows](https://openaccess.thecvf.com/content/ICCV2021/papers/Swedish_Objects_As_Cameras_Estimating_High-Frequency_Illumination_From_Shadows_ICCV_2021_paper.pdf)
* [Inference of Black Hole Fluid-Dynamics From Sparse Interferometric Measurements](https://openaccess.thecvf.com/content/ICCV2021/papers/Levis_Inference_of_Black_Hole_Fluid-Dynamics_From_Sparse_Interferometric_Measurements_ICCV_2021_paper.pdf)
* [Removing the Bias of Integral Pose Regression](https://openaccess.thecvf.com/content/ICCV2021/papers/Gu_Removing_the_Bias_of_Integral_Pose_Regression_ICCV_2021_paper.pdf)
* [A Light Stage on Every Desk](https://openaccess.thecvf.com/content/ICCV2021/papers/Sengupta_A_Light_Stage_on_Every_Desk_ICCV_2021_paper.pdf)<br>:house:[project](grail.cs.washington.edu/projects/Light_Stage_on_Every_Desk/)
* [Multi-Level Curriculum for Training a Distortion-Aware Barrel Distortion Rectification Model](https://openaccess.thecvf.com/content/ICCV2021/papers/Liao_Multi-Level_Curriculum_for_Training_a_Distortion-Aware_Barrel_Distortion_Rectification_Model_ICCV_2021_paper.pdf)
* [Generic Event Boundary Detection: A Benchmark for Event Segmentation](https://arxiv.org/abs/2101.10511)
* [Extreme Structure from Motion for Indoor Panoramas without Visual Overlaps](https://openaccess.thecvf.com/content/ICCV2021/papers/Shabani_Extreme_Structure_From_Motion_for_Indoor_Panoramas_Without_Visual_Overlaps_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/aminshabani/extreme-indoor-sfm)
* [Continual Prototype Evolution: Learning Online from Non-Stationary Data Streams](https://arxiv.org/abs/2009.00919)<br>:star:[code](https://github.com/mattdl/ContinualPrototypeEvolution)
* [VaPiD: A Rapid Vanishing Point Detector via Learned Optimizers](https://openaccess.thecvf.com/content/ICCV2021/papers/Liu_VaPiD_A_Rapid_Vanishing_Point_Detector_via_Learned_Optimizers_ICCV_2021_paper.pdf)
* [Multimodal Co-Attention Transformer for Survival Prediction in Gigapixel Whole Slide Images](https://openaccess.thecvf.com/content/ICCV2021/papers/Chen_Multimodal_Co-Attention_Transformer_for_Survival_Prediction_in_Gigapixel_Whole_Slide_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/mahmoodlab/MCAT)
* [Efficient and Differentiable Shadow Computation for Inverse Problems](https://arxiv.org/abs/2104.00359)
* [Minimal Cases for Computing the Generalized Relative Pose using Affine Correspondences](https://arxiv.org/abs/2007.10700)
* [Radial Distortion Invariant Factorization for Structure from Motion](https://openaccess.thecvf.com/content/ICCV2021/papers/Iglesias_Radial_Distortion_Invariant_Factorization_for_Structure_From_Motion_ICCV_2021_paper.pdf)
* [LaLaLoc: Latent Layout Localisation in Dynamic, Unvisited Environments](https://openaccess.thecvf.com/content/ICCV2021/papers/Howard-Jenkins_LaLaLoc_Latent_Layout_Localisation_in_Dynamic_Unvisited_Environments_ICCV_2021_paper.pdf)
* [Transforms Based Tensor Robust PCA: Corrupted Low-Rank Tensors Recovery via Convex Optimization](https://openaccess.thecvf.com/content/ICCV2021/papers/Lu_Transforms_Based_Tensor_Robust_PCA_Corrupted_Low-Rank_Tensors_Recovery_via_ICCV_2021_paper.pdf)
* [Synchronization of Group-labelled Multi-graphs](https://openaccess.thecvf.com/content/ICCV2021/papers/Dal_Cin_Synchronization_of_Group-Labelled_Multi-Graphs_ICCV_2021_paper.pdf)
* [Robust Watermarking for Deep Neural Networks via Bi-Level Optimization](https://openaccess.thecvf.com/content/ICCV2021/papers/Yang_Robust_Watermarking_for_Deep_Neural_Networks_via_Bi-Level_Optimization_ICCV_2021_paper.pdf)
* [CrossNorm and SelfNorm for Generalization under Distribution Shifts](https://arxiv.org/abs/2102.02811)<br>:star:[code](https://github.com/amazon-research/crossnorm-selfnorm)
* [Learning Temporal Dynamics from Cycles in Narrated Video](https://arxiv.org/abs/2101.02337)<br>:house:[project](https://dave.ml/mmcc/)
* [von Mises-Fisher Loss: An Exploration of Embedding Geometries for Supervised Learning](https://arxiv.org/abs/2103.15718)
* [Multiple Heads are Better than One: Few-shot Font Generation with Multiple Localized Experts](https://arxiv.org/abs/2104.00887)<br>:star:[code](https://github.com/clovaai/mxfont)
* [Me-Momentum: Extracting Hard Confident Examples From Noisily Labeled Data](https://openaccess.thecvf.com/content/ICCV2021/papers/Bai_Me-Momentum_Extracting_Hard_Confident_Examples_From_Noisily_Labeled_Data_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/tmllab/Me-Momentum)
* [ProFlip: Targeted Trojan Attack with Progressive Bit Flips](https://openaccess.thecvf.com/content/ICCV2021/papers/Chen_ProFlip_Targeted_Trojan_Attack_With_Progressive_Bit_Flips_ICCV_2021_paper.pdf)
* [Attention Is Not Enough: Mitigating the Distribution Discrepancy in Asynchronous Multimodal Sequence Fusion](https://openaccess.thecvf.com/content/ICCV2021/papers/Liang_Attention_Is_Not_Enough_Mitigating_the_Distribution_Discrepancy_in_Asynchronous_ICCV_2021_paper.pdf)
* [AdvRush: Searching for Adversarially Robust Neural Architectures](https://arxiv.org/abs/2108.01289)
* [Improving robustness against common corruptions with frequency biased models](https://arxiv.org/abs/2103.16241)
* [UASNet: Uncertainty Adaptive Sampling Network for Deep Stereo Matching](https://openaccess.thecvf.com/content/ICCV2021/papers/Mao_UASNet_Uncertainty_Adaptive_Sampling_Network_for_Deep_Stereo_Matching_ICCV_2021_paper.pdf)
* [Glimpse-Attend-and-Explore: Self-Attention for Active Visual Exploration](https://openaccess.thecvf.com/content/ICCV2021/papers/Seifi_Glimpse-Attend-and-Explore_Self-Attention_for_Active_Visual_Exploration_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/soroushseifi/glimpse-attend-explore)
* [Field Convolutions for Surface CNNs](https://arxiv.org/abs/2104.03916)<br>:open_mouth:oral:star:[code](https://github.com/twmitchel/FieldConv)
* [SIMstack: A Generative Shape and Instance Model for Unordered Object Stacks](https://arxiv.org/abs/2103.16442)
* [Learning Icosahedral Spherical Probability Map Based on Bingham Mixture Model for Vanishing Point Estimation](https://openaccess.thecvf.com/content/ICCV2021/papers/Li_Learning_Icosahedral_Spherical_Probability_Map_Based_on_Bingham_Mixture_Model_ICCV_2021_paper.pdf)
* [Incorporating Learnable Membrane Time Constant to Enhance Learning of Spiking Neural Networks](https://arxiv.org/abs/2007.05785)<br>:star:[code](https://github.com/fangwei123456/Parametric-Leaky-Integrate-and-Fire-Spiking-Neuron)
* [Real-Time Vanishing Point Detector Integrating Under-Parameterized RANSAC and Hough Transform](https://openaccess.thecvf.com/content/ICCV2021/papers/Wu_Real-Time_Vanishing_Point_Detector_Integrating_Under-Parameterized_RANSAC_and_Hough_Transform_ICCV_2021_paper.pdf)
* [Low-Rank Tensor Completion by Approximating the Tensor Average Rank](https://openaccess.thecvf.com/content/ICCV2021/papers/Wang_Low-Rank_Tensor_Completion_by_Approximating_the_Tensor_Average_Rank_ICCV_2021_paper.pdf)
* [Rotation Averaging in a Split Second: A Primal-Dual Method and a Closed-Form for Cycle Graphs](https://arxiv.org/abs/2109.08046)<br>:star:[code](https://github.com/gabmoreira/maks)
* [Effectively Leveraging Attributes for Visual Similarity](https://arxiv.org/abs/2105.01695)<br>:star:[code](https://github.com/samarth4149/PAN)
* [Localized Simple Multiple Kernel K-means](https://openaccess.thecvf.com/content/ICCV2021/papers/Liu_Localized_Simple_Multiple_Kernel_K-Means_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/xinwangliu/LocalizedSMKKM)
* [SmartShadow: Artistic Shadow Drawing Tool for Line Drawings](https://openaccess.thecvf.com/content/ICCV2021/papers/Zhang_SmartShadow_Artistic_Shadow_Drawing_Tool_for_Line_Drawings_ICCV_2021_paper.pdf)
* [PT-CapsNet: A Novel Prediction-Tuning Capsule Network Suitable for Deeper Architectures](https://openaccess.thecvf.com/content/ICCV2021/papers/Pan_PT-CapsNet_A_Novel_Prediction-Tuning_Capsule_Network_Suitable_for_Deeper_Architectures_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/Christinepan881/PT-CapsNet)
* [Generalized Shuffled Linear Regression](https://openaccess.thecvf.com/content/ICCV2021/papers/Li_Generalized_Shuffled_Linear_Regression_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/SILI1994/Generalized-Shuffled-Linear-Regression)
* [The Animation Transformer: Visual Correspondence via Segment Matching](https://openaccess.thecvf.com/content/ICCV2021/papers/Casey_The_Animation_Transformer_Visual_Correspondence_via_Segment_Matching_ICCV_2021_paper.pdf)
* [Weak Adaptation Learning: Addressing Cross-Domain Data Insufficiency With Weak Annotator](https://openaccess.thecvf.com/content/ICCV2021/papers/Xu_Weak_Adaptation_Learning_Addressing_Cross-Domain_Data_Insufficiency_With_Weak_Annotator_ICCV_2021_paper.pdf)
* [Building-GAN: Graph-Conditioned Architectural Volumetric Design Generation](https://openaccess.thecvf.com/content/ICCV2021/papers/Chang_Building-GAN_Graph-Conditioned_Architectural_Volumetric_Design_Generation_ICCV_2021_paper.pdf)
* [Procrustean Training for Imbalanced Deep Learning](https://arxiv.org/abs/2104.01769)



