# ICCV2021最新信息及已接收论文/代码(持续更新)

<div align="center">
  <img src="image/ICCV2021.png"/>
</div>

官网链接：http://iccv2021.thecvf.com/home<br>
开会时间：2021年10月11日至17日<br>

# :exclamation::exclamation::exclamation::star2::star2::star2:📗📗📗ICCV 2021收录论文已全部公布，下载可在【我爱计算机视觉】后台回复“paper”，即可收到。共计 1612 篇。

### :exclamation::exclamation::exclamation::star2::star2::star2:ICCV 2021收录论文，10 月 21 日整理如下：
* 视频识别
* [Motion-Augmented Self-Training for Video Recognition at Smaller Scale](https://arxiv.org/abs/2105.01646)
* 去雨
* [Unpaired Learning for Deep Image Deraining with Rain Direction Regularize](https://openaccess.thecvf.com/content/ICCV2021/papers/Liu_Unpaired_Learning_for_Deep_Image_Deraining_With_Rain_Direction_Regularizer_ICCV_2021_paper.pdf)<br>:house:[project](https://lewisyangliu.github.io/projects/UDRDR/)
* 分类
* [Variational Feature Disentangling for Fine-Grained Few-Shot Classification](https://arxiv.org/abs/2010.03255)<br>:star:[code](https://github.com/cvlab-stonybrook/vfd-iccv21)
* GAN
* [HeadGAN: One-shot Neural Head Synthesis and Editing](https://arxiv.org/abs/2012.08261)<br>:house:[project](https://michaildoukas.github.io/HeadGAN/):tv:[video](https://www.youtube.com/watch?v=Xo9IW3cMGTg)
* 视频字幕
* [Aligning Subtitles in Sign Language Videos](https://arxiv.org/abs/2105.02877)<br>:house:[project](https://www.robots.ox.ac.uk/~vgg/research/bslalign/):tv:[video](https://youtu.be/WF-I4nP8SMM)
* 自监督
* [Broaden Your Views for Self-Supervised Video Learning](https://arxiv.org/abs/2103.16559)
* [CDS: Cross-Domain Self-supervised Pre-training](https://openaccess.thecvf.com/content/ICCV2021/papers/Kim_CDS_Cross-Domain_Self-Supervised_Pre-Training_ICCV_2021_paper.pdf)
* [On Compositions of Transformations in Contrastive Self-Supervised Learning](https://openaccess.thecvf.com/content/ICCV2021/papers/Patrick_On_Compositions_of_Transformations_in_Contrastive_Self-Supervised_Learning_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/facebookresearch/GDT)
* 视频时序定位
* [Boundary-Sensitive Pre-Training for Temporal Localization in Videos](https://openaccess.thecvf.com/content/ICCV2021/papers/Xu_Boundary-Sensitive_Pre-Training_for_Temporal_Localization_in_Videos_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/frostinassiky/bsp):house:[project](https://frostinassiky.github.io/bsp/)
* 检索
* [HiT: Hierarchical Transformer With Momentum Contrast for Video-Text Retrieval](https://arxiv.org/abs/2103.15049)
* 视频关联性
* [Explainable Video Entailment With Grounded Visual Evidence](https://openaccess.thecvf.com/content/ICCV2021/papers/Chen_Explainable_Video_Entailment_With_Grounded_Visual_Evidence_ICCV_2021_paper.pdf)
* 动作识别
* [Unsupervised Few-Shot Action Recognition via Action-Appearance Aligned Meta-Adaptation](https://arxiv.org/abs/2109.15317)<br>:open_mouth:oral
* 点云
* [Distinctiveness Oriented Positional Equilibrium for Point Cloud Registration]()
* 图像着色
* [Deep Edge-Aware Interactive Colorization Against Color-Bleeding Effects](https://arxiv.org/abs/2107.01619)<br>:open_mouth:oral:house:[project](https://eungyeupkim.github.io/edge-enhancing-colorization/)
* 线段检测
* [ELSD: Efficient Line Segment Detector and Descriptor](https://arxiv.org/abs/2104.14205)
* MVS
* [PatchMatch-RL: Deep MVS with Pixelwise Depth, Normal, and Visibility](https://openaccess.thecvf.com/content/ICCV2021/papers/Lee_PatchMatch-RL_Deep_MVS_With_Pixelwise_Depth_Normal_and_Visibility_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/leejaeyong7/patchmatch-rl)
* 光流估计
* [Separable Flow: Learning Motion Cost Volumes for Optical Flow Estimation](https://openaccess.thecvf.com/content/ICCV2021/papers/Zhang_Separable_Flow_Learning_Motion_Cost_Volumes_for_Optical_Flow_Estimation_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/feihuzhang/SeparableFlow)
* 说话头
* [Learned Spatial Representations for Few-Shot Talking-Head Synthesis](https://arxiv.org/abs/2104.14557)<br>:star:[code](https://github.com/MoustafaMeshry/lsr):house:[project](http://www.cs.umd.edu/~mmeshry/projects/lsr/)
* Transformer
* [Vision Transformers for Dense Prediction](https://arxiv.org/abs/2103.13413)<br>:star:[code](https://github.com/isl-org/DPT)
* [AgentFormer: Agent-Aware Transformers for Socio-Temporal Multi-Agent Forecasting](https://arxiv.org/abs/2103.14023)<br>:star:[code](https://github.com/Khrylx/AgentFormer):house:[project](https://www.ye-yuan.com/agentformer/)
* [Incorporating Convolution Designs into Visual Transformers](https://arxiv.org/abs/2103.11816)<br>:star:[code](https://github.com/coeusguo/ceit)
* 去反射
* [V-DESIRR: Very Fast Deep Embedded Single Image Reflection Removal](https://openaccess.thecvf.com/content/ICCV2021/papers/Prasad_V-DESIRR_Very_Fast_Deep_Embedded_Single_Image_Reflection_Removal_ICCV_2021_paper.pdf)<br>:star:[code](https://www.github.com/ee19d005/vdesirr)
* 细粒度裂纹检测
* [CrackFormer: Transformer Network for Fine-Grained Crack Detection](https://openaccess.thecvf.com/content/ICCV2021/papers/Liu_CrackFormer_Transformer_Network_for_Fine-Grained_Crack_Detection_ICCV_2021_paper.pdf)
* 目标检测
* [Detecting Invisible People](https://arxiv.org/abs/2012.08419)<br>:house:[project](http://www.cs.cmu.edu/~tkhurana/invisible.htm):tv:[video](https://youtu.be/StEfnshXrCE)
* [CaT: Weakly Supervised Object Detection With Category Transfer](https://arxiv.org/abs/2108.07487)<br>:star:[code](https://github.com/MediaBrain-SJTU/CaT)
* [Dynamic DETR: End-to-End Object Detection With Dynamic Attention](https://openaccess.thecvf.com/content/ICCV2021/papers/Dai_Dynamic_DETR_End-to-End_Object_Detection_With_Dynamic_Attention_ICCV_2021_paper.pdf)
* 3D渲染
* [GANcraft: Unsupervised 3D Neural Rendering of Minecraft Worlds](https://arxiv.org/abs/2104.07659)<br>:open_mouth:oral:star:[code](https://github.com/NVlabs/imaginaire):house:[project](https://nvlabs.github.io/GANcraft/):tv:[video](https://youtu.be/5K-AgDmCtt0)
* 跟踪
* [Transparent Object Tracking Benchmark](https://arxiv.org/abs/2011.10875)<br>:house:[project](https://hengfan2010.github.io/projects/TOTB/)
* [DepthTrack: Unveiling the Power of RGBD Tracking](https://arxiv.org/abs/2108.13962)<br>:star:[code](https://github.com/xiaozai/DeT)
* 渲染
* [4DComplete: Non-Rigid Motion Estimation Beyond the Observable Surface](https://arxiv.org/abs/2105.01905)<br>:star:[code](https://github.com/rabbityl/DeformingThings4D):tv:[video](https://www.youtube.com/watch?v=QrSsVoTRpWk)
* 分割
* [Scaling Semantic Segmentation Beyond 1K Classes on a Single GPU](http://arxiv.org/abs/2012.07489):star:[code](https://github.com/shipra25jain/ESSNet)
* 手写文本生成
* [Handwriting Transformers](https://arxiv.org/abs/2104.03964)<br>:star:[code](https://github.com/ankanbhunia/Handwriting-Transformers)
* 人员重识别
* [BV-Person: A Large-Scale Dataset for Bird-View Person Re-Identification](https://openaccess.thecvf.com/content/ICCV2021/papers/Yan_BV-Person_A_Large-Scale_Dataset_for_Bird-View_Person_Re-Identification_ICCV_2021_paper.pdf)<br>:sunflower:[dataset](https://github.com/daidaidouer/BVPerson)
* 视频帧插值
* [XVFI: eXtreme Video Frame Interpolation](https://arxiv.org/abs/2103.16206)<br>:open_mouth:oral:star:[code](https://github.com/JihyongOh/XVFI):tv:[video](https://www.youtube.com/watch?v=5qAiffYFJh8)

<br>:star:[code]():tv:[video]()


<br>:open_mouth:oral:star:[code]():house:[project]():tv:[video]()
<br>:house:[project]():tv:[video]()
<br>:star:[code]():house:[project]()
<br>:sunflower:[dataset]()
<br>:star:[code]()
<br>:house:[project]()
:newspaper:解读:
<br>:trophy:
<br>:star:[code]():house:[project]():tv:[video]()



* 其它
* [Self-Supervised Image Prior Learning with GMM from a Single Noisy Image](https://openaccess.thecvf.com/content/ICCV2021/papers/Liu_Self-Supervised_Image_Prior_Learning_With_GMM_From_a_Single_Noisy_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/HUST-Tan/SS-GMM)
* [Deep Implicit Surface Point Prediction Networks](https://arxiv.org/abs/2106.05779)<br>:star:[code](https://github.com/rahulvenkk/csp-net):house:[project](https://sites.google.com/view/cspnet):tv:[video](https://drive.google.com/file/d/1fferQxWo3ug14NMiV3UqwDGmFaWc5tjh/view)
* [Poly-NL: Linear Complexity Non-local Layers With 3rd Order Polynomials](https://openaccess.thecvf.com/content/ICCV2021/papers/Babiloni_Poly-NL_Linear_Complexity_Non-Local_Layers_With_3rd_Order_Polynomials_ICCV_2021_paper.pdf)
* [Factorizing Perception and Policy for Interactive Instruction Following](https://arxiv.org/abs/2012.03208)<br>:star:[code](https://github.com/gistvision/moca)
* [Group-Wise Inhibition Based Feature Regularization for Robust Classification](https://arxiv.org/abs/2103.02152)<br>:star:[code](https://github.com/LinusWu/TENET_Training)
* [Searching for Robustness: Loss Learning for Noisy Classification Tasks](https://arxiv.org/abs/2103.00243)

# 目录

|:dog:|:mouse:|:hamster:|:tiger:|
|------|------|------|------|
|[61.Metric Learning(元学习)](#61)|[62.Open-Set Recognition(开放集识别)](#62)|[63.Data Augmentation(数据增强)](#63)|[64.Anomaly Detection(异常检测)](#64)|
|[57.Image Matching(图像匹配)](#57)|[58.Computational Photography(光学、几何、光场成像、计算摄影)](#58)|[59.Graph Neural Networks(图神经网络)](#59)|[60.Federated Learning(联合学习)](#60)
|[53.Vision Localization(视觉定位)](#53)|[54.Sketch recognition(草图)](#54)|[55.Activity Recognition(活动识别)](#55)|[56.Dataset(数据集)](#56)|
|[49.Human-Object Interaction(人物交互)](#49)|[50.Continual Learning(持续学习)](#50)|[51.View Synthesis(视图合成)](#51)|[52.Vision-and-Language(视觉语言)](#52)|
|[45.Image Caption(图像字幕)](#45)|[46.Defect Detection(缺陷检测)](#46)|[47.NAS](#47)|[48.6DoF](#48)|
|[41.Out-of-Distribution Detection(OOD)](#41)|[42.Visual Representations Learning(视觉表征学习)](#42)|[43.Dense Prediction(密集预测)](#43)|[44.Human motion prediction(人体运动预测)](#44)|
|[37.Multitask Learning(多任务学习)](#37)|[38.Weakly/Semi-Supervised/Self-supervised/Unsupervised Learning(自/半/弱监督学习)](#38)|[39.Incremental Learning(增量学习)](#39)|[40.Metric Learning(度量学习)](#40)|
|[33.Remote Sensing Images(遥感影像)](#33)|[34.Image Super-Resolution(图像超分辨率)](#34)|[35.Quantization/Pruning/Knowledge Distillation/Model Compression(量化、剪枝、蒸馏、模型压缩/扩展与优化)](#35)|[36.SLAM/AR/VR/机器人](#36)|
|[29.Image Retrieval(图像检索)](#29)|[30.Image Generation/synthesis(图像生成/合成)](#30)|[31.Style Transfer(风格迁移)](#31)|[32.语音](#32)|
|[25.Medical Image(医学影像)](#25)|[26.Image Processing(图像处理)](#26)|[27.Multi-label image recognition(多标签图像识别)](#27)|[28.Contrastive Learning(对比学习)](#28)]
|[21.Active Learning(主动学习)](#21)|[22.GAN](#22)|[23.Gaze Estimation(视线估计)](#23)|[24.Face(人脸)](#24)|
|[17.3D(三维视觉)](#17)|[18.Transformers](#18)|[19.Self-Driving Vehicles(自动驾驶)](#19)|[20.Adversarial Learning(对抗学习)](#20)|
|[13.Image Segmentation(图像分割)](#13)|[14.Object Detection(目标检测)](#13)|[15.Object Tracking(目标跟踪)](#15)|[16.Re-Identification(重识别)](#16)|
|[9.Video](#9)|[10.OCR](10)|[11.Visual Question Answering(视觉问答)](#11)|[12.Image/Fine-Grained Classification(图像/细粒度分类)](#12)|
|[5.Few-Shot/Zero-Shot Learning;Domain Generalization/Adaptation(小/零样本学习;域适应/泛化)](#5)|[6.Point Cloud(点云)](#6)|[7.Scene Graph Generation(场景图生成)](#7)|[8.Human Pose Estimation(人体姿态估计)](#8)|
|[1.Other(其它)](#1)|[2.Sign Language(手语识别)](#2)|[3.Image Clustering(图像聚类)](#3)|[4.Neural rendering(神经渲染)](#4)|

<a name="64"/>

## 64.Anomaly Detection(异常检测)
* 表面异常检测
  * [DRÆM – A discriminatively trained reconstru](https://openaccess.thecvf.com/content/ICCV2021/papers/Zavrtanik_DRAEM_-_A_Discriminatively_Trained_Reconstruction_Embedding_for_Surface_Anomaly_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/VitjanZ/DRAEM)

<a name="63"/>

## 63.Data Augmentation(数据增强)
* [DivAug: Plug-In Automated Data Augmentation With Explicit Diversity Maximization](https://arxiv.org/abs/2103.14545)<br>:star:[code](https://github.com/warai-0toko/DivAug)

<a name="62"/>

## 62.Open-Set Recognition(开放集识别)
* [OpenGAN: Open-Set Recognition via Open Data Generation](https://arxiv.org/abs/2104.02939)<br>:trophy:Best Paper Honorable Mention
* [Conditional Variational Capsule Network for Open Set Recognition](https://arxiv.org/abs/2104.09159)<br>:star:[code](https://github.com/guglielmocamporese/cvaecaposr)

<a name="61"/>

## 61.Metric Learning(元学习)
* [Do Different Deep Metric Learning Losses Lead to Similar Learned Features?](https://openaccess.thecvf.com/content/ICCV2021/papers/Kobs_Do_Different_Deep_Metric_Learning_Losses_Lead_to_Similar_Learned_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/konstantinkobs/DML-analysis)

<a name="60"/>

## 60.Federated Learning(联合学习)
* [Federated Learning for Non-IID Data via Unified Feature Learning and Optimization Objective Alignment](https://openaccess.thecvf.com/content/ICCV2021/papers/Zhang_Federated_Learning_for_Non-IID_Data_via_Unified_Feature_Learning_and_ICCV_2021_paper.pdf)
* [Ensemble Attention Distillation for Privacy-Preserving Federated Learning](https://openaccess.thecvf.com/content/ICCV2021/papers/Gong_Ensemble_Attention_Distillation_for_Privacy-Preserving_Federated_Learning_ICCV_2021_paper.pdf)

<a name="59"/>

## 59.Graph Neural Networks(图神经网络)
* [Meta-Aggregator: Learning to Aggregate for 1-bit Graph Neural Networks](https://arxiv.org/abs/2109.12872) 
* [PoGO-Net: Pose Graph Optimization With Graph Neural Networks](https://openaccess.thecvf.com/content/ICCV2021/papers/Li_PoGO-Net_Pose_Graph_Optimization_With_Graph_Neural_Networks_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/xxylii/PoGO-Net)
* [Dynamic Dual Gating Neural Networks](https://openaccess.thecvf.com/content/ICCV2021/papers/Li_Dynamic_Dual_Gating_Neural_Networks_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/lfr-0531/DGNet)

<a name="58"/>

## 58.Computational Photography(光学、几何、光场成像、计算摄影)
* [An Asynchronous Kalman Filter for Hybrid Event Cameras](https://arxiv.org/abs/2012.05590)<br>:star:[code](https://github.com/ziweiWWANG/AKF)
* [4D Cloud Scattering Tomography](https://openaccess.thecvf.com/content/ICCV2021/papers/Ronen_4D_Cloud_Scattering_Tomography_ICCV_2021_paper.pdf)
* Snapshot compressive imaging(快照压缩成像)
  * [Dense Deep Unfolding Network with 3D-CNN Prior for Snapshot Compressive Imaging](https://arxiv.org/abs/2109.06548)<br>:star:[code](https://github.com/jianzhangcs/SCI3D)
* 光场
  * [Light Field Saliency Detection with Dual Local Graph Learning andReciprocative Guidance](https://arxiv.org/abs/2110.00698)
  * [Fast Light-Field Disparity Estimation With Multi-Disparity-Scale Cost Aggregation](https://openaccess.thecvf.com/content/ICCV2021/papers/Huang_Fast_Light-Field_Disparity_Estimation_With_Multi-Disparity-Scale_Cost_Aggregation_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/zcong17huang/FastLFnet)
* 压缩成像
  * [Time-Multiplexed Coded Aperture Imaging: Learned Coded Aperture and Pixel Exposures for Compressive Imaging Systems](https://arxiv.org/abs/2104.02820)
* Homography Estimation
  * [LocalTrans: A Multiscale Local Transformer Network for Cross-Resolution Homography Estimation](http://arxiv.org/abs/2106.04067)

<a name="57"/>

## 57.Image Matching(图像匹配)
* [Matching in the Dark: A Dataset for Matching Image Pairs of Low-light Scenes](https://arxiv.org/abs/2109.03585)
* 特征点匹配
  * [P2-Net: Joint Description and Detection of Local Features for Pixel and Point Matching](https://openaccess.thecvf.com/content/ICCV2021/papers/Wang_P2-Net_Joint_Description_and_Detection_of_Local_Features_for_Pixel_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/BingCS/P2-Net)
 
<a name="56"/>

## 56.Dataset(数据集)
* [Large Scale Multi-Illuminant (LSMI) Dataset for Developing White Balance Algorithm Under Mixed Illumination](https://openaccess.thecvf.com/content/ICCV2021/papers/Kim_Large_Scale_Multi-Illuminant_LSMI_Dataset_for_Developing_White_Balance_Algorithm_ICCV_2021_paper.pdf)<br>:sunflower:[dataset](https://github.com/DY112/LSMI-dataset)
* [FloW: A Dataset and Benchmark for Floating Waste Detection in Inland Waters](https://openaccess.thecvf.com/content/ICCV2021/papers/Cheng_FloW_A_Dataset_and_Benchmark_for_Floating_Waste_Detection_in_ICCV_2021_paper.pdf)<br>:sunflower:[dataset](https://github.com/ORCA-Uboat/FloW-Dataset)<br>内陆水域漂浮废物检测数据集和基准
* [FloorPlanCAD: A Large-Scale CAD Drawing Dataset for Panoptic Symbol Spotting](https://openaccess.thecvf.com/content/ICCV2021/papers/Fan_FloorPlanCAD_A_Large-Scale_CAD_Drawing_Dataset_for_Panoptic_Symbol_Spotting_ICCV_2021_paper.pdf)<br>:house:[project](https://floorplancad.github.io/)
* 生物医学图像
  * [BioFors: A Large Biomedical Image Forensics Dataset](https://arxiv.org/abs/2108.12961)
* 3D重建
  * [Common Objects in 3D: Large-Scale Learning and Evaluation of Real-life 3D Category Reconstruction](https://arxiv.org/abs/2109.00512)<br>:sunflower:[dataset](https://github.com/facebookresearch/co3d)
* 航空影像数据集
  * [Beyond Road Extraction: A Dataset for Map Update using Aerial Images](https://arxiv.org/abs/2110.04690)<br>:star:[code](https://github.com/favyen/muno21):house:[project](https://favyen.com/muno21/)<br>用于使用航拍图像更新地图的数据集
* 动作识别
  * [HAA500: Human-Centric Atomic Action Dataset with Curated Videos](https://arxiv.org/abs/2009.05224)
* 目标识别
  * [ORBIT: A Real-World Few-Shot Dataset for Teachable Object Recognition](https://arxiv.org/abs/2104.03841)<br>:star:[code](https://github.com/microsoft/ORBIT-Dataset):sunflower:[dataset](https://city.figshare.com/articles/dataset/ORBIT_A_real-world_few-shot_dataset_for_teachable_object_recognition_collected_from_people_who_are_blind_or_low_vision/14294597)
* 车道线检测
  * [VIL-100: A New Dataset and a Baseline Model for Video Instance Lane Detection](https://openaccess.thecvf.com/content/ICCV2021/papers/Zhang_VIL-100_A_New_Dataset_and_a_Baseline_Model_for_Video_ICCV_2021_paper.pdf)<br>:sunflower:[dataset](https://github.com/yujun0-0/MMA-Net)
* 自动驾驶
  * [Large Scale Interactive Motion Forecasting for Autonomous Driving: The Waymo Open Motion Dataset](https://openaccess.thecvf.com/content/ICCV2021/papers/Ettinger_Large_Scale_Interactive_Motion_Forecasting_for_Autonomous_Driving_The_Waymo_ICCV_2021_paper.pdf)

<a name="55"/>

## 55.Activity Recognition(活动识别)
* [Selective Feature Compression for Efficient Activity Recognition Inference](https://arxiv.org/abs/2104.00179)
* 小组活动识别
  * [Spatio-Temporal Dynamic Inference Network for Group Activity Recognition](https://arxiv.org/abs/2108.11743)<br>:star:[code](https://github.com/JacobYuan7/DIN_GAR)
  * [GroupFormer: Group Activity Recognition with Clustered Spatial-Temporal Transformer](https://arxiv.org/abs/2108.12630)<br>:star:[code](https://github.com/xueyee/GroupFormer) 
 

<a name="54"/>

## 54.Sketch recognition(草图)
* [SketchLattice: Latticed Representation for Sketch Manipulation](https://arxiv.org/abs/2108.11636)
* [SketchAA: Abstract Representation for Abstract Sketches](https://openaccess.thecvf.com/content/ICCV2021/papers/Yang_SketchAA_Abstract_Representation_for_Abstract_Sketches_ICCV_2021_paper.pdf)

<a name="53"/>

## 53.Vision Localization(视觉定位)
* [Continual Learning for Image-Based Camera Localization](https://arxiv.org/abs/2108.09112)<br>:star:[code](https://github.com/AaltoVision/CL_HSCNet)
* [CrowdDriven: A New Challenging Dataset for Outdoor Visual Localization](https://arxiv.org/abs/2109.04527)<br>:sunflower:[dataset](http://mapillary.com/)

<a name="52"/>

## 52.Vision-and-Language(视觉语言)
* [YouRefIt: Embodied Reference Understanding with Language and Gesture](https://arxiv.org/abs/2109.03413)<br>:open_mouth:oral:house:[project](https://yixchen.github.io/YouRefIt/)
* 视觉语言导航
  * [Airbert: In-domain Pretraining for Vision-and-Language Navigation](https://arxiv.org/abs/2108.09105)<br>:house:[project](https://airbert-vln.github.io/)
  * [Waypoint Models for Instruction-guided Navigation in Continuous Environments](https://arxiv.org/abs/2110.02207)<br>:open_mouth:oral:star:[code](https://github.com/jacobkrantz/VLN-CE):house:[project](https://jacobkrantz.github.io/waypoint-vlnce/):tv:[video](https://youtu.be/hrHj9-1xoio)
  * [The Road To Know-Where: An Object-and-Room Informed Sequential BERT for Indoor Vision-Language Navigation](https://openaccess.thecvf.com/content/ICCV2021/papers/Qi_The_Road_To_Know-Where_An_Object-and-Room_Informed_Sequential_BERT_for_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/YuankaiQi/ORIST)

<a name="51"/>

## 51.View Synthesis(视图合成)
* [Out-of-boundary View Synthesis Towards Full-Frame Video Stabilization](https://arxiv.org/abs/2108.09041)<br>:star:[code](https://github.com/Annbless/OVS_Stabilization)
* [Deep 3D Mask Volume for View Synthesis of Dynamic Scenes](https://arxiv.org/abs/2108.13408)<br>:house:[project](https://cseweb.ucsd.edu//~viscomp/projects/ICCV21Deep/)
* [Embedding Novel Views in a Single JPEG Image](https://arxiv.org/abs/2108.13003)
* [Video Autoencoder: self-supervised disentanglement of static 3D structure and motion](https://arxiv.org/abs/2110.02951)<br>:open_mouth:oral:star:[code](https://github.com/zlai0/VideoAutoencoder/):house:[project](https://zlai0.github.io/VideoAutoencoder/#method_video):tv:[video](https://www.youtube.com/watch?v=UaJZd4FrM8E)
* [Geometry-Free View Synthesis: Transformers and No 3D Priors](https://arxiv.org/abs/2104.07652)<br>:star:[code](https://github.com/CompVis/geometry-free-view-synthesis)
* [Dynamic View Synthesis From Dynamic Monocular Video](https://arxiv.org/abs/2105.06468)<br>:house:[project](https://free-view-video.github.io/):tv:[video](https://youtu.be/j8CUzIR0f8M)

<a name="50"/>

## 50.Continual Learning(持续学习)
* [Online Continual Learning with Natural Distribution Shifts: An Empirical Study with Visual Data](https://arxiv.org/abs/2108.09020)<br>:star:[code](https://github.com/IntelLabs/continuallearning)
* [Continual Learning on Noisy Data Streams via Self-Purified Replay](https://openaccess.thecvf.com/content/ICCV2021/papers/Kim_Continual_Learning_on_Noisy_Data_Streams_via_Self-Purified_Replay_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/ecrireme/SPR)
* [Rehearsal Revealed: The Limits and Merits of Revisiting Samples in Continual Learning](https://arxiv.org/abs/2104.07446)<br>:star:[code](https://github.com/Mattdl/RehearsalRevealed)
* [Co2L: Contrastive Continual Learning](https://openaccess.thecvf.com/content/ICCV2021/papers/Cha_Co2L_Contrastive_Continual_Learning_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/chaht01/Co2L)

  
<a name="49"/>

## 49.Human-Object Interaction(人物交互)
* [Exploiting Scene Graphs for Human-Object Interaction Detection](https://arxiv.org/abs/2108.08584)<br>:star:[code](https://github.com/ht014/SG2HOI)
* [Spatially Conditioned Graphs for Detecting Human-Object Interactions](https://arxiv.org/abs/2012.06060)<br>:star:[code](https://github.com/fredzzhang/spatially-conditioned-graphs):tv:[video](https://www.youtube.com/watch?v=gkBWi_rWedU)
* [Virtual Multi-Modality Self-Supervised Foreground Matting for Human-Object Interaction](https://arxiv.org/abs/2110.03278)
* [Detecting Human-Object Relationships in Videos](https://openaccess.thecvf.com/content/ICCV2021/papers/Ji_Detecting_Human-Object_Relationships_in_Videos_ICCV_2021_paper.pdf)
* H2O
  * [H2O: A Benchmark for Visual Human-Human Object Handover Analysis](https://arxiv.org/abs/2104.11466)
* Human Interaction Understanding
  * [Consistency-Aware Graph Network for Human Interaction Understanding](https://arxiv.org/abs/2011.10250)<br>:star:[code](https://github.com/deepgogogo/CAGNet?v=1)
* object-hand interaction
  * [Toward Human-Like Grasp: Dexterous Grasping via Semantic Representation of Object-Hand](https://openaccess.thecvf.com/content/ICCV2021/papers/Zhu_Toward_Human-Like_Grasp_Dexterous_Grasping_via_Semantic_Representation_of_Object-Hand_ICCV_2021_paper.pdf)

<a name="48"/>

## 48.6DoF
* [SO-Pose: Exploiting Self-Occlusion for Direct 6D Pose Estimation](https://arxiv.org/abs/2108.08367)<br>:star:[code](https://github.com/shangbuhuan13/SO-Pose)
* [StereOBJ-1M: Large-scale Stereo Image Dataset for 6D Object Pose Estimation](https://arxiv.org/abs/2109.10115)
* [SGPA: Structure-Guided Prior Adaptation for Category-Level 6D Object Pose Estimation](https://openaccess.thecvf.com/content/ICCV2021/papers/Chen_SGPA_Structure-Guided_Prior_Adaptation_for_Category-Level_6D_Object_Pose_Estimation_ICCV_2021_paper.pdf)
* [RePOSE: Fast 6D Object Pose Refinement via Deep Texture Rendering](https://arxiv.org/abs/2104.00633)<br>:star:[code](https://github.com/sh8/repose)
* [DualPoseNet: Category-Level 6D Object Pose and Size Estimation Using Dual Pose Network With Refined Learning of Pose Consistency](https://arxiv.org/abs/2103.06526)<br>:star:[code](https://github.com/Gorilla-Lab-SCUT/DualPoseNet)
* 物体姿势估计
  * [CAPTRA: CAtegory-Level Pose Tracking for Rigid and Articulated Objects From Point Clouds](https://arxiv.org/abs/2104.03437)<br>:open_mouth:oral:star:[code](https://github.com/halfsummer11/CAPTRA):house:[project](https://yijiaweng.github.io/CAPTRA/):tv:[video](https://youtu.be/EkcCEj7gZGg)

<a name="47"/>

## 47.NAS
* [BN-NAS: Neural Architecture Search with Batch Normalization](https://arxiv.org/abs/2108.07375)<br>:star:[code](https://github.com/bychen515/BNNAS)
* [RANK-NOSH: Efficient Predictor-Based Architecture Search via Non-Uniform Successive Halving](https://arxiv.org/abs/2108.08019)
* [Pi-NAS: Improving Neural Architecture Search by Reducing Supernet Training Consistency Shift](https://arxiv.org/abs/2108.09671)<br>:star:[code](https://github.com/Ernie1/Pi-NAS)
* [Evolving Search Space for Neural Architecture Search](https://arxiv.org/abs/2011.10904)<br>:star:[code](https://github.com/orashi/NSE_NAS):tv:[video](https://www.youtube.com/watch?v=fq21WBaumRc)
* [FairNAS: Rethinking Evaluation Fairness of Weight Sharing Neural Architecture Search](https://arxiv.org/abs/1907.01845)<br>:star:[code](https://github.com/xiaomi-automl/FairNAS)
* [GLiT: Neural Architecture Search for Global and Local Image Transformer](https://arxiv.org/abs/2107.02960)<br>:star:[code](https://github.com/bychen515/GLiT)
* [Neural Architecture Search for Joint Human Parsing and Pose Estimation](https://openaccess.thecvf.com/content/ICCV2021/papers/Zeng_Neural_Architecture_Search_for_Joint_Human_Parsing_and_Pose_Estimation_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/GuHuangAI/NPP)
* [Distilling Optimal Neural Networks: Rapid Search in Diverse Spaces](https://arxiv.org/abs/2012.08859)
* [Learning Latent Architectural Distribution in Differentiable Neural Architecture Search via Variational Information Maximization](https://openaccess.thecvf.com/content/ICCV2021/papers/Wang_Learning_Latent_Architectural_Distribution_in_Differentiable_Neural_Architecture_Search_via_ICCV_2021_paper.pdf)
* [Not All Operations Contribute Equally: Hierarchical Operation-Adaptive Predictor for Neural Architecture Search](https://openaccess.thecvf.com/content/ICCV2021/papers/Chen_Not_All_Operations_Contribute_Equally_Hierarchical_Operation-Adaptive_Predictor_for_Neural_ICCV_2021_paper.pdf)

<a name="46"/>

## 46.Defect Detection(缺陷检测)
* [DRÆM -- A discriminatively trained reconstruction embedding for surface anomaly detection](https://arxiv.org/abs/2108.07610)

<a name="45"/>

## 45.Image Caption(图像字幕)
* [Who's Waldo? Linking People Across Text and Images](https://arxiv.org/abs/2108.07253)<br>:open_mouth:oral:house:[project](https://whoswaldo.github.io/)<br>:newspaper:解读:[ICCV2021 Oral-新任务！新数据集！康奈尔大学提出了类似VG但又不是VG的PVG任务](https://mp.weixin.qq.com/s/QC1UQRmZKgS0dctTXQ77Bg)
* [Partial Off-Policy Learning: Balance Accuracy and Diversity for Human-Oriented Image Captioning](https://openaccess.thecvf.com/content/ICCV2021/papers/Shi_Partial_Off-Policy_Learning_Balance_Accuracy_and_Diversity_for_Human-Oriented_Image_ICCV_2021_paper.pdf)
* [Topic Scene Graph Generation by Attention Distillation From Caption](https://openaccess.thecvf.com/content/ICCV2021/papers/Wang_Topic_Scene_Graph_Generation_by_Attention_Distillation_From_Caption_ICCV_2021_paper.pdf)<br>:star:[code](https://vipl.ict.ac.cn/view_database.php?id=6)
* art description generation(艺术描述生成)
  * [Explain Me the Painting: Multi-Topic Knowledgeable Art Description Generation](https://arxiv.org/abs/2109.05743)<br>:star:[code](https://github.com/noagarcia/explain-paintings)
* Change Captioning
  * [Viewpoint-Agnostic Change Captioning With Cycle Consistency](https://openaccess.thecvf.com/content/ICCV2021/papers/Kim_Viewpoint-Agnostic_Change_Captioning_With_Cycle_Consistency_ICCV_2021_paper.pdf)

<a name="44"/>

## 44.Human motion prediction(人体运动预测)
* [MSR-GCN: Multi-Scale Residual Graph Convolution Networks for Human Motion Prediction](https://arxiv.org/abs/2108.07152)<br>:star:[code](https://github.com/Droliven/MSRGCN)
* [Stochastic Scene-Aware Motion Prediction](https://arxiv.org/abs/2108.08284)<br>:star:[code](https://github.com/mohamedhassanmus/SAMP):house:[project](https://samp.is.tue.mpg.de/)  
* [Generating Smooth Pose Sequences for Diverse Human Motion Prediction](https://arxiv.org/abs/2108.08422)<br>:open_mouth:oral:star:[code](https://github.com/wei-mao-2019/gsps)
* [TRiPOD: Human Trajectory and Pose Dynamics Forecasting in the Wild](https://arxiv.org/abs/2104.04029)<br>:house:[project](https://somof.stanford.edu/)
* 3D人体运动预测
  * [Contextually Plausible and Diverse 3D Human Motion Prediction](https://arxiv.org/abs/1912.08521)

<a name="43"/>

## 43.Dense Prediction(密集预测)
* [FaPN: Feature-aligned Pyramid Network for Dense Image Prediction](https://arxiv.org/abs/2108.07058)<br>:star:[code](https://github.com/EMI-Group/FaPN)
* 多任务密集预测
  * [Exploring Relational Context for Multi-Task Dense Prediction](https://arxiv.org/abs/2104.13874)

<a name="42"/>

## 42.Representations Learning(表征学习)
* [Learning From Noisy Data With Robust Representation Learning](https://openaccess.thecvf.com/content/ICCV2021/papers/Li_Learning_From_Noisy_Data_With_Robust_Representation_Learning_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/salesforce/RRL/)
* [Self-Supervised Representation Learning From Flow Equivariance](https://arxiv.org/abs/2101.06553)
* 视觉表征学习
  * [Self-Supervised Visual Representations Learning by Contrastive Mask Prediction](https://arxiv.org/abs/2108.07954)
  * [Temporal Knowledge Consistency for Unsupervised Visual Representation Learning](https://arxiv.org/abs/2108.10668)
  * [Contrasting Contrastive Self-Supervised Representation Learning Pipelines](https://arxiv.org/abs/2103.14005)<br>:star:[code](https://github.com/allenai/virb)
  * [Concept Generalization in Visual Representation Learning](https://arxiv.org/abs/2012.05649)<br>:house:[project](https://europe.naverlabs.com/cog-benchmark)
  * [Collaborative Unsupervised Visual Representation Learning from Decentralized Data](https://arxiv.org/abs/2108.06492)
  * [Episodic Transformer for Vision-and-Language Navigation](https://arxiv.org/abs/2105.06453)<br>:star:[code](https://github.com/alexpashevich/E.T.)
  * [Multi-VAE: Learning Disentangled View-Common and View-Peculiar Visual Representations for Multi-View Clustering](https://openaccess.thecvf.com/content/ICCV2021/papers/Xu_Multi-VAE_Learning_Disentangled_View-Common_and_View-Peculiar_Visual_Representations_for_Multi-View_ICCV_2021_paper.pdf)
* 视频表示学习
  * [Composable Augmentation Encoding for Video Representation Learning](https://arxiv.org/abs/2104.00616)
  * [Motion-Focused Contrastive Learning of Video Representations](https://openaccess.thecvf.com/content/ICCV2021/papers/Li_Motion-Focused_Contrastive_Learning_of_Video_Representations_ICCV_2021_paper.pdf)
 
<a name="41"/>

## 41.Out-of-Distribution Detection(OOD)
* [CODEs: Chamfer Out-of-Distribution Examples against Overconfidence Issue](https://arxiv.org/abs/2108.06024)
* [Semantically Coherent Out-of-Distribution Detection](https://arxiv.org/abs/2108.11941)<br>:star:[code](https://github.com/jingkang50/ICCV21_SCOOD):house:[project](https://jingkang50.github.io/projects/scood)
* [The Many Faces of Robustness: A Critical Analysis of Out-of-Distribution Generalization](https://arxiv.org/abs/2006.16241)<br>:star:[code](https://github.com/hendrycks/imagenet-r)

<a name="40"/>

## 40.Metric Learning(度量学习)
* [Towards Interpretable Deep Metric Learning with Structural Matching](https://arxiv.org/abs/2108.05889)<br>:star:[code](https://github.com/wl-zhao/DIML)
* [Deep Relational Metric Learning](https://arxiv.org/abs/2108.10026)<br>:star:[code](https://github.com/zbr17/DRML)
* [LoOp: Looking for Optimal Hard Negative Embeddings for Deep Metric Learning](https://arxiv.org/abs/2108.09335)<br>:star:[code](https://github.com/puneesh00/LoOp)

<a name="39"/>

## 39.Incremental Learning(增量学习)
* 类增量学习
  * [Always Be Dreaming: A New Approach for Data-Free Class-Incremental Learning](https://arxiv.org/abs/2106.09701)<br>:newspaper:解读:[让模型实现“终生学习”，佐治亚理工学院提出Data-Free的增量学习](https://mp.weixin.qq.com/s/Fm9ufPD6rzL2VzaqpdFpjg)

<a name="38"/>

## 38.Weakly/Semi-Supervised/Self-supervised/Unsupervised Learning(自/半/弱监督学习)
* 半监督
  * [Trash to Treasure: Harvesting OOD Data with Cross-Modal Matching for Open-Set Semi-Supervised Learning](https://arxiv.org/abs/2108.05617)
  * [Semi-Supervised Learning of Visual Features by Non-Parametrically Predicting View Assignments With Support Samples](https://arxiv.org/abs/2104.13963)<br>:star:[code](https://github.com/facebookresearch/suncet)
  * [Semi-Supervised Active Learning for Semi-Supervised Models: Exploit Adversarial Examples With Graph-Based Virtual Labels](https://openaccess.thecvf.com/content/ICCV2021/papers/Guo_Semi-Supervised_Active_Learning_for_Semi-Supervised_Models_Exploit_Adversarial_Examples_With_ICCV_2021_paper.pdf)
* 自监督
  * [Focus on the Positives: Self-Supervised Learning for Biodiversity Monitoring](https://arxiv.org/abs/2108.06435)<br>:star:[code](https://github.com/omipan/camera_traps_self_supervised)
  * [Self-supervised Neural Networks for Spectral Snapshot Compressive Imaging](https://arxiv.org/abs/2108.12654)<br>:star:[code](https://github.com/mengziyi64/CASSI-Self-Supervised)
  * [ISD: Self-Supervised Learning by Iterative Similarity Distillation](https://arxiv.org/abs/2012.09259)<br>:star:[code](https://github.com/UMBCvision/ISD)
  * [Contrast and Order Representations for Video Self-Supervised Learning](https://openaccess.thecvf.com/content/ICCV2021/papers/Hu_Contrast_and_Order_Representations_for_Video_Self-Supervised_Learning_ICCV_2021_paper.pdf)
  * [On Feature Decorrelation in Self-Supervised Learning](https://arxiv.org/abs/2105.00470)<br>:open_mouth:oral
  * [Geography-Aware Self-Supervised Learning](https://arxiv.org/abs/2011.09980)
  * [Multimodal Clustering Networks for Self-supervised Learning from Unlabeled Videos](https://arxiv.org/abs/2104.12671)
  * [Efficient Visual Pretraining with Contrastive Detection](https://arxiv.org/abs/2103.10957)
* 弱监督
  * [Weakly Supervised Representation Learning With Coarse Labels](https://arxiv.org/abs/2005.09681)<br>:star:[code](https://github.com/idstcv/CoIns)

<a name="37"/>

## 37.Multitask Learning(多任务学习)
* [MultiTask-CenterNet (MCN): Efficient and Diverse Multitask Learning using an Anchor Free Approach](https://arxiv.org/abs/2108.05060)<br>:newspaper:解读:[ICCV2021《MultiTask CenterNet》CV多任务新进展！一节更比三节强](https://mp.weixin.qq.com/s/toAZS0OHdW4MG30P1wAAUA)
* [Multi-Task Self-Training for Learning General Representations](https://arxiv.org/abs/2108.11353)<br>:newspaper:解读:[ICCV2021 MuST：还在特定任务里为刷点而苦苦挣扎？谷歌的大佬们都已经开始玩多任务训练了](https://mp.weixin.qq.com/s/nhv1l9xBSaZceibIn5fhqw)
* [UniT: Multimodal Multitask Learning With a Unified Transformer](https://arxiv.org/abs/2102.10772)<br>:star:[code](https://mmf.sh/)
* [Learning Multiple Pixelwise Tasks Based on Loss Scale Balancing](https://openaccess.thecvf.com/content/ICCV2021/papers/Lee_Learning_Multiple_Pixelwise_Tasks_Based_on_Loss_Scale_Balancing_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/jaehanlee-mcl/LSB-MTL)
* [Learning With Privileged Tasks](https://openaccess.thecvf.com/content/ICCV2021/papers/Song_Learning_With_Privileged_Tasks_ICCV_2021_paper.pdf)

<a name="36"/>

## 36.SLAM/AR/VR/机器人
* 机器人
  * 室内导航
    * [The Surprising Effectiveness of Visual Odometry Techniques for Embodied PointGoal Navigation](https://arxiv.org/abs/2108.11550)<br>:star:[code](https://github.com/Xiaoming-Zhao/PointNav-VO):house:[project](https://xiaoming-zhao.github.io/projects/pointnav-vo/)
    * [Pathdreamer: A World Model for Indoor Navigation](https://arxiv.org/abs/2105.08756)<br>:tv:[video](https://www.youtube.com/watch?v=StklIENGqs0)
* VR/AR
  * [The Power of Points for Modeling Humans in Clothing](https://arxiv.org/abs/2109.01137)<br>:star:[code](https://github.com/qianlim/POP):house:[project](https://qianlim.github.io/POP):tv:[video](https://youtu.be/5M4F9zSWIEE)
  * 虚拟试穿  
    * [M3D-VTON: A Monocular-to-3D Virtual Try-On Network](https://arxiv.org/abs/2108.05126)<br>:star:[code](https://github.com/fyviezhao/M3D-VTON)
    * [ZFlow: Gated Appearance Flow-based Virtual Try-on with 3D Priors](https://arxiv.org/abs/2109.07001)
    * [Dressing in Order: Recurrent Person Image Generation for Pose Transfer, Virtual Try-On and Outfit Editing](https://arxiv.org/abs/2104.07021)
* SLAM
  * [On the Limits of Pseudo Ground Truth in Visual Camera Re-localisation](https://arxiv.org/abs/2109.00524)<br>:star:[code](https://github.com/tsattler/visloc_pseudo_gt_limitations/)
  * [Transfusion: A Novel SLAM Method Focused on Transparent Objects](https://openaccess.thecvf.com/content/ICCV2021/papers/Zhu_Transfusion_A_Novel_SLAM_Method_Focused_on_Transparent_Objects_ICCV_2021_paper.pdf)
  * [iMAP: Implicit Mapping and Positioning in Real-Time](https://arxiv.org/abs/2103.12352)
  * Place Recognition
    * [Attentional Pyramid Pooling of Salient Visual Residuals for Place Recognition](https://openaccess.thecvf.com/content/ICCV2021/papers/Peng_Attentional_Pyramid_Pooling_of_Salient_Visual_Residuals_for_Place_Recognition_ICCV_2021_paper.pdf)
 
<a name="35"/>

## 35.Quantization/Pruning/Knowledge Distillation/Model Compression(量化、剪枝、蒸馏、模型压缩/扩展与优化)
* 知识蒸馏
  * [Distilling Holistic Knowledge with Graph Neural Networks](https://arxiv.org/abs/2108.05507)<br>:star:[code](https://github.com/wyc-ruiker/HKD)
  * [Lipschitz Continuity Guided Knowledge Distillation](https://arxiv.org/abs/2108.12905)<br>:star:[code](https://github.com/42Shawn/LONDON/tree/master)
  * [Densely Guided Knowledge Distillation Using Multiple Teacher Assistants](https://arxiv.org/abs/2009.08825)<br>:star:[code](https://github.com/wonchulSon/DGKD)
  * [Revisiting Adversarial Robustness Distillation: Robust Soft Labels Make Student Better](https://arxiv.org/abs/2108.07969)<br>:star:[code](https://github.com/zibojia/RSLAD)
  * [Compressing Visual-linguistic Model via Knowledge Distillation](https://arxiv.org/abs/2104.02096)
  * [Self-Knowledge Distillation With Progressive Refinement of Targets](https://arxiv.org/abs/2006.12000)<br>:star:[code](https://github.com/lgcnsai/PS-KD-Pytorch):tv:[video](https://drive.google.com/file/d/1QxqSbzn-egdYI13IYn3W4dmIvm_Iw4ku/view)
* 量化
  * [Distance-aware Quantization](https://arxiv.org/abs/2108.06983)<br>:star:[code](https://github.com/cvlab-yonsei/DAQ):house:[project](https://cvlab.yonsei.ac.kr/projects/DAQ/) 
  * [Dynamic Network Quantization for Efficient Video Inference](https://arxiv.org/abs/2108.10394)<br>:star:[code](https://github.com/sunxm2357/VideoIQ):house:[project](https://cs-people.bu.edu/sunxm/VideoIQ/project.html)
  * [Cluster-Promoting Quantization with Bit-Drop for Minimizing Network Quantization Loss](https://arxiv.org/abs/2109.02100)
  * [Improving Low-Precision Network Quantization via Bin Regularization](https://openaccess.thecvf.com/content/ICCV2021/papers/Han_Improving_Low-Precision_Network_Quantization_via_Bin_Regularization_ICCV_2021_paper.pdf)
* 模型压缩
  * [GDP: Stabilized Neural Network Pruning via Gates with Differentiable Polarization](https://arxiv.org/abs/2109.02220)
* 剪枝
  * [ResRep: Lossless CNN Pruning via Decoupling Remembering and Forgetting](https://arxiv.org/abs/2007.03260)<br>:star:[code](https://github.com/DingXiaoH/ResRep)

<a name="34"/>

## 34.Super-Resolution(超分辨率)
* ISR
  * [Mutual Affine Network for Spatially Variant Kernel Estimation in Blind Image Super-Resolution](https://arxiv.org/abs/2108.05302)<br>:star:[code](https://github.com/JingyunLiang/MANet)
  * [Hierarchical Conditional Flow: A Unified Framework for Image Super-Resolution and Image Rescaling](https://arxiv.org/abs/2108.05301)<br>:star:[code](https://github.com/JingyunLiang/HCFlow)
  * [Deep Reparametrization of Multi-Frame Super-Resolution and Denoising](https://arxiv.org/abs/2108.08286)<br>:open_mouth:oral
  * [Dual-Camera Super-Resolution with Aligned Attention Modules](https://arxiv.org/abs/2109.01349)<br>:star:[code](https://github.com/Tengfei-Wang/DualCameraSR):house:[project](https://tengfei-wang.github.io/Dual-Camera-SR/index.html):tv:[video](https://www.youtube.com/watch?v=5TiUfAcNvuw)
  * [Attention-Based Multi-Reference Learning for Image Super-Resolution](https://arxiv.org/abs/2108.13697)<br>:star:[code](https://github.com/marcopesavento/AMRSR):house:[project](https://marcopesavento.github.io/AMRSR/)
  * [Learning a Single Network for Scale-Arbitrary Super-Resolution](https://arxiv.org/abs/2004.03791)
  * [Fourier Space Losses for Efficient Perceptual Image Super-Resolution](https://arxiv.org/abs/2106.00783)<br>:star:[code](https://github.com/dariofuoli)
  * [Achieving On-Mobile Real-Time Super-Resolution With Neural Architecture and Pruning Search](https://arxiv.org/abs/2108.08910)
  * [Designing a Practical Degradation Model for Deep Blind Image Super-Resolution](https://arxiv.org/abs/2103.14006)<br>:star:[code](https://github.com/cszn/BSRGAN)
  * [Event Stream Super-Resolution via Spatiotemporal Constraint Learning](https://openaccess.thecvf.com/content/ICCV2021/papers/Li_Event_Stream_Super-Resolution_via_Spatiotemporal_Constraint_Learning_ICCV_2021_paper.pdf)
  * [Dynamic High-Pass Filtering and Multi-Spectral Attention for Image Super-Resolution](https://openaccess.thecvf.com/content/ICCV2021/papers/Magid_Dynamic_High-Pass_Filtering_and_Multi-Spectral_Attention_for_Image_Super-Resolution_ICCV_2021_paper.pdf)
  * [Super-Resolving Cross-Domain Face Miniatures by Peeking at One-Shot Exemplar](https://arxiv.org/abs/2103.08863)
  * [Context Reasoning Attention Network for Image Super-Resolution](https://openaccess.thecvf.com/content/ICCV2021/papers/Zhang_Context_Reasoning_Attention_Network_for_Image_Super-Resolution_ICCV_2021_paper.pdf)
  * [EvIntSR-Net: Event Guided Multiple Latent Frames Reconstruction and Super-Resolution](https://openaccess.thecvf.com/content/ICCV2021/papers/Han_EvIntSR-Net_Event_Guided_Multiple_Latent_Frames_Reconstruction_and_Super-Resolution_ICCV_2021_paper.pdf)
  * [Super Resolve Dynamic Scene from Continuous Spike Streams](https://openaccess.thecvf.com/content/ICCV2021/papers/Zhao_Super_Resolve_Dynamic_Scene_From_Continuous_Spike_Streams_ICCV_2021_paper.pdf)
  * [Deep Blind Video Super-Resolution](https://openaccess.thecvf.com/content/ICCV2021/papers/Pan_Deep_Blind_Video_Super-Resolution_ICCV_2021_paper.pdf)
* VSR
  * [Omniscient Video Super-Resolution](https://arxiv.org/abs/2103.15683)<br>:star:[code](https://github.com/psychopa4/OVSR)
  * [COMISR: Compression-Informed Video Super-Resolution](https://arxiv.org/abs/2105.01237)<br>:star:[code](https://github.com/google-research/google-research/tree/master/comisr)

<a name="33"/>

## 33.Remote Sensing Images(遥感影像)
* [SUNet: Symmetric Undistortion Network for Rolling Shutter Correction](https://arxiv.org/abs/2108.04775)
* [Change is Everywhere: Single-Temporal Supervised Object Change Detection in Remote Sensing Imagery](https://arxiv.org/abs/2108.07002)<br>:star:[code](https://github.com/Z-Zheng/ChangeStar)
* 卫星图像的全景分割
  * [Panoptic Segmentation of Satellite Image Time Series With Convolutional Temporal Attention Networks](https://arxiv.org/abs/2107.07933)<br>:star:[code](https://github.com/VSainteuf/utae-paps):sunflower:[PASTIS dataset](https://github.com/VSainteuf/pastis-benchmark)
* 基于卫星影像的交通事故检测
  * [Inferring High-Resolution Traffic Accident Risk Maps Based on Satellite Imagery and GPS Trajectories](https://openaccess.thecvf.com/content/ICCV2021/papers/He_Inferring_High-Resolution_Traffic_Accident_Risk_Maps_Based_on_Satellite_Imagery_ICCV_2021_paper.pdf)
* 遥感数据
  * [Seasonal Contrast: Unsupervised Pre-Training From Uncurated Remote Sensing Data](https://openaccess.thecvf.com/content/ICCV2021/papers/Manas_Seasonal_Contrast_Unsupervised_Pre-Training_From_Uncurated_Remote_Sensing_Data_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/ElementAI/seasonal-contrast)
  * [Dynamic Cross Feature Fusion for Remote Sensing Pansharpening](https://openaccess.thecvf.com/content/ICCV2021/papers/Wu_Dynamic_Cross_Feature_Fusion_for_Remote_Sensing_Pansharpening_ICCV_2021_paper.pdf)

<a name="32"/>

## 32.语音
* [The Right to Talk: An Audio-Visual Transformer Approach](https://arxiv.org/abs/2108.03256)<br>:star:[code](https://github.com/uark-cviu/Right2Talk)
* [Move2Hear: Active Audio-Visual Source Separation](https://openaccess.thecvf.com/content/ICCV2021/papers/Majumder_Move2Hear_Active_Audio-Visual_Source_Separation_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/SAGNIKMJR/move2hear-active-AV-separation):house:[project](http://vision.cs.utexas.edu/projects/move2hear/)
* 音频分离
  * [Visual Scene Graphs for Audio Source Separation](https://arxiv.org/abs/2109.11955)
* 音频-手势
  * [Audio2Gestures: Generating Diverse Gestures from Speech Audio with Conditional Variational Autoencoders](https://arxiv.org/abs/2108.06720)<br>:house:[project](https://openaccess.thecvf.com/content/ICCV2021/papers/Xiang_SnowflakeNet_Point_Cloud_Completion_by_Snowflake_Point_Deconvolution_With_Skip-Transformer_ICCV_2021_paper.pdf)
* Active Speaker Detection(ASD主动式扬声器检测)
  * [How To Design a Three-Stage Architecture for Audio-Visual Active Speaker Detection in the Wild](https://arxiv.org/abs/2106.03932)<br>:star:[code](https://github.com/okankop/ASDNet)
  * [MAAS: Multi-Modal Assignation for Active Speaker Detection](https://openaccess.thecvf.com/content/ICCV2021/papers/Alcazar_MAAS_Multi-Modal_Assignation_for_Active_Speaker_Detection_ICCV_2021_paper.pdf)
* 从人脸视频中重新收集音频
  * [Multi-Modality Associative Bridging Through Memory: Speech Sound Recollected From Face Video](https://openaccess.thecvf.com/content/ICCV2021/papers/Kim_Multi-Modality_Associative_Bridging_Through_Memory_Speech_Sound_Recollected_From_Face_ICCV_2021_paper.pdf)

<a name="31"/>

## 31.Style Transfer(风格迁移)
* [AdaAttN: Revisit Attention Mechanism in Arbitrary Neural Style Transfer](https://arxiv.org/abs/2108.03647)<br>:star:[code](https://github.com/Huage001/AdaAttN)
* [Domain-Aware Universal Style Transfer](https://arxiv.org/abs/2108.04441)<br>:star:[code](https://github.com/Kibeom-Hong/Domain-Aware-Style-Transfer)
* [Diverse Image Style Transfer via Invertible Cross-Space Mapping](https://openaccess.thecvf.com/content/ICCV2021/papers/Chen_Diverse_Image_Style_Transfer_via_Invertible_Cross-Space_Mapping_ICCV_2021_paper.pdf)
* [StyleFormer: Real-Time Arbitrary Style Transfer via Parametric Style Composition](https://openaccess.thecvf.com/content/ICCV2021/papers/Wu_StyleFormer_Real-Time_Arbitrary_Style_Transfer_via_Parametric_Style_Composition_ICCV_2021_paper.pdf)

<a name="30"/>

## 30.Image Generation/synthesis(图像生成/合成)
* [ILVR: Conditioning Method for Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2108.02938)<br>:open_mouth:oral
* [Image Synthesis via Semantic Composition](https://arxiv.org/abs/2109.07053)<br>:star:[code](https://github.com/dvlab-research/SCGAN):house:[project](https://shepnerd.github.io/scg/)
* [Image Synthesis From Layout With Locality-Aware Mask Adaption](https://openaccess.thecvf.com/content/ICCV2021/papers/Li_Image_Synthesis_From_Layout_With_Locality-Aware_Mask_Adaption_ICCV_2021_paper.pdf)
* 图像融合
  * [DTMNet: A Discrete Tchebichef Moments-Based Deep Neural Network for Multi-Focus Image Fusion](https://openaccess.thecvf.com/content/ICCV2021/papers/Xiao_DTMNet_A_Discrete_Tchebichef_Moments-Based_Deep_Neural_Network_for_Multi-Focus_ICCV_2021_paper.pdf)

<a name="29"/>

## 29.Image Retrieval(图像检索)
* [DOLG: Single-Stage Image Retrieval with Deep Orthogonal Fusion of Local and Global Features](https://arxiv.org/abs/2108.02927)<br>:star:[code](https://github.com/feymanpriv/DOLG)
* [Image Retrieval on Real-life Images with Pre-trained Vision-and-Language Models](https://arxiv.org/abs/2108.04024)<br>:star:[code](https://github.com/Cuberick-Orion/CIRR):house:[project](https://cuberick-orion.github.io/CIRR/)
* [Self-supervised Product Quantization for Deep Unsupervised Image Retrieval](https://arxiv.org/abs/2109.02244)<br>:star:[code](https://github.com/youngkyunJang/SPQ)
* [Instance-Level Image Retrieval Using Reranking Transformers](https://arxiv.org/abs/2103.12236)<br>:star:[code](https://github.com/uvavision/RerankingTransformer)
* [Learning Attribute-Driven Disentangled Representations for Interactive Fashion Retrieval](https://openaccess.thecvf.com/content/ICCV2021/papers/Hou_Learning_Attribute-Driven_Disentangled_Representations_for_Interactive_Fashion_Retrieval_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/amzn/fashion-attribute-disentanglement)
* [Telling the What While Pointing to the Where: Multimodal Queries for Image Retrieval](https://openaccess.thecvf.com/content/ICCV2021/papers/Changpinyo_Telling_the_What_While_Pointing_to_the_Where_Multimodal_Queries_ICCV_2021_paper.pdf)
* 跨域检索
  * [Universal Cross-Domain Retrieval: Generalizing Across Classes and Domains](https://arxiv.org/abs/2108.08356)
* Visual Geolocalization
  * [Viewpoint Invariant Dense Matching for Visual Geolocalization](https://arxiv.org/abs/2109.09827)<br>:star:[code](https://github.com/gmberton/geo_warp)
* 跨模态检索
  * [Ask&Confirm: Active Detail Enriching for Cross-Modal Retrieval With Partial Query](https://openaccess.thecvf.com/content/ICCV2021/papers/Cai_AskConfirm_Active_Detail_Enriching_for_Cross-Modal_Retrieval_With_Partial_Query_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/CuthbertCai/Ask-Confirm)
  * [Product1M: Towards Weakly Supervised Instance-Level Product Retrieval via Cross-modal Pretraining](https://arxiv.org/abs/2107.14572)<br>:star:[code](https://github.com/zhanxlin/Product1M)
* 文本-视频检索
  * [TEACHTEXT: CrossModal Generalized Distillation for Text-Video Retrieval](https://arxiv.org/abs/2104.08271)<br>:house:[project](https://www.robots.ox.ac.uk/~vgg/research/teachtext/)
* image-based 3D shape retrieval 
  * [Single Image 3D Shape Retrieval via Cross-Modal Instance and Category Contrastive Learning](https://openaccess.thecvf.com/content/ICCV2021/papers/Lin_Single_Image_3D_Shape_Retrieval_via_Cross-Modal_Instance_and_Category_ICCV_2021_paper.pdf)


<a name="28"/>

## 28.Contrastive Learning(对比学习)
* [Improving Contrastive Learning by Visualizing Feature Transformation](https://arxiv.org/abs/2108.02982)<br>:open_mouth:oral:star:[code](https://github.com/DTennant/CL-Visualizing-Feature-Transformation)
* [TACo: Token-aware Cascade Contrastive Learning for Video-Text Alignment](https://arxiv.org/abs/2108.09980)<br>:newspaper:解读:[ICCV2021-TOCo-微软&CMU提出Token感知的级联对比学习方法，在视频文本对齐任务上“吊打”其他SOTA方法](https://mp.weixin.qq.com/s/sNwvYL1qsgyVrRe3-QmzhA)
* [A Broad Study on the Transferability of Visual Representations With Contrastive Learning](https://arxiv.org/abs/2103.13517)<br>:star:[code](https://github.com/asrafulashiq/transfer_broad)
* [Vi2CLR: Video and Image for Visual Contrastive Learning of Representation](https://openaccess.thecvf.com/content/ICCV2021/papers/Diba_Vi2CLR_Video_and_Image_for_Visual_Contrastive_Learning_of_Representation_ICCV_2021_paper.pdf)
* [LatentCLR: A Contrastive Learning Approach for Unsupervised Discovery of Interpretable Directions](https://arxiv.org/abs/2104.00820)<br>:star:[code](https://github.com/catlab-team/latentclr)
* [CrossCLR: Cross-Modal Contrastive Learning for Multi-Modal Video Representations](https://arxiv.org/abs/2109.14910)
* [Social NCE: Contrastive Learning of Socially-Aware Motion Representations](https://arxiv.org/abs/2012.11717)(https://github.com/vita-epfl/social-nce):tv:[video](https://www.youtube.com/watch?v=s1khZWWiQfA)
* [With a Little Help From My Friends: Nearest-Neighbor Contrastive Learning of Visual Representations](https://arxiv.org/abs/2104.14548)
* [Contrastive Learning of Image Representations With Cross-Video Cycle-Consistency](https://arxiv.org/abs/2105.06463)<br>:house:[project](https://happywu.github.io/cycle_contrast_video/)

<a name="27"/>

## 27.Multi-label image recognition(多标签图像识别)
* [Residual Attention: A Simple but Effective Method for Multi-Label Recognition](https://arxiv.org/abs/2108.02456)
* [Transformer-based Dual Relation Graph for Multi-label Image Recognition](https://arxiv.org/abs/2110.04722)

<a name="26"/>

## 26.Image Processing(图像处理)
* 图像形状操纵
  * [Image Shape Manipulation from a Single Augmented Training Sample](https://arxiv.org/abs/2109.06151)<br>:open_mouth:oral:star:[code](https://github.com/eliahuhorwitz/DeepSIM):house:[project](http://www.vision.huji.ac.il/deepsim/)
* 边缘检测
  * [RINDNet: Edge Detection for Discontinuity in Reflectance, Illumination, Normal and Depth](https://arxiv.org/abs/2108.00616)<br>:open_mouth:oral:star:[code](https://github.com/MengyangPu/RINDNet)
  * [Pixel Difference Networks for Efficient Edge Detection](https://arxiv.org/abs/2108.07009)<br>:star:[code](https://github.com/zhuoinoulu/pidinet)
* 图像识别
  * [MicroNet: Improving Image Recognition with Extremely Low FLOPs](https://arxiv.org/abs/2108.05894)<br>:star:[code](https://github.com/liyunsheng13/micronet)
* 图像去模糊
  * [Rethinking Coarse-to-Fine Approach in Single Image Deblurring](https://arxiv.org/abs/2108.05054)<br>:star:[code](https://github.com/chosj95/MIMO-UNet)
  * [Single Image Defocus Deblurring Using Kernel-Sharing Parallel Atrous Convolutions](https://arxiv.org/abs/2108.09108)
  * [Defocus Map Estimation and Deblurring From a Single Dual-Pixel Image](https://openaccess.thecvf.com/content/ICCV2021/papers/Xin_Defocus_Map_Estimation_and_Deblurring_From_a_Single_Dual-Pixel_Image_ICCV_2021_paper.pdf)
  * [Motion Deblurring with Real Events](https://arxiv.org/abs/2109.13695)
* Image quality assessment(图像质量评估IQA)
  * [MUSIQ: Multi-scale Image Quality Transformer](https://arxiv.org/abs/2108.05997)<br>:star:[code](https://github.com/google-research/google-research/tree/master/musiq)
* Image Harmonization
  * [SSH: A Self-Supervised Framework for Image Harmonization](https://arxiv.org/abs/2108.06805)<br>:star:[code](https://github.com/VITA-Group/SSHarmonization)
  * [Learning Conditional Knowledge Distillation for Degraded-Reference Image Quality Assessment](https://arxiv.org/abs/2108.07948)<br>:star:[code](https://github.com/researchmm/CKDN)
* 去阴影
  * [CANet: A Context-Aware Network for Shadow Removal](https://arxiv.org/abs/2108.09894)
* 去噪
  * [Rethinking Deep Image Prior for Denoising](https://arxiv.org/abs/2108.12841)<br>:star:[code](https://github.com/gistvision/DIP-denosing)
  * [Rethinking Noise Synthesis and Modeling in Raw Denoising](https://arxiv.org/abs/2110.04756)<br>:star:[code](https://github.com/zhangyi-3/noise-synthesis)
  * [C2N: Practical Generative Noise Modeling for Real-World Denoising](https://openaccess.thecvf.com/content/ICCV2021/papers/Jang_C2N_Practical_Generative_Noise_Modeling_for_Real-World_Denoising_ICCV_2021_paper.pdf)
  * [The Benefit of Distraction: Denoising Camera-Based Physiological Measurements Using Inverse Attention](https://openaccess.thecvf.com/content/ICCV2021/papers/Nowara_The_Benefit_of_Distraction_Denoising_Camera-Based_Physiological_Measurements_Using_Inverse_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/ewanowara/benefitofdistraction)
  * [Hyperspectral Image Denoising with Realistic Data](https://openaccess.thecvf.com/content/ICCV2021/papers/Zhang_Hyperspectral_Image_Denoising_With_Realistic_Data_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/ColinTaoZhang/HSIDwRD)
  * [End-to-End Unsupervised Document Image Blind Denoising](https://openaccess.thecvf.com/content/ICCV2021/papers/Gangeh_End-to-End_Unsupervised_Document_Image_Blind_Denoising_ICCV_2021_paper.pdf)
  * 视频去噪
    * [Patch Craft: Video Denoising by Deep Modeling and Patch Matching](http://arxiv.org/abs/2103.13767)
* 图像着色
  * [Towards Vivid and Diverse Image Colorization with Generative Color Prior](https://arxiv.org/abs/2108.08826)
* 图像增强
  * [Real-time Image Enhancer via Learnable Spatial-aware 3D Lookup Tables](https://arxiv.org/abs/2108.08697)
  * [Adaptive Unfolding Total Variation Network for Low-Light Image Enhancement](https://arxiv.org/abs/2110.00984)<br>:star:[code](https://github.com/CharlieZCJ/UTVNet)
  * [Representative Color Transform for Image Enhancement](https://openaccess.thecvf.com/content/ICCV2021/papers/Kim_Representative_Color_Transform_for_Image_Enhancement_ICCV_2021_paper.pdf)
* 图像恢复
  * [Spatially-Adaptive Image Restoration using Distortion-Guided Networks](https://arxiv.org/abs/2108.08617)<br>:star:[code](https://github.com/human-analysis/spatially-adaptive-image-restoration)
  * [Dynamic Attentive Graph Learning for Image Restoration](https://arxiv.org/abs/2109.06620)<br>:star:[code](https://github.com/jianzhangcs/DAGL)
  * [Self-Supervised Cryo-Electron Tomography Volumetric Image Restoration From Single Noisy Volume With Sparsity Constraint](https://openaccess.thecvf.com/content/ICCV2021/papers/Yang_Self-Supervised_Cryo-Electron_Tomography_Volumetric_Image_Restoration_From_Single_Noisy_Volume_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/icthrm/SC-Net)
* 图像压缩
  * [Variable-Rate Deep Image Compression through Spatially-Adaptive Feature Transform](https://arxiv.org/abs/2108.09551)<br>:star:[code](https://github.com/micmic123/QmapCompression)
* 图像修复
  * [Image Inpainting via Conditional Texture and Structure Dual Generation](https://arxiv.org/abs/2108.09760)<br>:star:[code](https://github.com/Xiefan-Guo/CTSDG)
 * Image extrapolation
  * [SemIE: Semantically-aware Image Extrapolation](https://arxiv.org/abs/2108.13702)<br>:house:[project](https://semie-iccv.github.io/)
 * Reversible Image Conversion
  * [IICNet: A Generic Framework for Reversible Image Conversion](https://arxiv.org/abs/2109.04242)<br>:star:[code](https://github.com/felixcheng97/IICNet)
  * [CR-Fill: Generative Image Inpainting With Auxiliary Contextual Reconstruction](https://openaccess.thecvf.com/content/ICCV2021/papers/Zeng_CR-Fill_Generative_Image_Inpainting_With_Auxiliary_Contextual_Reconstruction_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/zengxianyu/crfill)
* 伪影去除
  * [Towards Flexible Blind JPEG Artifacts Removal](https://arxiv.org/abs/2109.14573)<br>:star:[code](https://github.com/jiaxi-jiang/FBCNN)
  * [Learning Dual Priors for JPEG Compression Artifacts Removal](https://openaccess.thecvf.com/content/ICCV2021/papers/Fu_Learning_Dual_Priors_for_JPEG_Compression_Artifacts_Removal_ICCV_2021_paper.pdf)
  * [Let's See Clearly: Contaminant Artifact Removal for Moving Cameras](https://openaccess.thecvf.com/content/ICCV2021/papers/Li_Lets_See_Clearly_Contaminant_Artifact_Removal_for_Moving_Cameras_ICCV_2021_paper.pdf)
* De-rendering
  * [De-rendering Stylized Texts](https://arxiv.org/abs/2110.01890)<br>:star:[code](https://github.com/CyberAgentAILab/derendering-text):house:[project](https://cyberagentailab.github.io/derendering-text/)
* 去除光晕
  * [Light Source Guided Single-Image Flare Removal From Unpaired Data](https://openaccess.thecvf.com/content/ICCV2021/papers/Qiao_Light_Source_Guided_Single-Image_Flare_Removal_From_Unpaired_Data_ICCV_2021_paper.pdf)
* 全景图拼接
  * [Minimal Solutions for Panoramic Stitching Given Gravity Prior](https://arxiv.org/abs/2012.00465)
* Flare Removal
  * [How to Train Neural Networks for Flare Removal](https://arxiv.org/abs/2011.12485)<br>:house:[project](https://yichengwu.github.io/flare-removal/):tv:[video](https://www.youtube.com/watch?v=eAXhcDjWoZ0)
* 图像裁剪
  * [TransView: Inside, Outside, and Across the Cropping View Boundaries](https://openaccess.thecvf.com/content/ICCV2021/papers/Pan_TransView_Inside_Outside_and_Across_the_Cropping_View_Boundaries_ICCV_2021_paper.pdf)
* 去反射
  * [Location-Aware Single Image Reflection Removal](https://arxiv.org/abs/2012.07131)<br>:star:[code](https://github.com/zdlarr/Location-aware-SIRR)
* 去雨
  * [Improving De-Raining Generalization via Neural Reorganization](https://openaccess.thecvf.com/content/ICCV2021/papers/Xiao_Improving_De-Raining_Generalization_via_Neural_Reorganization_ICCV_2021_paper.pdf)
* 图像失真去除
  * [Unsupervised Non-Rigid Image Distortion Removal via Grid Deformation](https://openaccess.thecvf.com/content/ICCV2021/papers/Li_Unsupervised_Non-Rigid_Image_Distortion_Removal_via_Grid_Deformation_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/Nianyi-Li/unsupervised-NDIR):tv:[video](https://www.youtube.com/watch?v=aeJkb5u0Cb8)
* 消除水下图像的折射失真
  * [Learning To Remove Refractive Distortions From Underwater Images](https://openaccess.thecvf.com/content/ICCV2021/papers/Thapa_Learning_To_Remove_Refractive_Distortions_From_Underwater_Images_ICCV_2021_paper.pdf)
* 图像补全
  * [High-Fidelity Pluralistic Image Completion With Transformers](https://arxiv.org/abs/2103.14031)<br>:star:[code](https://github.com/raywzy/ICT):house:[project](http://raywzy.com/ICT/)

<a name="25"/>

## 25.Medical Image(医学影像)
* 医学图像分割
  * [Recurrent Mask Refinement for Few-Shot Medical Image Segmentation](https://arxiv.org/abs/2108.00622)<br>:star:[code](https://github.com/uci-cbcl/RP-Net)
* 病理学图像表示
  * [A QuadTree Image Representation for Computational Pathology](https://arxiv.org/abs/2108.10873)
* 医学图像分析
  * [Preservational Learning Improves Self-supervised Medical Image Models by Reconstructing Diverse Contexts](https://arxiv.org/abs/2109.04379)<br>:star:[code](https://github.com/Luchixiang/PCRL)
* 医学图像去噪
  * [Eformer: Edge Enhancement based Transformer for Medical Image Denoising](https://arxiv.org/abs/2109.08044)
* 视频翻译
  * [Long-Term Temporally Consistent Unpaired Video Translation From Simulated Surgical 3D Data](https://arxiv.org/abs/2103.17204)<br>:star:[code](https://gitlab.com/nct_tso_public/surgical-video-sim2real):house:[project](http://opencas.dkfz.de/video-sim2real/)
* 病理学图像核检测分割
  * [Mutual-Complementing Framework for Nuclei Detection and Segmentation in Pathology Image](https://openaccess.thecvf.com/content/ICCV2021/papers/Feng_Mutual-Complementing_Framework_for_Nuclei_Detection_and_Segmentation_in_Pathology_Image_ICCV_2021_paper.pdf)
* 医学报告生成
  * [Visual-Textual Attentive Semantic Consistency for Medical Report Generation](https://openaccess.thecvf.com/content/ICCV2021/papers/Zhou_Visual-Textual_Attentive_Semantic_Consistency_for_Medical_Report_Generation_ICCV_2021_paper.pdf)
* 息肉分割
  * [Collaborative and Adversarial Learning of Focused and Dispersive Representations for Semi-Supervised Polyp Segmentation](https://openaccess.thecvf.com/content/ICCV2021/papers/Wu_Collaborative_and_Adversarial_Learning_of_Focused_and_Dispersive_Representations_for_ICCV_2021_paper.pdf)
* CT重建
  * [Dynamic CT Reconstruction From Limited Views With Implicit Neural Representations and Parametric Motion Fields](https://arxiv.org/abs/2104.11745)<br>:star:[code](https://github.com/awreed/DynamicCTReconstruction)
* CT
  * [3DeepCT: Learning Volumetric Scattering Tomography of Clouds](https://openaccess.thecvf.com/content/ICCV2021/papers/Sde-Chen_3DeepCT_Learning_Volumetric_Scattering_Tomography_of_Clouds_ICCV_2021_paper.pdf)
* 医学图像识别
  * [GLoRIA: A Multimodal Global-Local Representation Learning Framework for Label-Efficient Medical Image Recognition](https://openaccess.thecvf.com/content/ICCV2021/papers/Huang_GLoRIA_A_Multimodal_Global-Local_Representation_Learning_Framework_for_Label-Efficient_Medical_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/marshuang80/gloria)
* 血管分割
  * [Self-Supervised Vessel Segmentation via Adversarial Learning](https://github.com/AISIGSJTU/SSVS)

<a name="24"/>

## 24.Face(人脸)
* [VariTex: Variational Neural Face Textures](https://arxiv.org/abs/2104.05988)<br>:star:[code](https://github.com/mcbuehler/VariTex):house:[project](https://mcbuehler.github.io/VariTex/):tv:[video](https://www.youtube.com/watch?v=6-GFHcLkbik)
* 人脸造假检测
  * [OpenForensics: Large-Scale Challenging Dataset For Multi-Face Forgery Detection And Segmentation In-The-Wild](https://arxiv.org/abs/2107.14480)<br>:house:[project](https://sites.google.com/view/ltnghia/research/openforensics)
  * [Exploring Temporal Coherence for More General Video Face Forgery Detection](https://arxiv.org/abs/2108.06693)
* 人脸合成
  * [Disentangled Lifespan Face Synthesis](https://arxiv.org/abs/2108.02874)<br>:star:[code](https://github.com/SenHe/DLFS):house:[project](https://senhe.github.io/projects/iccv_2021_lifespan_face/):tv:[video](https://youtu.be/uklX03ns0m0)
* 人脸识别                                                                                      
  * [PASS: Protected Attribute Suppression System for Mitigating Bias in Face Recognition](https://arxiv.org/abs/2108.03764)
  * [SynFace: Face Recognition with Synthetic Data](https://arxiv.org/abs/2108.07960)
  * [Adaptive Label Noise Cleaning With Meta-Supervision for Deep Face Recognition](https://openaccess.thecvf.com/content/ICCV2021/papers/Zhang_Adaptive_Label_Noise_Cleaning_With_Meta-Supervision_for_Deep_Face_Recognition_ICCV_2021_paper.pdf)
  * [Disentangled Representation for Age-Invariant Face Recognition: A Mutual Information Minimization Perspective](https://openaccess.thecvf.com/content/ICCV2021/papers/Hou_Disentangled_Representation_for_Age-Invariant_Face_Recognition_A_Mutual_Information_Minimization_ICCV_2021_paper.pdf)
  * [Teacher-Student Adversarial Depth Hallucination To Improve Face Recognition](https://openaccess.thecvf.com/content/ICCV2021/papers/Uppal_Teacher-Student_Adversarial_Depth_Hallucination_To_Improve_Face_Recognition_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/hardik-uppal/teacher-student-gan)
  * “去”识别
    * [Personalized and Invertible Face De-Identification by Disentangled Identity Information Manipulation](https://openaccess.thecvf.com/content/ICCV2021/papers/Cao_Personalized_and_Invertible_Face_De-Identification_by_Disentangled_Identity_Information_Manipulation_ICCV_2021_paper.pdf)
* Face perception面部感知
 * [Learning Facial Representations from the Cycle-consistency of Face](https://arxiv.org/abs/2108.03427)<br>:star:[code](https://github.com/JiaRenChang/FaceCycle)
* 说话人脸生成
  * [FACIAL: Synthesizing Dynamic Talking Face with Implicit Attribute Learning](https://arxiv.org/abs/2108.07938)
* 说话头合成
  * [AD-NeRF: Audio Driven Neural Radiance Fields for Talking Head Synthesis](https://openaccess.thecvf.com/content/ICCV2021/papers/Guo_AD-NeRF_Audio_Driven_Neural_Radiance_Fields_for_Talking_Head_Synthesis_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/YudongGuo/AD-NeRF)
* 人脸表情识别
  * [Understanding and Mitigating Annotation Bias in Facial Expression Recognition](https://arxiv.org/abs/2108.08504)
  * [TransFER: Learning Relation-aware Facial Expression Representations with Transformers](https://arxiv.org/abs/2108.11116)
* 人脸呈现攻击检测
  * [Detection and Continual Learning of Novel Face Presentation Attacks](https://arxiv.org/abs/2108.12081)<br>:star:[code](https://github.com/mrostami1366)
* 人脸编辑
  * [Talk-to-Edit: Fine-Grained Facial Editing via Dialog](https://arxiv.org/abs/2109.04425)<br>:star:[code](https://github.com/yumingj/Talk-to-Edit):house:[project](https://www.mmlab-ntu.com/project/talkedit/)<br>:newspaper:解读:[ICCV2021 | 南洋理工大学、港中大提出Talk-to-Edit，对话实现高细粒度人脸编辑](https://mp.weixin.qq.com/s/48FsUqsppXaXUu-QMUIhCQ)
  * [A Latent Transformer for Disentangled Face Editing in Images and Videos](https://arxiv.org/abs/2106.11895)<br>:star:[code](https://github.com/InterDigitalInc/latent-transformer)
* 人脸对齐
  * [ADNet: Leveraging Error-Bias Towards Normal Direction in Face Alignment](https://arxiv.org/abs/2109.05721) 
* 人脸图像重建
  * [Focal Frequency Loss for Image Reconstruction and Synthesis](https://arxiv.org/abs/2012.12821)<br>:star:[code](https://github.com/EndlessSora/focal-frequency-loss):house:[project](https://www.mmlab-ntu.com/project/ffl/index.html):tv:[video](https://www.youtube.com/watch?v=RNTnDtKvcpc)
* 3D人脸重建
  * [Topologically Consistent Multi-View Face Inference Using Volumetric Sampling](https://arxiv.org/abs/2110.02948)<br>:star:[code](https://tianyeli.github.io/tofu) 
  * [Self-Supervised 3D Face Reconstruction via Conditional Estimation](https://arxiv.org/abs/2110.04800)
* 三维人脸动画
  * [MeshTalk: 3D Face Animation From Speech Using Cross-Modality Disentanglement](https://openaccess.thecvf.com/content/ICCV2021/papers/Richard_MeshTalk_3D_Face_Animation_From_Speech_Using_Cross-Modality_Disentanglement_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/facebookresearch/meshtalk):tv:[video](https://research.fb.com/wp-content/uploads/2021/04/mesh_talk.mp4)
* Remote Photoplethysmography (rPPG远程光电容积描记术)
  * [The Way to My Heart Is Through Contrastive Learning: Remote Photoplethysmography From Unlabelled Video](https://openaccess.thecvf.com/content/ICCV2021/papers/Gideon_The_Way_to_My_Heart_Is_Through_Contrastive_Learning_Remote_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/ToyotaResearchInstitute/RemotePPG)
* 人脸加密
  * [Towards Face Encryption by Generating Adversarial Identity Masks](https://arxiv.org/abs/2003.06814)<br>:star:[code](https://github.com/ShawnXYang/TIP-IM)
* Deepfake检测
  * [Learning Self-Consistency for Deepfake Detection](https://arxiv.org/abs/2012.09311)<br>:open_mouth:oral
<a name="23"/>

## 23.Gaze Estimation(视线估计)
* [Generalizing Gaze Estimation with Outlier-guided Collaborative Adaptation](https://arxiv.org/abs/2107.13780)<br>:star:[code](https://github.com/DreamtaleCore/PnP-GA)
* 视线跟踪
  * [Looking Here or There? Gaze Following in 360-Degree Images](https://openaccess.thecvf.com/content/ICCV2021/papers/Li_Looking_Here_or_There_Gaze_Following_in_360-Degree_Images_ICCV_2021_paper.pdf)

<a name="22"/>

## 22.GAN
* [Sketch Your Own GAN](https://arxiv.org/abs/2108.02774)<br>:star:[code](https://github.com/peterwang512/GANSketching):house:[project](https://peterwang512.github.io/GANSketching/)
* [Online Multi-Granularity Distillation for GAN Compression](https://arxiv.org/abs/2108.06908)<br>:star:[code](https://github.com/bytedance/OMGD)
* [Dual Projection Generative Adversarial Networks for Conditional Image Generation](https://arxiv.org/abs/2108.09016)<br>:star:[code](https://github.com/phymhan/P2GAN)
* [InSeGAN: A Generative Approach to Segmenting Identical Instances in Depth Images](https://arxiv.org/abs/2108.13865)
* [ReStyle: A Residual-Based StyleGAN Encoder via Iterative Refinement](https://arxiv.org/abs/2104.02699)<br>:star:[code](https://github.com/yuval-alaluf/restyle-encoder):house:[project](https://yuval-alaluf.github.io/restyle-encoder/):tv:[video](https://youtu.be/6pGzLECSIWM)
* [WarpedGANSpace: Finding non-linear RBF paths in GAN latent space](https://arxiv.org/abs/2109.13357)<br>:star:[code](https://github.com/chi0tzp/WarpedGANSpace)
* [Toward a Visual Concept Vocabulary for GAN Latent Space](https://arxiv.org/abs/2110.04292)
* [Collaging Class-specific GANs for Semantic Image Synthesis](https://arxiv.org/abs/2110.04281)<br>:house:[project](https://yuheng-li.github.io/CollageGAN/)
* [Latent Transformations via NeuralODEs for GAN-Based Image Editing](https://openaccess.thecvf.com/content/ICCV2021/papers/Khrulkov_Latent_Transformations_via_NeuralODEs_for_GAN-Based_Image_Editing_ICCV_2021_paper.pdf)
* [Reality Transform Adversarial Generators for Image Splicing Forgery Detection and Localization](https://openaccess.thecvf.com/content/ICCV2021/papers/Bi_Reality_Transform_Adversarial_Generators_for_Image_Splicing_Forgery_Detection_and_ICCV_2021_paper.pdf)
* [GAN-Control: Explicitly Controllable GANs](https://openaccess.thecvf.com/content/ICCV2021/papers/Shoshan_GAN-Control_Explicitly_Controllable_GANs_ICCV_2021_paper.pdf)(https://alonshoshan10.github.io/gan_control/)<br>:house:[project](https://alonshoshan10.github.io/gan_control/)
* [Omni-GAN: On the Secrets of cGANs and Beyond](https://openaccess.thecvf.com/content/ICCV2021/papers/Zhou_Omni-GAN_On_the_Secrets_of_cGANs_and_Beyond_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/PeterouZh/Omni-GAN-PyTorch)
* [Unsupervised Image Generation with Infinite Generative Adversarial Networks](https://arxiv.org/abs/2108.07975)<br>:star:[code](https://github.com/yinghdb/MICGANs)
* [DAE-GAN: Dynamic Aspect-Aware GAN for Text-to-Image Synthesis](https://openaccess.thecvf.com/content/ICCV2021/papers/Ruan_DAE-GAN_Dynamic_Aspect-Aware_GAN_for_Text-to-Image_Synthesis_ICCV_2021_paper.pdf)
* [Detail Me More: Improving GAN’s photo-realism of complex scenes](https://openaccess.thecvf.com/content/ICCV2021/papers/Gadde_Detail_Me_More_Improving_GANs_Photo-Realism_of_Complex_Scenes_ICCV_2021_paper.pdf)
* [Unsupervised Segmentation Incorporating Shape Prior via Generative Adversarial Networks](https://openaccess.thecvf.com/content/ICCV2021/papers/Kim_Unsupervised_Segmentation_Incorporating_Shape_Prior_via_Generative_Adversarial_Networks_ICCV_2021_paper.pdf)
* [DRB-GAN: A Dynamic ResBlock Generative Adversarial Network for Artistic Style Transfer](https://openaccess.thecvf.com/content/ICCV2021/papers/Xu_DRB-GAN_A_Dynamic_ResBlock_Generative_Adversarial_Network_for_Artistic_Style_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/xuwenju123/DRB-GAN)
* [Dual Contrastive Loss and Attention for GANs](https://arxiv.org/abs/2103.16748)
* [Semi-Supervised Single-Stage Controllable GANs for Conditional Fine-Grained Image Generation](https://openaccess.thecvf.com/content/ICCV2021/papers/Chen_Semi-Supervised_Single-Stage_Controllable_GANs_for_Conditional_Fine-Grained_Image_Generation_ICCV_2021_paper.pdf)
* [Gradient Normalization for Generative Adversarial Networks](https://arxiv.org/abs/2109.02235)<br>:star:[code](https://github.com/basiclab/GNGAN-PyTorch)
* [EigenGAN: Layer-Wise Eigen-Learning for GANs](https://arxiv.org/abs/2104.12476)<br>:star:[code](https://github.com/LynnHo/EigenGAN-Tensorflow)
* [Retrieve in Style: Unsupervised Facial Feature Transfer and Retrieval](https://arxiv.org/abs/2107.06256)<br>:star:[code](https://github.com/mchong6/RetrieveInStyle)
* GAN inversion(GAN逆映射)
  * [From Continuity to Editability: Inverting GANs with Consecutive Images](https://arxiv.org/abs/2107.13812)<br>:star:[code](https://github.com/cnnlstm/InvertingGANs_with_ConsecutiveImgs)
  * [GAN Inversion for Out-of-Range Images with Geometric Transformations](https://arxiv.org/abs/2108.08998)<br>:house:[project](https://kkang831.github.io/publication/ICCV_2021_BDInvert/)
* 图像到图像翻译
  * [Unaligned Image-to-Image Translation by Learning to Reweight](https://arxiv.org/abs/2109.11736)<br>:star:[code](https://github.com/Mid-Push/IrwGAN)
  * [Bridging the Gap between Label- and Reference-based Synthesis in Multi-attribute Image-to-Image Translation](https://arxiv.org/abs/2110.05055)<br>:star:[code](https://github.com/huangqiusheng/BridgeGAN)
  * [Instance-Wise Hard Negative Example Generation for Contrastive Learning in Unpaired Image-to-Image Translation](https://arxiv.org/abs/2108.04547)
* Image translation
  * [Scaling-up Disentanglement for Image Translation](https://arxiv.org/abs/2103.14017)<br>:star:[code](https://github.com/avivga/overlord):house:[project](http://www.vision.huji.ac.il/overlord/)
  * [Harnessing the Conditioning Sensorium for Improved Image Translation](https://openaccess.thecvf.com/content/ICCV2021/papers/Nederhood_Harnessing_the_Conditioning_Sensorium_for_Improved_Image_Translation_ICCV_2021_paper.pdf)


<a name="21"/>

## 21.Active Learning(主动学习)
* [Semi-Supervised Active Learning with Temporal Output Discrepancy](https://arxiv.org/abs/2107.14153)<br>:star:[code](https://github.com/siyuhuang/TOD)
* [Influence Selection for Active Learning](https://arxiv.org/abs/2108.09331)
* [Active Domain Adaptation via Clustering Uncertainty-weighted Embeddings](https://arxiv.org/abs/2010.08666)<br>:star:[code](https://github.com/virajprabhu/CLUE)
* [Contrastive Coding for Active Learning under Class Distribution Mismatch](https://openaccess.thecvf.com/content/ICCV2021/papers/Du_Contrastive_Coding_for_Active_Learning_Under_Class_Distribution_Mismatch_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/RUC-DWBI-ML/CCAL)  

<a name="20"/>

## 20.Adversarial Learning(对抗学习)
* [Low Curvature Activations Reduce Overfitting in Adversarial Training](https://arxiv.org/abs/2102.07861)
* [Removing Adversarial Noise in Class Activation Feature Space](https://arxiv.org/abs/2104.09197)
* [Sample Efficient Detection and Classification of Adversarial Attacks via Self-Supervised Embeddings](https://arxiv.org/abs/2108.13797)
* [Invisible Backdoor Attack With Sample-Specific Triggers](https://arxiv.org/abs/2012.03816)<br>:star:[code](https://github.com/yuezunli/ISSBA)
* 对抗攻击
  * [Feature Importance-aware Transferable Adversarial Attacks](https://arxiv.org/abs/2107.14185)<br>:star:[code](https://github.com/hcguoO0/FIA)
  * [TkML-AP: Adversarial Attacks to Top-k Multi-Label Learning](https://arxiv.org/abs/2108.00146)<br>:star:[code](https://github.com/discovershu/TKML-AP)
  * [Meta Gradient Adversarial Attack](https://arxiv.org/abs/2108.04204)
  * [AGKD-BML: Defense Against Adversarial Attack by Attention Guided Knowledge Distillation and Bi-directional Metric Learning](https://arxiv.org/abs/2108.06017)<br>:star:[code](https://github.com/hongw579/AGKD-BML)
  * [Exploiting Multi-Object Relationships for Detecting Adversarial Attacks in Complex Scenes](https://arxiv.org/abs/2108.08421)
  * [AdvDrop: Adversarial Attack to DNNs by Dropping Information](https://arxiv.org/abs/2108.09034)<br>:star:[code](https://github.com/RjDuan/AdvDrop)
  * [Adversarial Attacks Are Reversible With Natural Supervision](https://arxiv.org/abs/2103.14222)
  * [Attack As the Best Defense: Nullifying Image-to-Image Translation GANs via Limit-Aware Adversarial Attack](https://openaccess.thecvf.com/content/ICCV2021/papers/Yeh_Attack_As_the_Best_Defense_Nullifying_Image-to-Image_Translation_GANs_via_ICCV_2021_paper.pdf)
  * [Learnable Boundary Guided Adversarial Training](https://arxiv.org/abs/2011.11164)<br>:star:[code](https://github.com/dvlab-research/LBGAT)
* 对抗样本
  * [Adversarial Example Detection Using Latent Neighborhood Graph](https://openaccess.thecvf.com/content/ICCV2021/papers/Abusnaina_Adversarial_Example_Detection_Using_Latent_Neighborhood_Graph_ICCV_2021_paper.pdf)
  * [On the Robustness of Vision Transformers to Adversarial Examples](https://arxiv.org/abs/2104.02610)
* 黑盒
  * [Black-Box Detection of Backdoor Attacks With Limited Information and Data](https://arxiv.org/abs/2103.13127)
  * [Aha! Adaptive History-Driven Attack for Decision-Based Black-Box Models](https://openaccess.thecvf.com/content/ICCV2021/papers/Li_Aha_Adaptive_History-Driven_Attack_for_Decision-Based_Black-Box_Models_ICCV_2021_paper.pdf)
  * [Data-Free Universal Adversarial Perturbation and Black-Box Attack](https://openaccess.thecvf.com/content/ICCV2021/papers/Zhang_Data-Free_Universal_Adversarial_Perturbation_and_Black-Box_Attack_ICCV_2021_paper.pdf)

<a name="19"/>

## 19.Self-Driving Vehicles(自动驾驶)
* [End-to-End Urban Driving by Imitating a Reinforcement Learning Coach](https://arxiv.org/abs/2108.08265)<br>:star:[code](https://github.com/zhejz/carla-roach)  
* [MultiSiam: Self-supervised Multi-instance Siamese Representation Learning for Autonomous Driving](https://arxiv.org/abs/2108.12178)<br>:star:[code](https://github.com/KaiChen1998/MultiSiam)
* [NEAT: Neural Attention Fields for End-to-End Autonomous Driving](https://arxiv.org/abs/2109.04456)<br>:star:[code](https://github.com/autonomousvision/neat)
* [Safety-aware Motion Prediction with Unseen Vehicles for Autonomous Driving](https://arxiv.org/abs/2109.01510)<br>:star:[code](https://github.com/xrenaa/Safety-Aware-Motion-Prediction)
* [Social-NCE: Contrastive Learning of Socially-aware Motion Representations](https://arxiv.org/abs/2012.11717)<br>:star:[code](https://github.com/vita-epfl/social-nce-crowdnav):tv:[video](https://www.youtube.com/watch?v=s1khZWWiQfA)
* [Learning To Drive From a World on Rails](https://arxiv.org/abs/2105.00636)<br>:open_mouth:oral:star:[code](https://github.com/dotchen/WorldOnRails):house:[project](https://dotchen.github.io/world_on_rails/)
* [DRIVE: Deep Reinforced Accident Anticipation With Visual Explanation](https://arxiv.org/abs/2107.10189)<br>:star:[code](https://github.com/Cogito2012/DRIVE):house:[project](https://www.rit.edu/actionlab/drive):tv:[video](https://youtu.be/e2K2wTorKOc)
* [LookOut: Diverse Multi-Future Prediction and Planning for Self-Driving](https://arxiv.org/abs/2101.06547)
* [Prediction by Anticipation: An Action-Conditional Prediction Method Based on Interaction Learning](https://arxiv.org/abs/2012.13478)<br>:star:[code](https://github.com/Atcold/pytorch-PPUU):tv:[video](https://www.youtube.com/watch?v=X2s7gy3wIYw)
* [TMCOSS: Thresholded Multi-Criteria Online Subset Selection for Data-Efficient Autonomous Driving](https://openaccess.thecvf.com/content/ICCV2021/papers/Das_TMCOSS_Thresholded_Multi-Criteria_Online_Subset_Selection_for_Data-Efficient_Autonomous_Driving_ICCV_2021_paper.pdf)
* [FIERY: Future Instance Prediction in Bird's-Eye View From Surround Monocular Cameras](https://openaccess.thecvf.com/content/ICCV2021/papers/Hu_FIERY_Future_Instance_Prediction_in_Birds-Eye_View_From_Surround_Monocular_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/wayveai/fiery)
* Human trajectory prediction(人体轨迹预测)
  * [Personalized Trajectory Prediction via Distribution Discrimination](https://arxiv.org/abs/2107.14204)<br>:star:[code](https://github.com/CHENGY12/DisDis)
  * [Human Trajectory Prediction via Counterfactual Analysis](https://arxiv.org/abs/2107.14202)(https://github.com/CHENGY12/CausalHTP)
  * [From Goals, Waypoints & Paths to Long Term Human Trajectory Forecasting](https://arxiv.org/abs/2012.01526)<br>:star:[code](https://github.com/HarshayuGirase/PECNet):house:[project](https://karttikeya.github.io/publication/ynet/):tv:[video](https://youtu.be/XCWCHwGlBgE)
* 轨迹预测
  * [Unlimited Neighborhood Interaction for Heterogeneous Trajectory Prediction](https://arxiv.org/abs/2108.00238)
  * [LOKI: Long Term and Key Intentions for Trajectory Prediction](https://arxiv.org/abs/2108.08236)
  * [MG-GAN: A Multi-Generator Model Preventing Out-of-Distribution Samples in Pedestrian Trajectory Prediction](https://arxiv.org/abs/2108.09274)<br>:star:[code](https://github.com/selflein/MG-GAN)
  * [DenseTNT: End-to-end Trajectory Prediction from Dense Goal Sets](https://arxiv.org/abs/2108.09640)
  * [Where Are You Heading? Dynamic Trajectory Prediction With Expert Goal Examples](https://openaccess.thecvf.com/content/ICCV2021/papers/Zhao_Where_Are_You_Heading_Dynamic_Trajectory_Prediction_With_Expert_Goal_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/JoeHEZHAO/expert_traj)
  * [Three Steps to Multimodal Trajectory Prediction: Modality Clustering, Classification and Synthesis](https://arxiv.org/abs/2103.07854)
* 运动预测
  * [RAIN: Reinforced Hybrid Attention Inference Network for Motion Forecasting](https://arxiv.org/abs/2108.01316)<br>:house:[project](https://jiachenli94.github.io/publications/RAIN/)
  * [SLAMP: Stochastic Latent Appearance and Motion Prediction](https://arxiv.org/abs/2108.02760)<br>:house:[project](https://kuis-ai.github.io/slamp/)
  * [SlowFast Rolling-Unrolling LSTMs for Action Anticipation in Egocentric Videos](https://arxiv.org/abs/2109.00829)
* 自动导航
  * [FOVEA: Foveated Image Magnification for Autonomous Navigation](https://arxiv.org/abs/2108.12102)<br>:house:[project](https://www.cs.cmu.edu/~mengtial/proj/fovea/)  
* 交通场景理解
  * [Structured Bird's-Eye-View Traffic Scene Understanding from Onboard Images](https://arxiv.org/abs/2110.01997)<br>:star:[code](https://github.com/ybarancan/STSU)
* 车辆车牌识别
  * 车辆重识别
    * [Heterogeneous Relational Complement for Vehicle Re-identification](https://arxiv.org/abs/2109.07894)
    * [Self-Supervised Geometric Features Discovery via Interpretable Attention for Vehicle Re-Identification and Beyond](https://openaccess.thecvf.com/content/ICCV2021/papers/Li_Self-Supervised_Geometric_Features_Discovery_via_Interpretable_Attention_for_Vehicle_Re-Identification_ICCV_2021_paper.pdf)
 * 自主赛车
   * [Learn-to-Race: A Multimodal Control Environment for Autonomous Racing](https://openaccess.thecvf.com/content/ICCV2021/papers/Herman_Learn-To-Race_A_Multimodal_Control_Environment_for_Autonomous_Racing_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/learn-to-race/l2r)
* 预测司机的视觉注意力
  * [MEDIRL: Predicting the Visual Attention of Drivers via Maximum Entropy Deep Inverse Reinforcement Learning](https://arxiv.org/abs/1912.07773)<br>:star:[code](https://github.com/soniabaee/MEDIRL-EyeCar)
* 姿势预测
  * [Space-Time-Separable Graph Convolutional Network for Pose Forecasting](https://openaccess.thecvf.com/content/ICCV2021/papers/Sofianos_Space-Time-Separable_Graph_Convolutional_Network_for_Pose_Forecasting_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/FraLuca/STSGCN):tv:[video](https://www.youtube.com/watch?v=tQIygtJrrtk)
 
<a name="18"/>

## 18.Transformers
* [Generic Attention-model Explainability for Interpreting Bi-Modal and Encoder-Decoder Transformers](https://arxiv.org/abs/2103.15679)<br>:open_mouth:oral:star:[code](https://github.com/hila-chefer/Transformer-MM-Explainability)<br>:newspaper:解读:[ICCV2021 Oral-TAU&Facebook提出了通用的Attention模型可解释性](https://mp.weixin.qq.com/s/3skb8XMiwM3QtdH7ZazoYg)
* [PlaneTR: Structure-Guided Transformers for 3D Plane Recovery](https://arxiv.org/abs/2107.13108)<br>:star:[code](https://github.com/IceTTTb/PlaneTR3D)
* [Rethinking and Improving Relative Position Encoding for Vision Transformer](https://arxiv.org/abs/2107.14222)<br>:star:[code](https://github.com/microsoft/AutoML/tree/main/iRPE)
* [Vision Transformer with Progressive Sampling](https://arxiv.org/abs/2108.01684)
* [Paint Transformer: Feed Forward Neural Painting with Stroke Prediction](https://arxiv.org/abs/2108.03798)<br>:open_mouth:oral:star:[code](https://github.com/Huage001/PaintTransformer)
* [Rethinking Spatial Dimensions of Vision Transformers](https://arxiv.org/abs/2103.16302)<br>:star:[code](https://github.com/naver-ai/pit)<br>:newspaper:解读:[ICCV2021-PiT-池化操作不是CNN的专属，ViT说：“我也可以”；南大提出池化视觉Transformer（PiT）](https://mp.weixin.qq.com/s/b051uw8SSu6x-5R27e8AMg)
* [PnP-DETR: Towards Efficient Visual Analysis with Transformers](https://arxiv.org/abs/2109.07036)<br>:star:[code](https://github.com/twangnh/pnp-detr)
* [Describing and Localizing Multiple Changes With Transformers](https://arxiv.org/abs/2103.14146)<br>:star:[code](https://github.com/cvpaperchallenge/Describing-and-Localizing-Multiple-Change-with-Transformers):house:[project](https://cvpaperchallenge.github.io/Describing-and-Localizing-Multiple-Change-with-Transformers/)
* [LeViT: A Vision Transformer in ConvNet's Clothing for Faster Inference](https://openaccess.thecvf.com/content/ICCV2021/papers/Graham_LeViT_A_Vision_Transformer_in_ConvNets_Clothing_for_Faster_Inference_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/facebookresearch/LeViT)
* [VidTr: Video Transformer Without Convolutions](http://arxiv.org/abs/2104.11746)
* [Visformer: The Vision-Friendly Transformer](https://arxiv.org/abs/2104.12533)<br>:star:[code](https://github.com/danczs/Visformer)
* [Going Deeper With Image Transformers](https://arxiv.org/abs/2103.17239)<br>:star:[code](https://github.com/facebookresearch/deit)
* [Multiscale Vision Transformers](https://arxiv.org/abs/2104.11227)<br>:star:[code](https://github.com/facebookresearch/SlowFast)
* [Learning Multi-Scene Absolute Pose Regression With Transformers](https://arxiv.org/abs/2103.11468)<br>:star:[code](https://github.com/yolish/multi-scene-pose-transformer)
* [Visual Saliency Transformer](https://arxiv.org/abs/2104.12099)<br>:star:[code](https://github.com/nnizhang/VST)
* [Event-Based Video Reconstruction Using Transformer](https://openaccess.thecvf.com/content/ICCV2021/papers/Weng_Event-Based_Video_Reconstruction_Using_Transformer_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/WarranWeng/ET-Net)
* [Swin Transformer: Hierarchical Vision Transformer Using Shifted Windows](https://arxiv.org/abs/2103.14030)<br>:star:[code](https://github.com/microsoft/Swin-Transformer)
* [An Empirical Study of Training Self-Supervised Vision Transformers](https://arxiv.org/abs/2104.02057)<br>:open_mouth:oral:star:[code](https://github.com/facebookresearch/moco-v3)
* [Tokens-to-Token ViT: Training Vision Transformers From Scratch on ImageNet](https://arxiv.org/abs/2101.11986)<br>:star:[code](https://github.com/yitu-opensource/T2T-ViT)
* 密集预测
  * [Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions](https://arxiv.org/abs/2102.12122)<br>:open_mouth:oral:star:[code](https://github.com/whai362/PVT)<br>:newspaper:解读:[大白话Pyramid Vision Transformer](https://mp.weixin.qq.com/s/oJHZWmStQYYzEEOveiuQrQ)
* 3D人体纹理估计
  * [3D Human Texture Estimation from a Single Image with Transformers](https://arxiv.org/abs/2109.02563)<br>:open_mouth:oral:house:[project](https://www.mmlab-ntu.com/project/texformer/)
* 图像编辑  
  * [Multi-Scale Vision Longformer: A New Vision Transformer for High-Resolution Image Encoding](https://arxiv.org/abs/2103.15358)<br>:star:[code](https://github.com/microsoft/vision-longformer)
* OCR  
  * [DocFormer: End-to-End Transformer for Document Understanding](https://arxiv.org/abs/2106.11539)
* 根据音乐生成舞蹈
  * [AI Choreographer: Music Conditioned 3D Dance Generation With AIST++](https://openaccess.thecvf.com/content/ICCV2021/papers/Li_AI_Choreographer_Music_Conditioned_3D_Dance_Generation_With_AIST_ICCV_2021_paper.pdf)<br>:house:[project](https://google.github.io/aichoreographer/)<br>:newspaper:简介:[Transformer又又来了，生成配有音乐的丝滑3D舞蹈，开放最大规模数据集AIST++](https://zhuanlan.zhihu.com/p/346151291)


<a name="17"/>

## 17.3D(三维视觉)
* [Discovering 3D Parts from Image Collections](https://arxiv.org/abs/2107.13629)<br>:open_mouth:oral:star:[code](https://github.com/chhankyao/lpd):house:[project](https://chhankyao.github.io/lpd/):tv:[video](https://youtu.be/erNQANNwK6k)
* [PixelSynth: Generating a 3D-Consistent Experience from a Single Image](https://arxiv.org/abs/2108.05892)<br>:star:[code](https://github.com/crockwell/pixelsynth):house:[project](https://crockwell.github.io/pixelsynth/)
* [Towers of Babel: Combining Images, Language, and 3D Geometry for Learning Multimodal Vision](https://arxiv.org/abs/2108.05863)<br>:star:[code](https://github.com/tgxs002/wikiscenes):house:[project](https://www.cs.cornell.edu/projects/babel/)
* [Pixel-Perfect Structure-from-Motion with Featuremetric Refinement](https://arxiv.org/abs/2108.08291)<br>:star:[code](https://github.com/cvg/pixel-perfect-sfm)
* [Learning Anchored Unsigned Distance Functions with Gradient Direction Alignment for Single-view Garment Reconstruction](https://arxiv.org/abs/2108.08478)<br>:open_mouth:oral:star:[code](https://github.com/zhaofang0627/AnchorUDF)
* [LSD-StructureNet: Modeling Levels of Structural Detail in 3D Part Hierarchies](https://arxiv.org/abs/2108.13459)
* [Rational Polynomial Camera Model Warping for Deep Learning Based Satellite Multi-View Stereo Matching](https://arxiv.org/abs/2109.11121)<br>:star:[code](https://github.com/WHU-GPCV/SatMVS)
* [Where2Act: From Pixels to Actions for Articulated 3D Objects](https://arxiv.org/abs/2101.02692)<br>:tv:[video](https://www.youtube.com/watch?v=cdMSZru3Aa8)
* [BuildingNet: Learning to Label 3D Buildings](https://arxiv.org/abs/2110.04955)<br>:open_mouth:oral:star:[code](https://github.com/buildingnet/buildingnet_dataset):house:[project](https://buildingnet.org/)
* [SurfGen: Adversarial 3D Shape Synthesis With Explicit Surface Discriminators](https://openaccess.thecvf.com/content/ICCV2021/papers/Luo_SurfGen_Adversarial_3D_Shape_Synthesis_With_Explicit_Surface_Discriminators_ICCV_2021_paper.pdf)
* [Deep Virtual Markers for Articulated 3D Shapes](https://arxiv.org/abs/2108.09000)<br>:star:[code](https://github.com/T2Kim/DeepVirtualMarkers):tv:[video](https://www.youtube.com/watch?v=Raq5axLdG6E)
* [Learning Efficient Photometric Feature Transform for Multi-view Stereo](https://arxiv.org/abs/2103.14794)<br>:house:[project](https://svbrdf.github.io/publications/ptmvs/project.html)
* [Neural-GIF: Neural Generalized Implicit Functions for Animating People in Clothing](https://openaccess.thecvf.com/content/ICCV2021/papers/Tiwari_Neural-GIF_Neural_Generalized_Implicit_Functions_for_Animating_People_in_Clothing_ICCV_2021_paper.pdf)
* [Just a Few Points Are All You Need for Multi-View Stereo: A Novel Semi-Supervised Learning Method for Multi-View Stereo](https://openaccess.thecvf.com/content/ICCV2021/papers/Kim_Just_a_Few_Points_Are_All_You_Need_for_Multi-View_ICCV_2021_paper.pdf)
* [3D-FRONT: 3D Furnished Rooms With layOuts and semaNTics](https://openaccess.thecvf.com/content/ICCV2021/papers/Fu_3D-FRONT_3D_Furnished_Rooms_With_layOuts_and_semaNTics_ICCV_2021_paper.pdf)
* [Learning Generative Models of Textured 3D Meshes from Real-World Images](https://arxiv.org/abs/2103.15627)<br>:star:[code](https://github.com/dariopavllo/textured-3d-gan)
* 深度估计
  * [StructDepth: Leveraging the structural regularities for self-supervised indoor depth estimation](https://arxiv.org/abs/2108.08574)<br>:star:[code](https://github.com/SJTU-ViSYS/StructDepth)
  * [Bridging Unsupervised and Supervised Depth from Focus via All-in-Focus Supervision](https://arxiv.org/abs/2108.10843)<br>:star:[code](https://github.com/albert100121/AiFDepthNet):house:[project](https://albert100121.github.io/AiFDepthNet/)
  * [Augmenting Depth Estimation with Geospatial Context](https://arxiv.org/abs/2109.09879)
  * [Can Scale-Consistent Monocular Depth Be Learned in a Self-Supervised Scale-Invariant Manner?](https://openaccess.thecvf.com/content/ICCV2021/papers/Wang_Can_Scale-Consistent_Monocular_Depth_Be_Learned_in_a_Self-Supervised_Scale-Invariant_ICCV_2021_paper.pdf)
  * [Revisiting Stereo Depth Estimation From a Sequence-to-Sequence Perspective With Transformers](https://arxiv.org/abs/2011.02910)<br>:open_mouth:oral:star:[code](https://github.com/mli0603/stereo-transformer)
  * [Adaptive Surface Normal Constraint for Depth Estimation](https://arxiv.org/abs/2103.15483)<br>:star:[code](https://github.com/xxlong0/ASNDepth)
  * Monocular Depth Estimation(单目深度估计)
    * [MonoIndoor: Towards Good Practice of Self-Supervised Monocular Depth Estimation for Indoor Environments](https://arxiv.org/abs/2107.12429)
    * [Regularizing Nighttime Weirdness: Efficient Self-supervised Monocular Depth Estimation in the Dark](https://arxiv.org/abs/2108.03830)<br>:star:[code](https://github.com/w2kun/RNW)
    * [Towards Interpretable Deep Networks for Monocular Depth Estimation](https://arxiv.org/abs/2108.05312)<br>:star:[code](https://github.com/youzunzhi/InterpretableMDE):tv:[video](https://www.youtube.com/watch?v=er7TF2CusWo)
    * [Self-supervised Monocular Depth Estimation for All Day Images using Domain Separation](https://arxiv.org/abs/2108.07628)<br>:star:[code](https://github.com/LINA-lln/ADDS-DepthNet)
    * [Fine-grained Semantics-aware Representation Enhancement for Self-supervised Monocular Depth Estimation](https://arxiv.org/abs/2108.08829)<br>:open_mouth:oral:star:[code](https://github.com/hyBlue/FSRE-Depth)
    * [Excavating the Potential Capacity of Self-Supervised Monocular Depth Estimation](https://arxiv.org/abs/2109.12484)<br>:star:[code](https://github.com/prstrive/EPCDepth)
    * [R-MSFM: Recurrent Multi-Scale Feature Modulation for Monocular Depth Estimating](https://openaccess.thecvf.com/content/ICCV2021/papers/Zhou_R-MSFM_Recurrent_Multi-Scale_Feature_Modulation_for_Monocular_Depth_Estimating_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/jsczzzk/R-MSFM)
    * [Adaptive Confidence Thresholding for Monocular Depth Estimation](https://arxiv.org/abs/2009.12840)(https://github.com/megvii-research/OMNet)
* 深度补全
  * [Bayesian Deep Basis Fitting for Depth Completion With Uncertainty](https://arxiv.org/abs/2103.15254)
* Omnidirectional Localization
  * [PICCOLO: Point Cloud-Centric Omnidirectional Localization](https://arxiv.org/abs/2108.06545)
* 三维重建
  * [Learning Signed Distance Field for Multi-view Surface Reconstruction](https://arxiv.org/abs/2108.09964)<br>:open_mouth:oral
  * [3DStyleNet: Creating 3D Shapes with Geometric and Texture Style Variations](https://arxiv.org/abs/2108.12958)<br>:open_mouth:oral:house:[project](https://nv-tlabs.github.io/3DStyleNet/)
  * [DensePose 3D: Lifting Canonical Surface Maps of Articulated Objects to the Third Dimension](https://arxiv.org/abs/2109.00033)
  * [In-the-Wild Single Camera 3D Reconstruction Through Moving Water Surfaces](https://vccimaging.org/Publications/Xiong2021MovingWater/Xiong2021MovingWater.pdf)<br>:open_mouth:oral:star:[code](https://github.com/vccimaging/Reconstrution_Through_Moving_Water):tv:[video](https://www.youtube.com/watch?v=F6R52hfAs6s)
  * [Gaussian Fusion: Accurate 3D Reconstruction via Geometry-Guided Displacement Interpolation](https://openaccess.thecvf.com/content/ICCV2021/papers/Chen_Gaussian_Fusion_Accurate_3D_Reconstruction_via_Geometry-Guided_Displacement_Interpolation_ICCV_2021_paper.pdf)
  * 三维场景重建
    * [Graph-to-3D: End-to-End Generation and Manipulation of 3D Scenes Using Scene Graphs](https://arxiv.org/abs/2108.08841)
    * [VolumeFusion: Deep Depth Fusion for 3D Scene Reconstruction](https://arxiv.org/abs/2108.08623)
  * 三维形状重建  
    * [3DIAS: 3D Shape Reconstruction with Implicit Algebraic Surfaces](https://arxiv.org/abs/2108.08653)<br>:house:[project](https://myavartanoo.github.io/3dias/)
    * [Multiresolution Deep Implicit Functions for 3D Shape Representation](https://arxiv.org/abs/2109.05591)
    * [MVTN: Multi-View Transformation Network for 3D Shape Recognition](https://arxiv.org/abs/2011.13244)<br>:star:[code](https://github.com/ajhamdi/MVTN):tv:[video](https://www.youtube.com/watch?v=1zaHx8ztlhk)
  * 三维网格重建    
    * [Vis2Mesh: Efficient Mesh Reconstruction from Unstructured Point Clouds of Large Scenes with Learned Virtual View Visibility](https://arxiv.org/abs/2108.08378)<br>:star:[code](https://github.com/GDAOSU/vis2mesh) 
* 三维场景
  * [Scene Synthesis via Uncertainty-Driven Attribute Synchronization](https://arxiv.org/abs/2108.13499)<br>:star:[code](https://github.com/yanghtr/Sync2Gen)
* 相机校准
  * [CTRL-C: Camera calibration TRansformer with Line-Classification](https://arxiv.org/abs/2109.02259) 
* 表面重建
  * [Temporally-Coherent Surface Reconstruction via Metric-Consistent Atlases](https://arxiv.org/abs/2104.06950)
  * [Planar Surface Reconstruction from Sparse Views](https://arxiv.org/abs/2103.14644)<br>:open_mouth:oral:star:[code](https://github.com/jinlinyi/SparsePlanes):house:[project](https://jinlinyi.github.io/SparsePlanes/):tv:[video](https://www.youtube.com/watch?v=US3EKPe3nAw)
* 3D场景合成
  * [Indoor Scene Generation From a Collection of Semantic-Segmented Depth Images](https://arxiv.org/abs/2108.09022)<br>:star:[code](https://github.com/mingjiayang/SGSDI)
* 3D形状识别
  * [Learning Canonical View Representation for 3D Shape Recognition With Arbitrary Views](https://arxiv.org/abs/2108.07084)<br>:star:[code](https://github.com/weixmath/CVR)
* 图像重建
  * [Semantic-embedded Unsupervised Spectral Reconstruction from Single RGB Images in the Wild](https://arxiv.org/abs/2108.06659)<br>:star:[code](https://github.com/zbzhzhy/Unsupervised-Spectral-Reconstruction)
* Multi-view Stereo
  * [Digging into Uncertainty in Self-supervised Multi-view Stereo](https://arxiv.org/abs/2108.12966)<br>:star:[code](https://github.com/ToughStoneX/U-MVS)
 
<a name="16"/>

## 16.Re-Identification(重识别)
##### Object Re-Identification目标(物体)重识别
* [TransReID: Transformer-Based Object Re-Identification](https://arxiv.org/abs/2102.04378)<br>:star:[code](https://github.com/damo-cv/TransReID)
#### Person Re-Identification(人员重识别)
* [Spatio-Temporal Representation Factorization for Video-based Person Re-Identification](https://arxiv.org/abs/2107.11878)
* [Learning Instance-level Spatial-Temporal Patterns for Person Re-identification](https://arxiv.org/abs/2108.00171)<br>:star:[code](https://github.com/RenMin1991/cleaned-DukeMTMC-reID/)
* [Towards Discriminative Representation Learning for Unsupervised Person Re-identification](https://arxiv.org/abs/2108.03439)
* [Learning by Aligning: Visible-Infrared Person Re-identification using Cross-Modal Correspondences](https://arxiv.org/abs/2108.07422)<br>:star:[code](https://github.com/cvlab-yonsei/LbA):house:[project](https://cvlab.yonsei.ac.kr/projects/LbA/)
* [Video-based Person Re-identification with Spatial and Temporal Memory Networks](https://arxiv.org/abs/2108.09039)<br>:house:[project](https://cvlab.yonsei.ac.kr/projects/STMN/)
* [Multi-Expert Adversarial Attack Detection in Person Re-identification Using Context Inconsistency](https://arxiv.org/abs/2108.09891)
* [Clothing Status Awareness for Long-Term Person Re-Identification](https://openaccess.thecvf.com/content/ICCV2021/papers/Huang_Clothing_Status_Awareness_for_Long-Term_Person_Re-Identification_ICCV_2021_paper.pdf)
* [Dense Interaction Learning for Video-Based Person Re-Identification](https://arxiv.org/abs/2103.09013)<br>:open_mouth:oral
* [Explainable Person Re-Identification With Attribute-Guided Metric Distillation](https://arxiv.org/abs/2103.01451)<br>:house:[project](https://xiaodongchen.cn/AMD.github.io/)
* [Online Pseudo Label Generation by Hierarchical Cluster Dynamics for Adaptive Person Re-Identification](https://openaccess.thecvf.com/content/ICCV2021/papers/Zheng_Online_Pseudo_Label_Generation_by_Hierarchical_Cluster_Dynamics_for_Adaptive_ICCV_2021_paper.pdf)
* [Pyramid Spatial-Temporal Aggregation for Video-Based Person Re-Identification](https://openaccess.thecvf.com/content/ICCV2021/papers/Wang_Pyramid_Spatial-Temporal_Aggregation_for_Video-Based_Person_Re-Identification_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/WangYQ9/VideoReID_PSTA)
* [ICE: Inter-Instance Contrastive Encoding for Unsupervised Person Re-Identification](https://arxiv.org/abs/2103.16364)<br>:star:[code](https://github.com/chenhao2345/ICE):tv:[video](https://drive.google.com/file/d/1E__ru9u_oRcb44-WIH_GjBTv1-_5rcO2/view)
* 域适应人员重识别
  * [IDM: An Intermediate Domain Module for Domain Adaptive Person Re-ID](https://arxiv.org/abs/2108.02413)<br>:open_mouth:oral:star:[code](https://github.com/SikaStar/IDM)
* Crowd Counting(拥挤人群计数)
  * [Rethinking Counting and Localization in Crowds:A Purely Point-Based Framework](https://arxiv.org/abs/2107.12746)<br>:open_mouth:oral:star:[code](https://github.com/TencentYoutuResearch)
  * [Uniformity in Heterogeneity:Diving Deep into Count Interval Partition for Crowd Counting](https://arxiv.org/abs/2107.12619)<br>:star:[code](https://github.com/TencentYoutuResearch)
  * [Spatial Uncertainty-Aware Semi-Supervised Crowd Counting](https://arxiv.org/abs/2107.13271)<br>:star:[code](https://github.com/smallmax00/SUA_crowd_counting)
  * [Variational Attention: Propagating Domain-Specific Knowledge for Multi-Domain Learning in Crowd Counting](https://arxiv.org/abs/2108.08023)<br>:star:[code](https://github.com/Zhaoyi-Yan/DKPNet)
  * [Exploiting Sample Correlation for Crowd Counting With Multi-Expert Network](https://openaccess.thecvf.com/content/ICCV2021/papers/Liu_Exploiting_Sample_Correlation_for_Crowd_Counting_With_Multi-Expert_Network_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/streamer-AP)
  * [Crowd Counting With Partial Annotations in an Image](https://openaccess.thecvf.com/content/ICCV2021/papers/Xu_Crowd_Counting_With_Partial_Annotations_in_an_Image_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/svip-lab/CrwodCountingPAL)
* [Towards A Universal Model for Cross-Dataset Crowd Counting](https://openaccess.thecvf.com/content/ICCV2021/papers/Ma_Towards_a_Universal_Model_for_Cross-Dataset_Crowd_Counting_ICCV_2021_paper.pdf)
* 跨模态人员重识别
  * [Cross-Modality Person Re-Identification via Modality Confusion and Center Aggregation](https://openaccess.thecvf.com/content/ICCV2021/papers/Hao_Cross-Modality_Person_Re-Identification_via_Modality_Confusion_and_Center_Aggregation_ICCV_2021_paper.pdf)
* Person Search
  * [ASMR: Learning Attribute-Based Person Search with Adaptive Semantic Margin Regularizer](https://arxiv.org/abs/2108.04533)<br>:house:[project](http://cvlab.postech.ac.kr/research/ASMR/)
* 行人检测
  * [MOTSynth: How Can Synthetic Data Help Pedestrian Detection and Tracking?](https://arxiv.org/abs/2108.09518)
  * [Stacked Homography Transformations for Multi-View Pedestrian Detection](https://openaccess.thecvf.com/content/ICCV2021/papers/Song_Stacked_Homography_Transformations_for_Multi-View_Pedestrian_Detection_ICCV_2021_paper.pdf)
* 行人属性识别
  * [Spatial and Semantic Consistency Regularizations for Pedestrian Attribute Recognition](https://arxiv.org/abs/2109.05686)
  * [LapsCore: Language-Guided Person Search via Color Reasoning](https://openaccess.thecvf.com/content/ICCV2021/papers/Wu_LapsCore_Language-Guided_Person_Search_via_Color_Reasoning_ICCV_2021_paper.pdf)
* Person Search(行人搜索)
  * [Weakly Supervised Person Search with Region Siamese Networks](https://arxiv.org/abs/2109.06109)
* 行人行为预测
  * [Bifold and Semantic Reasoning for Pedestrian Behavior Prediction](https://arxiv.org/abs/2012.03298)
* 步态识别
  * [Gait Recognition via Effective Global-Local Feature Representation and Local Temporal Aggregation](https://arxiv.org/abs/2011.01461)

<a name="15"/>

## 15.Object Tracking(目标跟踪)
* [Saliency-Associated Object Tracking](https://arxiv.org/abs/2108.03637)
* [Box-Aware Feature Enhancement for Single Object Tracking on Point Clouds](https://arxiv.org/abs/2108.04728)<br>:star:[code](https://github.com/Ghostish/BAT)
* [Learning to Track Objects from Unlabeled Videos](https://arxiv.org/abs/2108.12711)<br>:star:[code](https://github.com/VISION-SJTU/USOT)
* [DepthTrack : Unveiling the Power of RGBD Tracking](https://arxiv.org/abs/2108.13962)<br>:star:[code](https://github.com/xiaozai/DeT)
* [Learning Target Candidate Association To Keep Track of What Not To Track](https://arxiv.org/abs/2103.16556)<br>:star:[code](https://github.com/visionml/pytracking)
* 视觉目标跟踪
  * [Learning to Adversarially Blur Visual Object Tracking](https://arxiv.org/abs/2107.12085)<br>:star:[code](https://github.com/tsingqguo/ABA)
  * [Learn to Match: Automatic Matching Network Design for Visual Tracking](https://arxiv.org/abs/2108.00803)<br>:star:[code](https://github.com/JudasDie/SOTS)
  * [Video Annotation for Visual Tracking via Selection and Refinement](https://arxiv.org/abs/2108.03821)<br>:star:[code](https://github.com/Daikenan/VASR)
  * 3D视觉跟踪
    * [MLVSNet: Multi-Level Voting Siamese Network for 3D Visual Tracking](https://openaccess.thecvf.com/content/ICCV2021/papers/Wang_MLVSNet_Multi-Level_Voting_Siamese_Network_for_3D_Visual_Tracking_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/CodeWZT/MLVSNet)
* 卫星图像跟踪 
  * [HiFT: Hierarchical Feature Transformer for Aerial Tracking](https://arxiv.org/abs/2108.00202)<br>:star:[code](https://github.com/vision4robotics/HiFT)
* 3D多目标跟踪
  * [Exploring Simple 3D Multi-Object Tracking for Autonomous Driving](https://arxiv.org/abs/2108.10312)<br>:star:[code](https://github.com/qcraftai/simtrack)<br>:newspaper:解读:[ICCV 2021丨轻舟智航提出SimTrack: 3D多目标一体化检测与跟踪，简单又精确](https://mp.weixin.qq.com/s/7vCckbjGd65NMgW9evR4Ag)
* 多目标跟踪与分割
  * [Assignment-Space-Based Multi-Object Tracking and Segmentation](https://openaccess.thecvf.com/content/ICCV2021/papers/Choudhuri_Assignment-Space-Based_Multi-Object_Tracking_and_Segmentation_ICCV_2021_paper.pdf)
* 多目标跟踪
  * [A General Recurrent Tracking Framework Without Real Data](https://openaccess.thecvf.com/content/ICCV2021/papers/Wang_A_General_Recurrent_Tracking_Framework_Without_Real_Data_ICCV_2021_paper.pdf)
  * [Making Higher Order MOT Scalable: An Efficient Approximate Solver for Lifted Disjoint Paths](https://arxiv.org/abs/2108.10606)<br>:star:[code](https://github.com/TimoK93/ApLift)
* 视频目标跟踪
  * [TF-Blender: Temporal Feature Blender for Video Object Detection](https://openaccess.thecvf.com/content/ICCV2021/papers/Cui_TF-Blender_Temporal_Feature_Blender_for_Video_Object_Detection_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/goodproj13/TF-Blender)


<a name="14"/>

## 14.Object Detection(目标检测)
* [Rank & Sort Loss for Object Detection and Instance Segmentation](https://arxiv.org/abs/2107.11669)<br>:open_mouth:oral:star:[code](https://github.com/kemaloksuz/RankSortLoss)
* [MDETR : Modulated Detection for End-to-End Multi-Modal Understanding](https://arxiv.org/abs/2104.12763)<br>:open_mouth:oral:star:[code](https://github.com/ashkamath/mdetr)
* [SimROD: A Simple Adaptation Method for Robust Object Detection](https://arxiv.org/abs/2107.13389)<br>:open_mouth:oral:house:[project](https://marketplace.huaweicloud.com/markets/aihub/notebook/detail/?id=d6d7162f-32b9-483d-97d7-b16b32b148e2)
* [GraphFPN: Graph Feature Pyramid Network for Object Detection](https://arxiv.org/abs/2108.00580)
* [Fast Convergence of DETR with Spatially Modulated Co-Attention](https://arxiv.org/abs/2108.02404)<br>:star:[code](https://github.com/gaopengcuhk/SMCA-DETR)
* [Oriented R-CNN for Object Detection](https://arxiv.org/abs/2108.05699)<br>:star:[code](https://github.com/jbwang1997/OBBDetection) 
* [Conditional DETR for Fast Training Convergence](https://arxiv.org/abs/2108.06152)<br>:newspaper:解读:[通过显式寻找物体的 extremity 区域加快 DETR 的收敛：Conditional DETR](https://mp.weixin.qq.com/s/RbtdfFDczrSxi0F4apbx1w)
* [Vector-Decomposed Disentanglement for Domain-Invariant Object Detection](https://arxiv.org/abs/2108.06685)<br>:star:[code](https://github.com/AmingWu/VDD-DAOD)
* [G-DetKD: Towards General Distillation Framework for Object Detectors via Contrastive and Semantic-guided Feature Imitation](https://arxiv.org/abs/2108.07482)
* [ODAM: Object Detection, Association, and Mapping using Posed RGB Video](https://arxiv.org/abs/2108.10165)<br>:open_mouth:oral
* [Reconcile Prediction Consistency for Balanced Object Detection](https://arxiv.org/abs/2108.10809)
* [Deep Structured Instance Graph for Distilling Object Detectors](https://arxiv.org/abs/2109.12862)<br>:star:[code](https://github.com/dvlab-research/Dsig)
* [Towards Rotation Invariance in Object Detection](https://arxiv.org/abs/2109.13488)<br>:star:[code](https://github.com/akasha-imaging/ICCV2021)
* [Morphable Detector for Object Detection on Demand](https://arxiv.org/abs/2110.04917)<br>:star:[code](https://github.com/Zhaoxiangyun/Morphable-Detector)
* [DetCo: Unsupervised Contrastive Learning for Object Detection](https://arxiv.org/abs/2102.04803)<br>:star:[code](https://github.com/xieenze/DetCo)
* [Domain-Invariant Disentangled Network for Generalizable Object Detection](https://openaccess.thecvf.com/content/ICCV2021/papers/Lin_Domain-Invariant_Disentangled_Network_for_Generalizable_Object_Detection_ICCV_2021_paper.pdf)
* [MDETR - Modulated Detection for End-to-End Multi-Modal Understanding](https://openaccess.thecvf.com/content/ICCV2021/papers/Kamath_MDETR_-_Modulated_Detection_for_End-to-End_Multi-Modal_Understanding_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/ashkamath/mdetr)
* [Detecting Persuasive Atypicality by Modeling Contextual Compatibility](https://openaccess.thecvf.com/content/ICCV2021/papers/Guo_Detecting_Persuasive_Atypicality_by_Modeling_Contextual_Compatibility_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/MeiqiGuo/ICCV2021-AtypicalityDetection)
* [Wanderlust: Online Continual Object Detection in the Real World](https://openaccess.thecvf.com/content/ICCV2021/papers/Wang_Wanderlust_Online_Continual_Object_Detection_in_the_Real_World_ICCV_2021_paper.pdf)<br>:house:[project](https://oakdata.github.io/)
* [PreDet: Large-Scale Weakly Supervised Pre-Training for Detection](https://openaccess.thecvf.com/content/ICCV2021/papers/Ramanathan_PreDet_Large-Scale_Weakly_Supervised_Pre-Training_for_Detection_ICCV_2021_paper.pdf)
* [FMODetect: Robust Detection of Fast Moving Objects](https://arxiv.org/abs/2012.08216)
* [Multi-Source Domain Adaptation for Object Detection](https://arxiv.org/abs/2106.15793)
* [Self-Supervised Object Detection via Generative Image Synthesis](https://openaccess.thecvf.com/content/ICCV2021/papers/Mustikovela_Self-Supervised_Object_Detection_via_Generative_Image_Synthesis_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/NVlabs/SSOD)
* [Naturalistic Physical Adversarial Patch for Object Detectors](https://openaccess.thecvf.com/content/ICCV2021/papers/Hu_Naturalistic_Physical_Adversarial_Patch_for_Object_Detectors_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/aiiu-lab/Naturalistic-Adversarial-Patch)
* [Rethinking Transformer-Based Set Prediction for Object Detection](https://arxiv.org/abs/2011.10881)<br>:star:[code](https://github.com/Edward-Sun/TSP-Detection)
* 3D目标检测
  * [Geometry Uncertainty Projection Network for Monocular 3D Object Detection](https://arxiv.org/abs/2107.13774)<br>:star:[code](https://github.com/SuperMHP/GUPNet)
  * [Fog Simulation on Real LiDAR Point Clouds for 3D Object Detection in Adverse Weather](https://arxiv.org/abs/2108.05249)<br>:star:[code](https://github.com/MartinHahner/LiDAR_fog_sim):house:[project](https://www.trace.ethz.ch/publications/2021/lidar_fog_simulation/)
  * [Is Pseudo-Lidar needed for Monocular 3D Object detection?](https://arxiv.org/abs/2108.06417)<br>:star:[code](https://github.com/TRI-ML/dd3d)
  * [RandomRooms: Unsupervised Pre-training from Synthetic Shapes and Randomized Layouts for 3D Object Detection](https://arxiv.org/abs/2108.07794)
  * [LIGA-Stereo: Learning LiDAR Geometry Aware Representations for Stereo-based 3D Detector](https://arxiv.org/abs/2108.08258)<br>:star:[code](https://github.com/xy-guo/LIGA-Stereo):house:[project](https://xy-guo.github.io/liga/)  
  * [Improving 3D Object Detection with Channel-wise Transformer](https://arxiv.org/abs/2108.10723)
  * [4D-Net for Learned Multi-Modal Alignment](https://arxiv.org/abs/2109.01066)
  * [Pyramid R-CNN: Towards Better Performance and Adaptability for 3D Object Detection](https://arxiv.org/abs/2109.02499)
  * [Voxel Transformer for 3D Object Detection](https://arxiv.org/abs/2109.02497)
  * [An End-to-End Transformer Model for 3D Object Detection](https://arxiv.org/abs/2109.08141)<br>:open_mouth:oral:star:[code](https://github.com/facebookresearch/3detr):house:[project](https://facebookresearch.github.io/3detr/)
  * [Unsupervised Domain Adaptive 3D Detection With Multi-Level Consistency](https://arxiv.org/abs/2107.11355)
  * [Group-Free 3D Object Detection via Transformers](https://arxiv.org/abs/2104.00678)<br>:star:[code](https://github.com/zeliu98/Group-Free-3D)
  * [VENet: Voting Enhancement Network for 3D Object Detection](https://openaccess.thecvf.com/content/ICCV2021/papers/Xie_VENet_Voting_Enhancement_Network_for_3D_Object_Detection_ICCV_2021_paper.pdf)
  * [Multi-Echo LiDAR for 3D Object Detection](https://openaccess.thecvf.com/content/ICCV2021/papers/Man_Multi-Echo_LiDAR_for_3D_Object_Detection_ICCV_2021_paper.pdf)
  * [Voxel Transformer for 3D Object Detection](https://arxiv.org/abs/2109.02497)
  * [RangeDet: In Defense of Range View for LiDAR-Based 3D Object Detection](https://arxiv.org/abs/2103.10039)<br>:star:[code](https://github.com/TuSimple/RangeDet)
  * [The Devil Is in the Task: Exploiting Reciprocal Appearance-Localization Features for Monocular 3D Object Detection](https://openaccess.thecvf.com/content/ICCV2021/papers/Zou_The_Devil_Is_in_the_Task_Exploiting_Reciprocal_Appearance-Localization_Features_ICCV_2021_paper.pdf)
  * [Gated3D: Monocular 3D Object Detection From Temporal Illumination Cues](https://openaccess.thecvf.com/content/ICCV2021/papers/Julca-Aguilar_Gated3D_Monocular_3D_Object_Detection_From_Temporal_Illumination_Cues_ICCV_2021_paper.pdf)<br>:house:[project](https://light.princeton.edu/publication/gated3d/)
* 目标定位
  * [Contrastive Attention Maps for Self-supervised Co-localization](https://openaccess.thecvf.com/content/ICCV2021/papers/Ki_Contrastive_Attention_Maps_for_Self-Supervised_Co-Localization_ICCV_2021_paper.pdf)
  * 弱监督目标定位
    * [Normalization Matters in Weakly Supervised Object Localization](https://arxiv.org/abs/2107.13221)
    * [Online Refinement of Low-Level Feature Based Activation Map for Weakly Supervised Object Localization](https://openaccess.thecvf.com/content/ICCV2021/papers/Xie_Online_Refinement_of_Low-Level_Feature_Based_Activation_Map_for_Weakly_ICCV_2021_paper.pdf)
    * [Foreground Activation Maps for Weakly Supervised Object Localization](https://openaccess.thecvf.com/content/ICCV2021/papers/Meng_Foreground_Activation_Maps_for_Weakly_Supervised_Object_Localization_ICCV_2021_paper.pdf)
* Anomaly Detection(图像异常检测)
  * [Divide-and-Assemble: Learning Block-wise Memory for Unsupervised Anomaly Detection](https://arxiv.org/abs/2107.13118)
* 弱监督目标检测
  * [Boosting Weakly Supervised Object Detection via Learning Bounding Box Adjusters](https://arxiv.org/abs/2108.01499)<br>:star:[code](https://github.com/DongSky/lbba_boosted_wsod)
* OOD 检测
  * [Triggering Failures: Out-Of-Distribution detection by learning from local adversarial attacks in Semantic Segmentation](https://arxiv.org/abs/2108.01634)<br>:star:[code](https://github.com/valeoai/obsnet)
* 显著目标检测
  * [Disentangled High Quality Salient Object Detection](https://arxiv.org/abs/2108.03551)
  * [Specificity-preserving RGB-D Saliency Detection](https://arxiv.org/abs/2108.08162)<br>:star:[code](https://github.com/taozh2017/SPNet)
  * [Light Field Saliency Detection with Dual Local Graph Learning and Reciprocative Guidance](https://arxiv.org/abs/2108.06384)
  * [MFNet: Multi-Filter Directive Network for Weakly Supervised Salient Object Detection](https://openaccess.thecvf.com/content/ICCV2021/papers/Piao_MFNet_Multi-Filter_Directive_Network_for_Weakly_Supervised_Salient_Object_Detection_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/OIPLab-DUT/MFNet)
  * RGB-D显著目标检测
    * [RGB-D Saliency Detection via Cascaded Mutual Information Minimization](https://arxiv.org/abs/2109.07246)<br>:star:[code](https://github.com/JingZhang617/cascaded_rgbd_sod)
  * co-saliency detection
    * [Summarize and Search: Learning Consensus-aware Dynamic Convolution for Co-Saliency Detection](https://arxiv.org/abs/2110.00338)<br>:star:[code](https://github.com/nnizhang/CADC)
* 违禁物品检测
  * [Towards Real-World Prohibited Item Detection: A Large-Scale X-ray Benchmark](https://arxiv.org/abs/2108.07020)<br>:sunflower:[dataset](https://github.com/bywang2018/security-dataset)
* 小样本目标检测
  * [DeFRCN: Decoupled Faster R-CNN for Few-Shot Object Detection](https://arxiv.org/abs/2108.09017)<br>:star:[code](https://github.com/er-muyue/DeFRCN)
  * [Query Adaptive Few-Shot Object Detection With Heterogeneous Graph Convolutional Networks](https://openaccess.thecvf.com/content/ICCV2021/papers/Han_Query_Adaptive_Few-Shot_Object_Detection_With_Heterogeneous_Graph_Convolutional_Networks_ICCV_2021_paper.pdf)
  * [Universal-Prototype Enhancing for Few-Shot Object Detection](https://arxiv.org/abs/2103.01077)<br>:star:[code](https://github.com/AmingWu/UP-FSOD)
* 视觉关系协同定位
  * [Few-shot Visual Relationship Co-localization](https://arxiv.org/abs/2108.11618)<br>:star:[code](https://github.com/vl2g/VRC):house:[project](https://vl2g.github.io/projects/vrc/)
* 密集目标检测
  * [Mutual Supervision for Dense Object Detection](https://arxiv.org/abs/2109.05986)<br>:star:[code](https://github.com/MCG-NJU)
* 域适应目标检测
  * [Seeking Similarities over Differences: Similarity-based Domain Alignment for Adaptive Object Detection](https://arxiv.org/abs/2110.01428)
  * [Knowledge Mining and Transferring for Domain Adaptive Object Detection](https://openaccess.thecvf.com/content/ICCV2021/papers/Tian_Knowledge_Mining_and_Transferring_for_Domain_Adaptive_Object_Detection_ICCV_2021_paper.pdf)
* 图像篡改检测
  * [Image Manipulation Detection by Multi-View Multi-Scale Supervision](https://arxiv.org/abs/2104.06832)<br>:star:[code](https://github.com/dong03/MVSS-Net)
* Visual Relationship Detection(VRD视觉关系检测)
  * [Grounding Consistency: Distilling Spatial Common Sense for Precise Visual Relationship Detection](https://openaccess.thecvf.com/content/ICCV2021/papers/Diomataris_Grounding_Consistency_Distilling_Spatial_Common_Sense_for_Precise_Visual_Relationship_ICCV_2021_paper.pdf)
* 长尾目标检测
  * [Exploring Classification Equilibrium in Long-Tailed Object Detection](http://arxiv.org/abs/2108.07507)<br>:star:[code](https://github.com/fcjian/LOCE)
* Salient Object Ranking
  * [Salient Object Ranking with Position-Preserved Attention](https://arxiv.org/abs/2106.05047)<br>:star:[code](https://github.com/EricFH/SOR)
* 小目标检测
  * [Robust Small Object Detection on the Water Surface Through Fusion of Camera and Millimeter Wave Radar](https://openaccess.thecvf.com/content/ICCV2021/papers/Cheng_Robust_Small_Object_Detection_on_the_Water_Surface_Through_Fusion_ICCV_2021_paper.pdf)
* 黑暗中目标检测
  * [Multitask AET With Orthogonal Tangent Regularity for Dark Object Detection](https://openaccess.thecvf.com/content/ICCV2021/papers/Cui_Multitask_AET_With_Orthogonal_Tangent_Regularity_for_Dark_Object_Detection_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/cuiziteng/ICCV_MAET)
* 3D object prediction
  * [Holistic Pose Graph: Modeling Geometric Structure among Objects in a Scene using Graph Inference for 3D Object Prediction](https://openaccess.thecvf.com/content/ICCV2021/papers/Xiao_Holistic_Pose_Graph_Modeling_Geometric_Structure_Among_Objects_in_a_ICCV_2021_paper.pdf)<br>:star:[code](https://vipl.ict.ac.cn/view_database.php?id=6)
* 多目标检测
  * [Training Multi-Object Detector by Estimating Bounding Box Distribution for Input Image](https://arxiv.org/abs/1911.12721)<br>:star:[code](https://github.com/yoojy31/MDOD)
* 3D object grounding
  * [Free-form Description Guided 3D Visual Graph Network for Object Grounding in Point Cloud](https://arxiv.org/abs/2103.16381)<br>:star:[code](https://github.com/PNXD/FFL-3DOG)


<a name="13"/>

## 13.Image Segmentation(图像分割)
* [Standardized Max Logits: A Simple yet Effective Approach for Identifying Unexpected Road Obstacles in Urban-Scene Segmentation](https://arxiv.org/abs/2107.11264)<br>:open_mouth:oral:star:[code](https://github.com/shjung13/Standardized-max-logits):tv:[video](https://www.youtube.com/watch?v=leBJZHzX6xM)
* [TransForensics: Image Forgery Localization with Dense Self-Attention](https://arxiv.org/abs/2108.03871)
* [From Contexts to Locality: Ultra-high Resolution Image Segmentation via Locality-aware Contextual Correlation](https://arxiv.org/abs/2109.02580)<br>:star:[code](https://github.com/liqiokkk/FCtL)
* [Labels4Free: Unsupervised Segmentation using StyleGAN](https://arxiv.org/abs/2103.14968)<br>:house:[project](https://rameenabdal.github.io/Labels4Free/):tv:[video](https://www.youtube.com/watch?v=_pHunGpvLVk)
* [Warp-Refine Propagation: Semi-Supervised Auto-labeling via Cycle-consistency](https://arxiv.org/abs/2109.13432)
* [Scaling up instance annotation via label propagation](https://arxiv.org/abs/2110.02277)<br>:star:[code](https://github.com/ethanweber/scaling-anno):house:[project](http://scaling-anno.csail.mit.edu/)
* [Robust Trust Region for Weakly Supervised Segmentation](https://arxiv.org/abs/2104.01948)<br>:star:[code](https://github.com/dmitrii-marin/robust_trust_region):tv:[video](https://drive.google.com/file/d/1MLd3c-fpm2K3hgYyWYFFxW3Ve8FznfD2/view)
* [HPNet: Deep Primitive Segmentation Using Hybrid Representations](https://arxiv.org/abs/2105.10620)<br>:star:[code](https://github.com/SimingYan/HPNet)
* [Weakly Supervised Segmentation of Small Buildings With Point Labels](https://openaccess.thecvf.com/content/ICCV2021/papers/Lee_Weakly_Supervised_Segmentation_of_Small_Buildings_With_Point_Labels_ICCV_2021_paper.pdf)
* [BAPA-Net: Boundary Adaptation and Prototype Alignment for Cross-Domain Semantic Segmentation](https://openaccess.thecvf.com/content/ICCV2021/papers/Liu_BAPA-Net_Boundary_Adaptation_and_Prototype_Alignment_for_Cross-Domain_Semantic_Segmentation_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/manmanjun/BAPA-Net)
* [Conditional Diffusion for Interactive Segmentation](https://openaccess.thecvf.com/content/ICCV2021/papers/Chen_Conditional_Diffusion_for_Interactive_Segmentation_ICCV_2021_paper.pdf)
* 全景分割
  * [Panoptic Narrative Grounding](https://arxiv.org/abs/2109.04988)
* 语义分割
  * [Re-distributing Biased Pseudo Labels for Semi-supervised Semantic Segmentation: A Baseline Investigation](https://arxiv.org/abs/2107.11279)<br>:open_mouth:oral:star:[code](https://github.com/CVMI-Lab/DARS)
  * [Personalized Image Semantic Segmentation](https://arxiv.org/abs/2107.13978)<br>:star:[code](https://github.com/zhangyuygss/PIS)
  * [RECALL: Replay-based Continual Learning in Semantic Segmentation](https://arxiv.org/abs/2108.03673)<br>:star:[code](https://github.com/LTTM/RECALL)
  * [Deep Metric Learning for Open World Semantic Segmentation](https://arxiv.org/abs/2108.04562)
  * [LabOR: Labeling Only if Required for Domain Adaptive Semantic Segmentation](https://arxiv.org/abs/2108.05570)<br>:open_mouth:oral
  * [Dual Path Learning for Domain Adaptation of Semantic Segmentation](https://arxiv.org/abs/2108.06337)<br>:star:[code](https://github.com/royee182/DPL)
  * [Multi-Target Adversarial Frameworks for Domain Adaptation in Semantic Segmentation](https://arxiv.org/abs/2108.06962)
  * [Exploiting a Joint Embedding Space for Generalized Zero-Shot Semantic Segmentation](https://arxiv.org/abs/2108.06536)<br>:star:[code](https://github.com/cvlab-yonsei/JoEm):house:[project](https://cvlab.yonsei.ac.kr/projects/JoEm/)
  * [Multi-Anchor Active Domain Adaptation for Semantic Segmentation](https://arxiv.org/abs/2108.08012)<br>:open_mouth:oral:star:[code](https://github.com/munanning/MADA)
  * [Pixel Contrastive-Consistent Semi-Supervised Semantic Segmentation](https://arxiv.org/abs/2108.09025)
  * [Self-Regulation for Semantic Segmentation](https://arxiv.org/abs/2108.09702)<br>:star:[code](https://github.com/dongzhang89/SR-SS)
  * [ShapeConv: Shape-aware Convolutional Layer for Indoor RGB-D Semantic Segmentation](https://arxiv.org/abs/2108.10528)<br>:star:[code](https://github.com/hanchaoleng/ShapeConv)
  * [Generalize then Adapt: Source-Free Domain Adaptive Semantic Segmentation](https://arxiv.org/abs/2108.11249)<br>:house:[project](https://sites.google.com/view/sfdaseg)
  * [Mining Contextual Information Beyond Image for Semantic Segmentation](https://arxiv.org/abs/2108.11819)<br>:star:[code](https://github.com/CharlesPikachu/mcibi)
  * [ISNet: Integrate Image-Level and Semantic-Level Context for Semantic Segmentation](https://arxiv.org/abs/2108.12382)<br>:star:[code](https://github.com/SegmentationBLWX/sssegmentation)
  * [Pseudo-mask Matters in Weakly-supervised Semantic Segmentation](https://arxiv.org/abs/2108.12995)
  * [SIGN: Spatial-information Incorporated Generative Network for Generalized Zero-shot Semantic Segmentation](https://arxiv.org/abs/2108.12517)
  * [Region-Aware Contrastive Learning for Semantic Segmentation](https://openaccess.thecvf.com/content/ICCV2021/papers/Hu_Region-Aware_Contrastive_Learning_for_Semantic_Segmentation_ICCV_2021_paper.pdf)
  * [GP-S3Net: Graph-Based Panoptic Sparse Semantic Segmentation Network](https://openaccess.thecvf.com/content/ICCV2021/papers/Razani_GP-S3Net_Graph-Based_Panoptic_Sparse_Semantic_Segmentation_Network_ICCV_2021_paper.pdf)
  * [Domain Adaptive Semantic Segmentation With Self-Supervised Depth Estimation](https://openaccess.thecvf.com/content/ICCV2021/papers/Wang_Domain_Adaptive_Semantic_Segmentation_With_Self-Supervised_Depth_Estimation_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/qinenergy/corda)
  * [Scribble-Supervised Semantic Segmentation by Uncertainty Reduction on Neural Representation and Self-Supervision on Neural Eigenspace](https://arxiv.org/abs/2102.09896)
  * [Exploring Cross-Image Pixel Contrast for Semantic Segmentation](https://arxiv.org/abs/2101.11939)<br>:open_mouth:oral:star:[code](https://github.com/tfzhou/ContrastiveSeg)
  * [Dynamic Divide-and-Conquer Adversarial Training for Robust Semantic Segmentation](https://arxiv.org/abs/2003.06555)<br>:star:[code](https://github.com/dvlab-research/Robust-Semantic-Segmentation)
  * [Uncertainty-Aware Pseudo Label Refinery for Domain Adaptive Semantic Segmentation](https://openaccess.thecvf.com/content/ICCV2021/papers/Wang_Uncertainty-Aware_Pseudo_Label_Refinery_for_Domain_Adaptive_Semantic_Segmentation_ICCV_2021_paper.pdf)
  * [Contrastive Learning for Label Efficient Semantic Segmentation](https://arxiv.org/abs/2012.06985)
  * 小样本语义分割
    * [Simpler is Better: Few-shot Semantic Segmentation with Classifier Weight Transformer](https://arxiv.org/abs/2108.03032)<br>:star:[code](https://github.com/zhiheLu/CWT-for-FSS)
    * [Learning Meta-class Memory for Few-Shot Semantic Segmentation](https://arxiv.org/abs/2108.02958)
    * [Few-Shot Semantic Segmentation With Cyclic Memory Network](https://openaccess.thecvf.com/content/ICCV2021/papers/Xie_Few-Shot_Semantic_Segmentation_With_Cyclic_Memory_Network_ICCV_2021_paper.pdf)
  * 3D语义分割
    * [VMNet: Voxel-Mesh Network for Geodesic-Aware 3D Semantic Segmentation](https://arxiv.org/abs/2107.13824)<br>:open_mouth:oral:star:[code](https://github.com/hzykent/VMNet)
    * [Sparse-to-Dense Feature Matching: Intra and Inter Domain Cross-Modal Learning in Domain Adaptation for 3D Semantic Segmentation](https://arxiv.org/abs/2107.14724)<br>:star:[code](https://github.com/leolyj/DsCML)
    * [Weakly Supervised 3D Semantic Segmentation Using Cross-Image Consensus and Inter-Voxel Affinity Relations](https://openaccess.thecvf.com/content/ICCV2021/papers/Zhu_Weakly_Supervised_3D_Semantic_Segmentation_Using_Cross-Image_Consensus_and_Inter-Voxel_ICCV_2021_paper.pdf)
  * 视频语义分割
    * [Domain Adaptive Video Segmentation via Temporal Consistency Regularization](https://arxiv.org/abs/2107.11004)<br>:star:[code](https://github.com/Dayan-Guan/DA-VSN)
  * 弱监督语义分割
    * [Leveraging Auxiliary Tasks with Affinity Learning for Weakly Supervised Semantic Segmentation](https://arxiv.org/abs/2107.11787)<br>:star:[code](https://github.com/xulianuwa/AuxSegNet)
    * [Complementary Patch for Weakly Supervised Semantic Segmentation](https://arxiv.org/abs/2108.03852)
    * [ECS-Net: Improving Weakly Supervised Semantic Segmentation by Using Connections Between Class Activation Maps](https://openaccess.thecvf.com/content/ICCV2021/papers/Sun_ECS-Net_Improving_Weakly_Supervised_Semantic_Segmentation_by_Using_Connections_Between_ICCV_2021_paper.pdf)
    * [Unlocking the Potential of Ordinary Classifier: Class-Specific Adversarial Erasing Framework for Weakly Supervised Semantic Segmentation](https://openaccess.thecvf.com/content/ICCV2021/papers/Kweon_Unlocking_the_Potential_of_Ordinary_Classifier_Class-Specific_Adversarial_Erasing_Framework_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/KAIST-vilab/OC-CSE)
    * [Context Decoupling Augmentation for Weakly Supervised Semantic Segmentation](https://arxiv.org/abs/2103.01795)<br>:star:[code](https://github.com/suyukun666/CDA)
    * [Seminar Learning for Click-Level Weakly Supervised Semantic Segmentation](https://arxiv.org/abs/2108.13393)
  * 点云语义分割
    * [ReDAL: Region-based and Diversity-aware Active Learning for Point Cloud Semantic Segmentation](https://arxiv.org/abs/2107.11769)<br>:tv:[video](https://www.youtube.com/watch?v=XJeb9kMxs5E)
    * [Perturbed Self-Distillation: Weakly Supervised Large-Scale Point Cloud Semantic Segmentation](https://openaccess.thecvf.com/content/ICCV2021/papers/Zhang_Perturbed_Self-Distillation_Weakly_Supervised_Large-Scale_Point_Cloud_Semantic_Segmentation_ICCV_2021_paper.pdf)
    * [TempNet: Online Semantic Segmentation on Large-Scale Point Cloud Series](https://openaccess.thecvf.com/content/ICCV2021/papers/Zhou_TempNet_Online_Semantic_Segmentation_on_Large-Scale_Point_Cloud_Series_ICCV_2021_paper.pdf)
    * [Guided Point Contrastive Learning for Semi-Supervised Point Cloud Semantic Segmentation](https://openaccess.thecvf.com/content/ICCV2021/papers/Jiang_Guided_Point_Contrastive_Learning_for_Semi-Supervised_Point_Cloud_Semantic_Segmentation_ICCV_2021_paper.pdf)
    * [Learning With Noisy Labels for Robust Point Cloud Segmentation](https://arxiv.org/abs/2107.14230)<br>:star:[code](https://github.com/pleaseconnectwifi/PNAL):house:[project](https://shuquanye.com/PNAL_website/)
  * OOD
    * [Entropy Maximization and Meta Classification for Out-of-Distribution Detection in Semantic Segmentation](https://arxiv.org/abs/2012.06575)<br>:star:[code](https://github.com/robin-chan/meta-ood)
* 实例分割
  * [Rank & Sort Loss for Object Detection and Instance Segmentation](https://arxiv.org/abs/2107.11669)<br>:open_mouth:oral:star:[code](https://github.com/kemaloksuz/RankSortLoss)
  * [SOTR: Segmenting Objects with Transformers](https://arxiv.org/abs/2108.06747)<br>:star:[code](https://github.com/easton-cau/SOTR)
  * [A Weakly Supervised Amodal Segmenter with Boundary Uncertainty Estimation](https://arxiv.org/abs/2108.09897)
  * [Instances as Queries](https://arxiv.org/abs/2105.01928)<br>:star:[code](https://github.com/hustvl/QueryInst):tv:[video](https://www.youtube.com/watch?v=3Fqwvn6_oUQ)
  * [CrossVIS: Crossover Learning for Fast Online Video Instance Segmentation](https://arxiv.org/abs/2104.05970)<br>:star:[code](https://github.com/hustvl/CrossVIS):tv:[video](https://www.youtube.com/watch?v=tPvYYjTgaNs)
  * [CDNet: Centripetal Direction Network for Nuclear Instance Segmentation](https://openaccess.thecvf.com/content/ICCV2021/papers/He_CDNet_Centripetal_Direction_Network_for_Nuclear_Instance_Segmentation_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/honglianghe/CDNet)
  * [PrimitiveNet: Primitive Instance Segmentation With Local Primitive Embedding Under Adversarial Metric](https://openaccess.thecvf.com/content/ICCV2021/papers/Huang_PrimitiveNet_Primitive_Instance_Segmentation_With_Local_Primitive_Embedding_Under_Adversarial_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/hjwdzh/PrimitiveNet)
  * [FASA: Feature Augmentation and Sampling Adaptation for Long-Tailed Instance Segmentation](https://arxiv.org/abs/2102.12867)<br>:star:[code](https://github.com/yuhangzang/FASA):house:[project](https://www.mmlab-ntu.com/project/fasa/index.html)
  * [Prior to Segment: Foreground Cues for Weakly Annotated Classes in Partially Supervised Instance Segmentation](http://arxiv.org/abs/2011.11787)<br>:star:[code](https://github.com/dbtmpl/OPMask)
  * [DiscoBox: Weakly Supervised Instance Segmentation and Semantic Correspondence From Box Supervision](https://arxiv.org/abs/2105.06464)
  * [End-to-End Video Instance Segmentation via Spatial-Temporal Graph Neural Networks](https://openaccess.thecvf.com/content/ICCV2021/papers/Wang_End-to-End_Video_Instance_Segmentation_via_Spatial-Temporal_Graph_Neural_Networks_ICCV_2021_paper.pdf)
  * [The Surprising Impact of Mask-Head Architecture on Novel Class Segmentation](https://arxiv.org/abs/2104.00613)<br>:house:[project](https://google.github.io/deepmac/)
  * 视频实例分割
    * [Video Instance Segmentation with a Propose-Reduce Paradigm](https://arxiv.org/abs/2103.13746)<br>:star:[code](https://github.com/dvlab-research/ProposeReduce)
  * 3D实例分割
    * [Hierarchical Aggregation for 3D Instance Segmentation](https://arxiv.org/abs/2108.02350)<br>:star:[code](https://github.com/hustvl/HAIS)
* 小样本分割
  * [Mining Latent Classes for Few-shot Segmentation](https://arxiv.org/abs/2103.15402)<br>:open_mouth:oral:star:[code](https://github.com/LiheYoung/MiningFSS)
* Human Motion Segmentation(人体运动分割)
  * [Graph Constrained Data Representation Learning for Human Motion Segmentation](https://arxiv.org/abs/2107.13362)<br>:star:[code](https://github.com/mdimiccoli/GCRL-for-HMS/)
  * [Hypercorrelation Squeeze for Few-Shot Segmenation](https://openaccess.thecvf.com/content/ICCV2021/papers/Min_Hypercorrelation_Squeeze_for_Few-Shot_Segmenation_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/juhongm999/hsnet):house:[project](http://cvlab.postech.ac.kr/research/HSNet/)
* 点云分割
  * [Learning with Noisy Labels for Robust Point Cloud Segmentation](https://arxiv.org/abs/2107.14230)<br>:open_mouth:oral:star:[code](https://github.com/pleaseconnectwifi/PNAL):house:[project](https://shuquanye.com/PNAL_website/)
  * [DRINet: A Dual-Representation Iterative Learning Network for Point Cloud Segmentation](https://arxiv.org/abs/2108.04023)
* 视频目标分割(VOS)
  * [Full-Duplex Strategy for Video Object Segmentation](https://arxiv.org/abs/2108.03151)<br>:house:[project](http://dpfan.net/FSNet/)
  * [Joint Inductive and Transductive Learning for Video Object Segmentation](https://arxiv.org/abs/2108.03679)<br>:star:[code](https://github.com/maoyunyao/JOINT)
  * [Hierarchical Memory Matching Network for Video Object Segmentation](https://arxiv.org/abs/2109.11404)<br>:star:[code](https://github.com/Hongje/HMMN)
  * [Self-supervised Video Object Segmentation by Motion Grouping](https://arxiv.org/abs/2104.07658)<br>:star:[code](https://github.com/charigyang/motiongrouping):house:[project](https://charigyang.github.io/motiongroup/):tv:[video](https://www.youtube.com/watch?v=Q0dLExLXZIw)
  * [Deep Transport Network for Unsupervised Video Object Segmentation](https://openaccess.thecvf.com/content/ICCV2021/papers/Zhang_Deep_Transport_Network_for_Unsupervised_Video_Object_Segmentation_ICCV_2021_paper.pdf)
  * [Generating Masks From Boxes by Mining Spatio-Temporal Consistencies in Videos](https://arxiv.org/abs/2101.02196)<br>:star:[code](https://github.com/visionml/pytracking)
* 语义场景分割 
  * [BiMaL: Bijective Maximum Likelihood Approach to Domain Adaptation in Semantic Scene Segmentation](https://arxiv.org/abs/2108.03267)<br>:star:[code](https://github.com/uark-cviu/BiMaL)
* Referring Segmentation(基于文本的分割) 
  * [Vision-Language Transformer and Query Generation for Referring Segmentation](https://arxiv.org/abs/2108.05565)<br>:star:[code](https://github.com/henghuiding/Vision-Language-Transformer)
* 场景理解
  * [DeepPanoContext: Panoramic 3D Scene Understanding with Holistic Scene Context Graph and Relation-based Optimization](https://arxiv.org/abs/2108.10743)
  * [ACDC: The Adverse Conditions Dataset With Correspondences for Semantic Driving Scene Understanding](https://openaccess.thecvf.com/content/ICCV2021/papers/Sakaridis_ACDC_The_Adverse_Conditions_Dataset_With_Correspondences_for_Semantic_Driving_ICCV_2021_paper.pdf)<br>:house:[project](https://acdc.vision.ee.ethz.ch/)
  * [Hypersim: A Photorealistic Synthetic Dataset for Holistic Indoor Scene Understanding](https://arxiv.org/abs/2011.02523)<br>:star:[code](https://github.com/apple/ml-hypersim)
* CMA  
  * [Towards Better Explanations of Class Activation Mapping](https://arxiv.org/abs/2102.05228)
  * [Keep CALM and Improve Visual Feature Attribution](https://arxiv.org/abs/2106.07861)<br>:star:[code](https://github.com/naver-ai/calm)
* 多目标分割
  * [Faster Multi-Object Segmentation Using Parallel Quadratic Pseudo-Boolean Optimization](https://openaccess.thecvf.com/content/ICCV2021/papers/Jeppesen_Faster_Multi-Object_Segmentation_Using_Parallel_Quadratic_Pseudo-Boolean_Optimization_ICCV_2021_paper.pdf)
* 动作分割
  * [Weakly-Supervised Action Segmentation and Alignment via Transcript-Aware Union-of-Subspaces Learning](https://openaccess.thecvf.com/content/ICCV2021/papers/Lu_Weakly-Supervised_Action_Segmentation_and_Alignment_via_Transcript-Aware_Union-of-Subspaces_Learning_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/ZijiaLewisLu/ICCV21-TASL)
* 场景解析
  * [Interaction via Bi-Directional Graph of Semantic Region Affinity for Scene Parsing](https://openaccess.thecvf.com/content/ICCV2021/papers/Ding_Interaction_via_Bi-Directional_Graph_of_Semantic_Region_Affinity_for_Scene_ICCV_2021_paper.pdf)

<a name="12"/>

## 12.Image/Fine-Grained Classification(图像/细粒度分类) 
* [DiagViB-6: A Diagnostic Benchmark Suite for Vision Models in the Presence of Shortcut and Generalization Opportunities](https://arxiv.org/abs/2108.05779)
* [Online Continual Learning For Visual Food Classification](https://arxiv.org/abs/2108.06781)
* [A Unified Objective for Novel Class Discovery](https://arxiv.org/abs/2108.08536)<br>:open_mouth:oral:star:[code](https://github.com/DonkeyShot21/UNO):house:[project](https://ncd-uno.github.io/)<br>:newspaper:解读:[ICCV2021 Oral | UNO：用于“新类发现”的统一目标函数，简化训练流程！已开源！](https://mp.weixin.qq.com/s/3aQ5AUKOAnO7kDtsxhRJ3Q)
* [Improving Generalization of Batch Whitening by Convolutional Unit Optimization](https://arxiv.org/abs/2108.10629)<br>:star:[code](https://github.com/YooshinCho/pytorch_ConvUnitOptimization)
* [Towards Learning Spatially Discriminative Feature Representations](https://arxiv.org/abs/2109.01359)
* [CrossViT: Cross-Attention Multi-Scale Vision Transformer for Image Classification](https://arxiv.org/abs/2103.14899)<br>:star:[code](https://github.com/IBM/CrossViT)<br>:newspaper:解读:[ICCV2021 MIT-IBM沃森开源CrossViT：Transformer走向多分支、多尺度](https://mp.weixin.qq.com/s/aqDaF4Iy96Nx1pvX__6vHg)
* [SCOUTER: Slot Attention-based Classifier for Explainable Image Recognition](https://arxiv.org/abs/2009.06138)<br>:star:[code](https://github.com/wbw520/scouter)
* [Influence-Balanced Loss for Imbalanced Visual Classification](https://arxiv.org/abs/2110.02444)<br>:star:[code](https://github.com/pseulki/IB-Loss) 
* [Explanations for Occluded Images](https://arxiv.org/abs/2103.03622)<br>:star:[code](https://github.com/theyoucheng/deepcover):house:[project](https://www.cprover.org/deepcover/iccv2021/):tv:[video](https://www.cprover.org/deepcover/iccv2021/iccv2021-talk-compatible.mp4)
* [Understanding Robustness of Transformers for Image Classification](http://arxiv.org/abs/2103.14586)
* [Learning Rare Category Classifiers on a Tight Labeling Budget](https://openaccess.thecvf.com/content/ICCV2021/papers/Mullapudi_Learning_Rare_Category_Classifiers_on_a_Tight_Labeling_Budget_ICCV_2021_paper.pdf)
* [Discover the Unknown Biased Attribute of an Image Classifier](https://arxiv.org/abs/2104.14556)<br>:star:[code](https://github.com/hubertlee915/discover_unknown_biases)
* [Co-Scale Conv-Attentional Image Transformers](https://arxiv.org/abs/2104.06399)<br>:open_mouth:oral:star:[code](https://github.com/mlpc-ucsd/CoaT)
* [Benchmark Platform for Ultra-Fine-Grained Visual Categorization Beyond Human Performance](https://openaccess.thecvf.com/content/ICCV2021/papers/Yu_Benchmark_Platform_for_Ultra-Fine-Grained_Visual_Categorization_Beyond_Human_Performance_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/XiaohanYu-GU/Ultra-FGVC)
* 长尾识别
  * [Parametric Contrastive Learning](https://arxiv.org/abs/2107.12028)<br>:star:[code](https://github.com/jiequancui/Parametric-Contrastive-Learning)
  * [ACE: Ally Complementary Experts for Solving Long-Tailed Recognition in One-Shot](https://arxiv.org/abs/2108.02385)<br>:open_mouth:oral:star:[code](https://github.com/jrcai/ACE)
  * [Self Supervision to Distillation for Long-Tailed Visual Recognition](https://arxiv.org/abs/2109.04075)<br>:star:[code](https://github.com/MCG-NJU)
  * [Distilling Virtual Examples for Long-Tailed Recognition](https://arxiv.org/abs/2103.15042)
  * [Distributional Robustness Loss for Long-Tail Learning](https://arxiv.org/abs/2104.03066)
* 细粒度
  * [Webly Supervised Fine-Grained Recognition: Benchmark Datasets and An Approach](https://arxiv.org/abs/2108.02399)<br>:star:[code](https://github.com/NUST-Machine-Intelligence-Laboratory/weblyFG-dataset)
  * [Learning Canonical 3D Object Representation for Fine-Grained Recognition](https://arxiv.org/abs/2108.04628)
  * [Counterfactual Attention Learning for Fine-Grained Visual Categorization and Re-identification](https://arxiv.org/abs/2108.08728)<br>:star:[code](https://github.com/raoyongming/CAL)
* 小样本分类
  * [Transductive Few-Shot Classification on the Oblique Manifold](https://arxiv.org/abs/2108.04009)
  * [Relational Embedding for Few-Shot Classification](https://arxiv.org/abs/2108.09666)
  * [Binocular Mutual Learning for Improving Few-shot Classification](https://arxiv.org/abs/2108.12104)<br>:star:[code](https://github.com/ZZQzzq/BML)
  * [Partner-Assisted Learning for Few-Shot Image Classification](https://arxiv.org/abs/2109.07607)
  * [On the Importance of Distractors for Few-Shot Classification](https://arxiv.org/abs/2109.09883)<br>:star:[code](https://github.com/quantacode/Contrastive-Finetuning)
  * [Few-Shot Image Classification: Just Use a Library of Pre-Trained Feature Extractors and a Simple Classifier](https://arxiv.org/abs/2101.00562)
  * [Universal Representation Learning From Multiple Domains for Few-Shot Classification](https://arxiv.org/abs/2103.13841)<br>:star:[code](https://github.com/VICO-UoE/URL)
  * [A Multi-Mode Modulator for Multi-Domain Few-Shot Classification](https://openaccess.thecvf.com/content/ICCV2021/papers/Liu_A_Multi-Mode_Modulator_for_Multi-Domain_Few-Shot_Classification_ICCV_2021_paper.pdf)
* 多标签分类
  * [Asymmetric Loss For Multi-Label Classification](https://arxiv.org/abs/2009.14119)<br>:star:[code](https://github.com/Alibaba-MIIL/ASL)
  * [Semantic Diversity Learning for Zero-Shot Multi-label Classification](https://arxiv.org/abs/2105.05926)<br>:star:[code](https://github.com/Alibaba-MIIL/ZS_SDL)

  
<a name="11"/>

## 11.Visual Question Answering(视觉问答)
* [Greedy Gradient Ensemble for Robust Visual Question Answering](https://arxiv.org/abs/2107.12651)<br>:star:[code](https://github.com/GeraldHan/GGE)
* [Weakly Supervised Relative Spatial Reasoning for Visual Question Answering](https://arxiv.org/abs/2109.01934)<br>:star:[code](https://github.com/pratyay-banerjee/weak_sup_vqa)
* [Calibrating Concepts and Operations: Towards Symbolic Reasoning on Real Images](https://arxiv.org/abs/2110.00519)<br>:star:[code](https://github.com/Lizw14/CaliCO)
* [Unshuffling Data for Improved Generalization in Visual Question Answering](https://openaccess.thecvf.com/content/ICCV2021/papers/Teney_Unshuffling_Data_for_Improved_Generalization_in_Visual_Question_Answering_ICCV_2021_paper.pdf)
* [TRAR: Routing the Attention Spans in Transformer for Visual Question Answering](https://openaccess.thecvf.com/content/ICCV2021/papers/Zhou_TRAR_Routing_the_Attention_Spans_in_Transformer_for_Visual_Question_ICCV_2021_paper.pdf)(https://github.com/rentainhe/TRAR-VQA/)
* [Contrast and Classify: Training Robust VQA Models](https://arxiv.org/abs/2010.06087)
* video question answering
  * [Just Ask: Learning to Answer Questions from Millions of Narrated Videos](https://arxiv.org/abs/2012.00451)<br>:open_mouth:oral:star:[code](https://github.com/antoyang/just-ask):house:[project](https://antoyang.github.io/just-ask.html)
  * [Adversarial VQA: A New Benchmark for Evaluating the Robustness of VQA Models](https://arxiv.org/abs/2106.00245)<br>:house:[project](https://adversarialvqa.github.io/)
  * [Env-QA: A Video Question Answering Benchmark for Comprehensive Understanding of Dynamic Environments](https://openaccess.thecvf.com/content/ICCV2021/papers/Gao_Env-QA_A_Video_Question_Answering_Benchmark_for_Comprehensive_Understanding_of_ICCV_2021_paper.pdf)<br>:sunflower:[dataset](http://vipl.ict.ac.cn/resources/envqa)
  * [On the Hidden Treasure of Dialog in Video Question Answering](https://arxiv.org/abs/2103.14517)<br>:star:[code](https://github.com/InterDigitalInc/DialogSummary-VideoQA):house:[project](https://engindeniz.github.io/dialogsummary-videoqa)
  * [HAIR: Hierarchical Visual-Semantic Relational Reasoning for Video Question Answering](https://openaccess.thecvf.com/content/ICCV2021/papers/Liu_HAIR_Hierarchical_Visual-Semantic_Relational_Reasoning_for_Video_Question_Answering_ICCV_2021_paper.pdf)
* A-VQA
  * [Pano-AVQA: Grounded Audio-Visual Question Answering on 360∘ Videos](https://arxiv.org/abs/2110.05122)

<a name="10"/>

## 10.OCR
* [Joint Visual Semantic Reasoning: Multi-Stage Decoder for Text Recognition](https://arxiv.org/abs/2107.12090)<br>:tv:[video](https://www.youtube.com/watch?v=GPk-O3ZqIoI)
* [Text is Text, No Matter What: Unifying Text Recognition using Knowledge Distillation](https://arxiv.org/abs/2107.12087)<br>:tv:[video](https://www.youtube.com/watch?v=8VLkaf_hGdQ)
* [Towards the Unseen: Iterative Text Recognition by Distilling from Errors](https://arxiv.org/abs/2107.12081)<br>:tv:[video](https://www.youtube.com/watch?v=ywaGXFZIiDI)
* 任意形状文本检测
  * [Adaptive Boundary Proposal Network for Arbitrary Shape Text Detection](https://arxiv.org/abs/2107.12664)<br>:star:[code](https://github.com/GXYM/TextBPN)
* 场景文本识别
  * [From Two to One: A New Scene Text Recognizer with Visual Language Modeling Network](https://arxiv.org/abs/2108.09661)<br>:star:[code](https://github.com/wangyuxin87/VisionLAN)
* 场景文本替换
  * [STRIVE: Scene Text Replacement In Videos](https://arxiv.org/abs/2109.02762)<br>:house:[project](https://striveiccv2021.github.io/STRIVE-ICCV2021/)  
* 提取文档图像
  * [End-to-End Piece-Wise Unwarping of Document Images](https://openaccess.thecvf.com/content/ICCV2021/papers/Das_End-to-End_Piece-Wise_Unwarping_of_Document_Images_ICCV_2021_paper.pdf)<br>:house:[project](https://sagniklp.github.io/PiecewiseUnwarp/)
  
<a name="9"/>

## 9.Video
* Action Detection and Recognition(人体动作检测与识别)
  * [Channel-wise Topology Refinement Graph Convolution for Skeleton-Based Action Recognition](https://arxiv.org/abs/2107.12213)<br>:star:[code](https://github.com/Uason-Chen/CTR-GCN)
  * [MGSampler: An Explainable Sampling Strategy for Video Action Recognition](https://arxiv.org/abs/2104.09952)
  * [Skeleton Cloud Colorization for Unsupervised 3D Action Representation Learning](https://arxiv.org/abs/2108.01959)
  * [Video Pose Distillation for Few-Shot, Fine-Grained Sports Action Recognition](https://arxiv.org/abs/2109.01305)
  * [Class Semantics-based Attention for Action Detection](https://arxiv.org/abs/2109.02613)
  * [MultiSports: A Multi-Person Video Dataset of Spatio-Temporally Localized Sports Actions](https://arxiv.org/pdf/2105.07404.pdf)<br>:star:[code](https://github.com/MCG-NJU/MultiSports)
  * [AdaSGN: Adapting Joint Number and Model Size for Efficient Skeleton-Based Action Recognition](https://arxiv.org/abs/2103.11770)<br>:star:[code](https://github.com/lshiwjx/AdaSGN)
  * [OadTR: Online Action Detection With Transformers](https://arxiv.org/abs/2106.11149)<br>:star:[code](https://github.com/wangxiang1230/OadTR)
  * [Self-Supervised 3D Skeleton Action Representation Learning With Motion Consistency and Continuity](https://openaccess.thecvf.com/content/ICCV2021/papers/Su_Self-Supervised_3D_Skeleton_Action_Representation_Learning_With_Motion_Consistency_and_ICCV_2021_paper.pdf)
  * [Interactive Prototype Learning for Egocentric Action Recognition](https://openaccess.thecvf.com/content/ICCV2021/papers/Wang_Interactive_Prototype_Learning_for_Egocentric_Action_Recognition_ICCV_2021_paper.pdf)
  * [Efficient Action Recognition via Dynamic Knowledge Propagation](https://openaccess.thecvf.com/content/ICCV2021/papers/Kim_Efficient_Action_Recognition_via_Dynamic_Knowledge_Propagation_ICCV_2021_paper.pdf)
  * [Else-Net: Elastic Semantic Network for Continual Action Recognition From Skeleton Data](https://openaccess.thecvf.com/content/ICCV2021/papers/Li_Else-Net_Elastic_Semantic_Network_for_Continual_Action_Recognition_From_Skeleton_ICCV_2021_paper.pdf)
  * [Learning Self-Similarity in Space and Time As Generalized Motion for Video Action Recognition](https://openaccess.thecvf.com/content/ICCV2021/papers/Kwon_Learning_Self-Similarity_in_Space_and_Time_As_Generalized_Motion_for_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/arunos728/SELFY):house:[project](http://cvlab.postech.ac.kr/research/SELFY/)
  * [Temporal Action Detection With Multi-Level Supervision](https://arxiv.org/abs/2011.11893)<br>:star:[code](https://github.com/bfshi/SSAD_OSAD)
  * 零样本动作识别
    * [Elaborative Rehearsal for Zero-shot Action Recognition](https://arxiv.org/abs/2108.02833)<br>:star:[code](https://github.com/DeLightCMU/ElaborativeRehearsal)
  * Temporal Action Localization(时序动作定位)
    * [Enriching Local and Global Contexts for Temporal Action Localization](https://arxiv.org/abs/2107.12960)<br>:star:[code](https://github.com/buxiangzhiren/ContextLoc)
    * [Learning Action Completeness from Points for Weakly-supervised Temporal Action Localization](https://arxiv.org/abs/2108.05029)<br>:open_mouth:oral:star:[code](https://github.com/Pilhyeon)
    * [Foreground-Action Consistency Network for Weakly Supervised Temporal Action Localization](https://arxiv.org/abs/2108.06524)<br>:star:[code](https://github.com/LeonHLJ/FAC-Net)
    * [Video Self-Stitching Graph Network for Temporal Action Localization](http://arxiv.org/abs/2011.14598)
  * Temporal Action Proposal Generation(时序动作提案生成)
    * [Relaxed Transformer Decoders for Direct Action Proposal Generation](https://arxiv.org/abs/2102.01894)<br>:star:[code](https://github.com/MCG-NJU/RTD-Action)
* Action Quality Assessment(行动质量评估)
  * [Group-aware Contrastive Regression for Action Quality Assessment](https://arxiv.org/abs/2108.07797)
* Video Rescaling
  * [Self-Conditioned Probabilistic Learning of Video Rescaling](https://arxiv.org/abs/2107.11639)
* Video activity localisation
  * [Cross-Sentence Temporal and Semantic Relations in Video Activity Localisation](https://arxiv.org/abs/2107.11443)<br>:newspaper:解读:[ICCV2021 | 如何高效视频定位？QMUL&北大&Adobe强强联手提出弱监督CRM，性能SOTA](https://mp.weixin.qq.com/s/tlGzpUU56HWjVqDtVOkdWg)
* 视频修复
  * [Internal Video Inpainting by Implicit Long-range Propagation](https://arxiv.org/abs/2108.01912)<br>:star:[code](https://github.com/Tengfei-Wang/Implicit-Internal-Video-Inpainting):house:[project](https://tengfei-wang.github.io/Implicit-Internal-Video-Inpainting/)
  * [Occlusion-Aware Video Object Inpainting](https://arxiv.org/abs/2108.06765)<br>:house:[project](http://www.kelei.site/voin/)
  * [FuseFormer: Fusing Fine-Grained Information in Transformers for Video Inpainting](https://arxiv.org/abs/2109.02974)<br>:star:[code](https://github.com/ruiliu-ai/FuseFormer)
  * [Flow-Guided Video Inpainting with Scene Templates](https://openaccess.thecvf.com/content/ICCV2021/papers/Lao_Flow-Guided_Video_Inpainting_With_Scene_Templates_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/donglao/videoinpainting)
  * [Frequency-Aware Spatiotemporal Transformers for Video Inpainting Detection](https://openaccess.thecvf.com/content/ICCV2021/papers/Yu_Frequency-Aware_Spatiotemporal_Transformers_for_Video_Inpainting_Detection_ICCV_2021_paper.pdf)
* 视频分析
  * 视频表征学习
    * [Enhancing Self-supervised Video Representation Learning via Multi-level Feature Optimization](https://arxiv.org/abs/2108.02183)
    * [Self-Supervised Video Representation Learning with Meta-Contrastive Network](https://arxiv.org/abs/2108.08426)
    * [Long Short View Feature Decomposition via Contrastive Video Representation Learning](https://arxiv.org/abs/2109.11593)
* 视频剪辑
  * [Learning to Cut by Watching Movies](https://arxiv.org/abs/2108.04294)<br>:star:[code](https://github.com/PardoAlejo/LearningToCut):house:[project](https://www.alejandropardo.net/publication/learning-to-cut/) 
* 视频字幕
  * [Motion Guided Region Message Passing for Video Captioning](https://openaccess.thecvf.com/content/ICCV2021/papers/Chen_Motion_Guided_Region_Message_Passing_for_Video_Captioning_ICCV_2021_paper.pdf)
  * Dense Video Captioning
    * [End-to-End Dense Video Captioning with Parallel Decoding](https://arxiv.org/abs/2108.07781)<br>:star:[code](https://github.com/ttengwang/PDVC)
* 视频编码
  * [Overfitting the Data: Compact Neural Video Delivery via Content-aware Feature Modulation](https://arxiv.org/abs/2108.08202)<br>:star:[code](https://github.com/Neural-video-delivery/CaFM-Pytorch-ICCV2021)<br>:newspaper:解读:[ICCV2021—工业界中的神经网络视频传输超分算法](https://mp.weixin.qq.com/s/dQCZaZNz0oCMHQQEdV79aA)
* 视频生成
  * [Click to Move: Controlling Video Generation with Sparse Motion](https://arxiv.org/abs/2108.08815)<br>:star:[code](https://github.com/PierfrancescoArdino/C2M)
* Video Relation Detection(视频关系检测)
  * [Social Fabric: Tubelet Compositions for Video Relation Detection](https://arxiv.org/abs/2108.08363)<br>:star:[code](https://github.com/shanshuo/Social-Fabric)
* Video Grounding
  * [Support-Set Based Cross-Supervision for Video Grounding](https://arxiv.org/abs/2108.10576)
* 视频精彩片段检测
  * [Cross-category Video Highlight Detection via Set-based Learning](https://arxiv.org/abs/2108.11770)<br>:star:[code](https://github.com/ChrisAllenMing/Cross_Category_Video_Highlight)
  * [PR-Net: Preference Reasoning for Personalized Video Highlight Detection](https://arxiv.org/abs/2109.01799)
  * [HighlightMe: Detecting Highlights from Human-Centric Videos](https://arxiv.org/abs/2110.01774)
  * [Temporal Cue Guided Video Highlight Detection With Low-Rank Audio-Visual Fusion](https://openaccess.thecvf.com/content/ICCV2021/papers/Ye_Temporal_Cue_Guided_Video_Highlight_Detection_With_Low-Rank_Audio-Visual_Fusion_ICCV_2021_paper.pdf)
* 视频识别
  * [Searching for Two-Stream Models in Multivariate Space for Video Recognition](https://arxiv.org/abs/2108.12957)
  * [Adaptive Focus for Efficient Video Recognition](https://arxiv.org/abs/2105.03245)<br>:open_mouth:oral:star:[code](https://github.com/blackfeather-wang/AdaFocus)
  * [AdaMML: Adaptive Multi-Modal Learning for Efficient Video Recognition](https://arxiv.org/abs/2105.05165)<br>:star:[code](https://github.com/IBM/AdaMML):house:[project](https://rpand002.github.io/adamml.html)
  * [TAM: Temporal Adaptive Module for Video Recognition](https://arxiv.org/abs/2005.06803)<br>:star:[code](https://github.com/liu-zhy/temporal-adaptive-module)
  * [Condensing a Sequence to One Informative Frame for Video Recognition](https://openaccess.thecvf.com/content/ICCV2021/papers/Qiu_Condensing_a_Sequence_to_One_Informative_Frame_for_Video_Recognition_ICCV_2021_paper.pdf)
  * [VideoLT: Large-Scale Long-Tailed Video Recognition](https://arxiv.org/abs/2105.02668)<br>:star:[code](https://github.com/17Skye17/VideoLT)
* Motion Retargeting(运动重定位)
  * [Contact-Aware Retargeting of Skinned Motion](https://arxiv.org/abs/2109.07431)<br>:tv:[video](https://www.youtube.com/watch?v=qQ4HO2Hibsk)
* 视频预测
  * [A Hierarchical Variational Neural Uncertainty Model for Stochastic Video Prediction](https://arxiv.org/abs/2110.03446)<br>:open_mouth:oral
* 视频合成
  * [iPOKE: Poking a Still Image for Controlled Stochastic Video Synthesis](https://arxiv.org/abs/2107.02790)<br>:star:[code](https://github.com/CompVis/ipoke):house:[project](https://bit.ly/3dJN4Lf)
* 视频帧插值
  * [Training Weakly Supervised Video Frame Interpolation With Events](https://openaccess.thecvf.com/content/ICCV2021/papers/Yu_Training_Weakly_Supervised_Video_Frame_Interpolation_With_Events_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/YU-Zhiyang/WEVI)
  * [Asymmetric Bilateral Motion Estimation for Video Frame Interpolation](https://arxiv.org/abs/2108.06815)<br>:star:[code](https://arxiv.org/abs/2108.06815)
* Deepfake 视频检测
  * [ID-Reveal: Identity-aware DeepFake Video Detection](https://openaccess.thecvf.com/content/ICCV2021/papers/Cozzolino_ID-Reveal_Identity-Aware_DeepFake_Video_Detection_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/grip-unina/id-reveal)
* 视频稳定
  * [Hybrid Neural Fusion for Full-Frame Video Stabilization](https://arxiv.org/abs/2102.06205)<br>:star:[code](https://github.com/alex04072000/FuSta):house:[project](https://alex04072000.github.io/FuSta/):tv:[video](https://youtu.be/KO3sULs4hso)
* Video Frame-level Similarity(视频帧级相似度学习)
  * [Rethinking Self-supervised Correspondence Learning: A Video Frame-level Similarity Perspective](https://arxiv.org/abs/2103.17263)<br>:open_mouth:oral:star:[code](https://github.com/xvjiarui/VFS/):house:[project](https://jerryxu.net/VFS/):tv:[video](https://youtu.be/H6cwwdf1p4I)
* 视频压缩
  * [Online-Trained Upsampler for Deep Low Complexity Video Compression](https://openaccess.thecvf.com/content/ICCV2021/papers/Klopp_Online-Trained_Upsampler_for_Deep_Low_Complexity_Video_Compression_ICCV_2021_paper.pdf)
* 视频时刻检索
  * [Fast Video Moment Retrieval](https://openaccess.thecvf.com/content/ICCV2021/papers/Gao_Fast_Video_Moment_Retrieval_ICCV_2021_paper.pdf)
* 视频摘要
  * [Multiple Pairwise Ranking Networks for Personalized Video Summarization](https://openaccess.thecvf.com/content/ICCV2021/papers/Saquil_Multiple_Pairwise_Ranking_Networks_for_Personalized_Video_Summarization_ICCV_2021_paper.pdf)
* 视频质量评估
  * [Unsupervised Curriculum Domain Adaptation for No-Reference Video Quality Assessment](https://openaccess.thecvf.com/content/ICCV2021/papers/Chen_Unsupervised_Curriculum_Domain_Adaptation_for_No-Reference_Video_Quality_Assessment_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/cpf0079/UCDA)
* Video Grounding
  * [STVGBert: A Visual-Linguistic Transformer Based Framework for Spatio-Temporal Video Grounding](https://openaccess.thecvf.com/content/ICCV2021/papers/Su_STVGBert_A_Visual-Linguistic_Transformer_Based_Framework_for_Spatio-Temporal_Video_Grounding_ICCV_2021_paper.pdf)
* 视频定位
  * [Zero-Shot Natural Language Video Localization](https://arxiv.org/abs/2110.00428)<br>:open_mouth:oral:star:[code](https://github.com/gistvision/PSVL)
* 视频推理
  * [Real-Time Video Inference on Edge Devices via Adaptive Model Streaming](https://openaccess.thecvf.com/content/ICCV2021/papers/Khani_Real-Time_Video_Inference_on_Edge_Devices_via_Adaptive_Model_Streaming_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/modelstreaming/ams)
* 视频相关
  * [Anonymizing Egocentric Videos](https://openaccess.thecvf.com/content/ICCV2021/papers/Thapar_Anonymizing_Egocentric_Videos_ICCV_2021_paper.pdf)
* 视频异常检测
  * [Dance With Self-Attention: A New Look of Conditional Random Fields on Anomaly Detection in Videos](https://openaccess.thecvf.com/content/ICCV2021/papers/Purwanto_Dance_With_Self-Attention_A_New_Look_of_Conditional_Random_Fields_ICCV_2021_paper.pdf)
* 视频去噪
  * [Unsupervised Deep Video Denoising](https://arxiv.org/abs/2011.15045)<br>:star:[code](https://github.com/sreyas-mohan/udvd):house:[project](https://sreyas-mohan.github.io/udvd/)
* Video Portrait Relighting(人像视频重照明)
  * [Neural Video Portrait Relighting in Real-time via Consistency Modeling](https://arxiv.org/abs/2104.00484)

<a name="8"/>

## 8.Human Pose Estimation(人体姿态估计)
* [Human Pose Regression with Residual Log-likelihood Estimation](https://arxiv.org/abs/2107.11291)<br>:open_mouth:oral:star:[code](https://github.com/Jeff-sjtu/res-loglikelihood-regression)
* [Online Knowledge Distillation for Efficient Pose Estimation](https://arxiv.org/abs/2108.02092)
* [DECA: Deep viewpoint-Equivariant human pose estimation using Capsule Autoencoders](https://arxiv.org/abs/2108.08557)<br>:open_mouth:oral:star:[code](https://github.com/mmlab-cv/DECA)
* [Estimating and Exploiting the Aleatoric Uncertainty in Surface Normal Estimation](https://arxiv.org/abs/2109.09881)<br>:open_mouth:oral:star:[code](https://github.com/baegwangbin/surface_normal_uncertainty)
* [Dynamical Pose Estimation](https://arxiv.org/abs/2103.06182)<br>:star:[code](https://github.com/hankyang94/DAMP):tv:[video](https://www.youtube.com/watch?v=S6L0h-d0IYM)
* 3D 人体姿态估计
  * [PyMAF: 3D Human Pose and Shape Regression with Pyramidal Mesh Alignment Feedback Loop](https://arxiv.org/abs/2103.16507)<br>:open_mouth:oral:star:[code](https://github.com/HongwenZhang/PyMAF):house:[project](https://hongwenzhang.github.io/pymaf/)
  * [HuMoR: 3D Human Motion Model for Robust Pose Estimation](https://arxiv.org/abs/2105.04668)<br>:open_mouth:oral:house:[project](https://geometry.stanford.edu/projects/humor/):tv:[video](https://youtu.be/5VWirxUHG0Y)
  * [Probabilistic Monocular 3D Human Pose Estimation with Normalizing Flows](https://arxiv.org/abs/2107.13788)<br>:star:[code](https://github.com/twehrbein/Probabilistic-Monocular-3D-Human-Pose-Estimation-with-Normalizing-Flows):tv:[video](https://www.youtube.com/watch?v=gaNX5CIl1L8)
  * [Learning Skeletal Graph Neural Networks for Hard 3D Pose Estimation](https://arxiv.org/abs/2108.07181)<br>:star:[code](https://github.com/ailingzengzzz/Skeletal-GNN)
  * [EventHPE: Event-based 3D Human Pose and Shape Estimation](https://arxiv.org/abs/2108.06819)<br>:star:[code](https://github.com/JimmyZou/EventHPE)
  * [imGHUM: Implicit Generative Models of 3D Human Shape and Articulated Pose](https://arxiv.org/abs/2108.10842)
  * [Graph-Based 3D Multi-Person Pose Estimation Using Multi-View Images](https://arxiv.org/abs/2109.05885)
  * [Unsupervised 3D Pose Estimation for Hierarchical Dance Video Recognition](https://arxiv.org/abs/2109.09166)
  * [Learning to Regress Bodies from Images using Differentiable Semantic Rendering](https://arxiv.org/abs/2110.03480)<br>:house:[project](https://dsr.is.tue.mpg.de/)
  * [Hierarchical Kinematic Probability Distributions for 3D Human Shape and Pose Estimation from Images in the Wild](https://arxiv.org/abs/2110.00990)<br>:star:[code](https://github.com/akashsengupta1997/HierarchicalProbabilistic3DHuman)
  * [3D Human Pose Estimation With Spatial and Temporal Transformers](https://arxiv.org/abs/2103.10455)<br>:star:[code](https://github.com/zczcwh/PoseFormer):tv:[video](https://www.youtube.com/watch?v=z8HWOdXjGR8)
  * [PARE: Part Attention Regressor for 3D Human Body Estimation](http://arxiv.org/abs/2104.08527)<br>:star:[code](https://github.com/mkocabas/PARE):house:[project](https://pare.is.tue.mpg.de/):tv:[video](https://youtu.be/3C9hdFajO3k)
  * [Learning Causal Representation for Training Cross-Domain Pose Estimator via Generative Interventions](https://openaccess.thecvf.com/content/ICCV2021/papers/Zhang_Learning_Causal_Representation_for_Training_Cross-Domain_Pose_Estimator_via_Generative_ICCV_2021_paper.pdf)
  * [UltraPose: Synthesizing Dense Pose With 1 Billion Points by Human-Body Decoupling 3D Model](https://openaccess.thecvf.com/content/ICCV2021/papers/Yan_UltraPose_Synthesizing_Dense_Pose_With_1_Billion_Points_by_Human-Body_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/MomoAILab/ultrapose)
  * [Modulated Graph Convolutional Network for 3D Human Pose Estimation](https://openaccess.thecvf.com/content/ICCV2021/papers/Zou_Modulated_Graph_Convolutional_Network_for_3D_Human_Pose_Estimation_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/ZhimingZo/Modulated-GCN)
  * [Revitalizing Optimization for 3D Human Pose and Shape Estimation: A Sparse Constrained Formulation](https://arxiv.org/abs/2105.13965)<br>:star:[code](https://github.com/fantaosha/SCOPE):house:[project](https://sites.google.com/view/scope-human/):tv:[video](https://sites.google.com/view/scope-human/home#h.j4u4ho36ixe8)
* 3D姿势迁移
  * [Unsupervised Geodesic-preserved Generative Adversarial Networks for Unconstrained 3D Pose Transfer](https://arxiv.org/abs/2108.07520)<br>:star:[code](https://github.com/mikecheninoulu/Unsupervised_IEPGAN)
* 手部姿势
  * 手势合成
    * [Speech Drives Templates: Co-Speech Gesture Synthesis with Learned Templates](https://arxiv.org/abs/2108.08020)(https://github.com/ShenhanQian/SpeechDrivesTemplates)
  * 手势识别
    * [Hand Image Understanding via Deep Multi-Task Learning](https://arxiv.org/abs/2107.11646)<br>:star:[code](https://github.com/MandyMo/HIU-DMTL)
    * [SemiHand: Semi-Supervised Hand Pose Estimation With Consistency](https://openaccess.thecvf.com/content/ICCV2021/papers/Yang_SemiHand_Semi-Supervised_Hand_Pose_Estimation_With_Consistency_ICCV_2021_paper.pdf)
  * 3D 手部姿态
    * [HandFoldingNet: A 3D Hand Pose Estimation Network Using Multiscale-Feature Guided Folding of a 2D Hand Skeleton](https://arxiv.org/abs/2108.05545)<br>:star:[code](https://github.com/cwc1260/HandFold)
    * [EventHands: Real-Time Neural 3D Hand Pose Estimation From an Event Stream](https://openaccess.thecvf.com/content/ICCV2021/papers/Rudnev_EventHands_Real-Time_Neural_3D_Hand_Pose_Estimation_From_an_Event_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/r00tman/EventHands):house:[project](https://4dqv.mpi-inf.mpg.de/EventHands/):tv:[video](https://youtu.be/jB1nkSYtblU)
  * 手部交互姿势估计
    * [End-to-End Detection and Pose Estimation of Two Interacting Hands](https://openaccess.thecvf.com/content/ICCV2021/papers/Kim_End-to-End_Detection_and_Pose_Estimation_of_Two_Interacting_Hands_ICCV_2021_paper.pdf)
  * 3D手网格建模
    * [I2UV-HandNet: Image-to-UV Prediction Network for Accurate and High-Fidelity 3D Hand Mesh Modeling](https://openaccess.thecvf.com/content/ICCV2021/papers/Chen_I2UV-HandNet_Image-to-UV_Prediction_Network_for_Accurate_and_High-Fidelity_3D_Hand_ICCV_2021_paper.pdf)
  * 手部网格恢复
    * [Self-Supervised Transfer Learning for Hand Mesh Recovery From Binocular Images](https://openaccess.thecvf.com/content/ICCV2021/papers/Chen_Self-Supervised_Transfer_Learning_for_Hand_Mesh_Recovery_From_Binocular_Images_ICCV_2021_paper.pdf)
  * 手势学习
    * [TravelNet: Self-Supervised Physically Plausible Hand Motion Learning From Monocular Color Images](https://openaccess.thecvf.com/content/ICCV2021/papers/Zhao_TravelNet_Self-Supervised_Physically_Plausible_Hand_Motion_Learning_From_Monocular_Color_ICCV_2021_paper.pdf)<br>:house:[project](https://www.yangangwang.com/#publication)
  * 手势重建
    * [Interacting Two-Hand 3D Pose and Shape Reconstruction from Single Color Image](https://www.yangangwang.com/papers/ZHANG-ITH-2021-08.pdf)<br>:star:[code](https://github.com/BaowenZ/Intershape)
* 三维网格合成
  * [Deep Hybrid Self-Prior for Full 3D Mesh Generation](https://arxiv.org/abs/2108.08017)<br>:house:[project](https://yqdch.github.io/DHSP3D/)
* 人体重建
  * [ARCH++: Animation-Ready Clothed Human Reconstruction Revisited](https://arxiv.org/abs/2108.07845)<br>:tv:[video](https://www.youtube.com/watch?v=kNtlheGLSR8)
  * 3D 人体重建
    * [Probabilistic Modeling for Human Mesh Recovery](https://arxiv.org/abs/2108.11944)<br>:star:[code](https://github.com/nkolot/ProHMR):house:[project](https://www.seas.upenn.edu/~nkolot/projects/prohmr/)
    * [Gravity-Aware Monocular 3D Human-Object Reconstruction](https://arxiv.org/abs/2108.08844)<br>:house:[project](http://4dqv.mpi-inf.mpg.de/GraviCap/)
    * [THUNDR: Transformer-Based 3D Human Reconstruction With Markers](https://arxiv.org/abs/2106.09336)
* 4D人体捕捉
  * [Learning Motion Priors for 4D Human Body Capture in 3D Scenes](https://arxiv.org/abs/2108.10399)<br>:star:[code](https://github.com/sanweiliti/LEMO):house:[project](https://sanweiliti.github.io/LEMO/LEMO.html):tv:[video](https://youtu.be/ly8UaeFqFhw) 
* 人体姿态估计与合成
  * [Physics-based Human Motion Estimation and Synthesis from Videos](https://arxiv.org/abs/2109.09913)
* 多人姿态估计 
  * [Shape-aware Multi-Person Pose Estimation from Multi-View Images](https://arxiv.org/abs/2110.02330)<br>:star:[code](https://github.com/zj-dong/Multi-Person-Pose-Estimation):house:[project](https://ait.ethz.ch/projects/2021/multi-human-pose/):tv:[video](https://www.youtube.com/watch?v=KE5Jpnyqmh4)<br>论文公开
  * [The Center of Attention: Center-Keypoint Grouping via Attention for Multi-Person Pose Estimation](https://arxiv.org/abs/2110.05132)<br>:star:[code](https://github.com/dvl-tum)
* 人/物体姿态关键点检测
  * [Keypoint Communities](https://arxiv.org/abs/2110.00988)<br>:star:[code](https://github.com/DuncanZauss/Keypoint_Communities)
* 人体运动捕捉
  * [SOMA: Solving Optical Marker-Based MoCap Automatically](https://arxiv.org/abs/2110.04431)<br>:star:[code](https://github.com/nghorbani/soma):house:[project](https://soma.is.tue.mpg.de/):tv:[video](https://youtu.be/BEFCqIefLA8)
  * [DeepMultiCap: Performance Capture of Multiple Characters Using Sparse Multiview Cameras](https://arxiv.org/abs/2105.00261)<br>:house:[project](http://liuyebin.com/dmc/dmc.html)
* 2D人体姿势估计
  * [An Empirical Study of the Collapsing Problem in Semi-Supervised 2D Human Pose Estimation](https://arxiv.org/abs/2011.12498)<br>:star:[code](https://github.com/xierc/Semi_Human_Pose)
* Human Action Video Alignment
  * [Normalized Human Pose Features for Human Action Video Alignment](https://openaccess.thecvf.com/content/ICCV2021/papers/Liu_Normalized_Human_Pose_Features_for_Human_Action_Video_Alignment_ICCV_2021_paper.pdf)
* 3D姿态迁移
  * [Intrinsic-Extrinsic Preserved GANs for Unsupervised 3D Pose Transfer](https://arxiv.org/abs/2108.07520)<br>:star:[code](https://github.com/mikecheninoulu/Unsupervised_IEPGAN)
* 人体网格恢复
  * [Skeleton2Mesh: Kinematics Prior Injected Unsupervised Human Mesh Recovery](https://openaccess.thecvf.com/content/ICCV2021/papers/Yu_Skeleton2Mesh_Kinematics_Prior_Injected_Unsupervised_Human_Mesh_Recovery_ICCV_2021_paper.pdf)<br>:house:[project](https://sites.google.com/view/skeleton2mesh)
  * [Uncertainty-Aware Human Mesh Recovery From Video by Learning Part-Based 3D Dynamics](https://openaccess.thecvf.com/content/ICCV2021/papers/Lee_Uncertainty-Aware_Human_Mesh_Recovery_From_Video_by_Learning_Part-Based_3D_ICCV_2021_paper.pdf)
* 根据人体姿势估计距离
  * [Single View Physical Distance Estimation using Human Pose](https://arxiv.org/abs/2106.10335)<br>:house:[project](https://feixh.github.io/projects/physical_distance/)
* 3D人体
  * [SNARF: Differentiable Forward Skinning for Animating Non-Rigid Neural Implicit Shapes](https://arxiv.org/abs/2104.03953)<br>:star:[code](https://github.com/xuchen-ethz/snarf):house:[project](https://xuchen-ethz.github.io/snarf):tv:[video](https://www.youtube.com/watch?v=rCEpFTKjFHE)
* 3D人体运动合成
  * [Action-Conditioned 3D Human Motion Synthesis With Transformer VAE](https://openaccess.thecvf.com/content/ICCV2021/papers/Petrovich_Action-Conditioned_3D_Human_Motion_Synthesis_With_Transformer_VAE_ICCV_2021_paper.pdf)

<a name="7"/>

## 7.Scene Graph Generation(场景图生成)
* [Spatial-Temporal Transformer for Dynamic Scene Graph Generation](https://arxiv.org/abs/2107.12309)<br>:star:[code](https://github.com/yrcong/STTran):tv:[video](https://www.youtube.com/watch?v=6D3ExjQpbjQ&feature=youtu.be)
* [Unconditional Scene Graph Generation](https://arxiv.org/abs/2108.05884)<br>:house:[project](https://scenegraphgen.github.io/)
* [Target Adaptive Context Aggregation for Video Scene Graph Generation](https://arxiv.org/abs/2108.08121)<br>:star:[code](https://github.com/MCG-NJU/TRACE)
* [Learning to Generate Scene Graph from Natural Language Supervision](https://arxiv.org/pdf/2109.02227)<br>:star:[code](https://github.com/YiwuZhong/SGG_from_NLS)
* [Segmentation-Grounded Scene Graph Generation](https://arxiv.org/abs/2104.14207)<br>:star:[code](https://github.com/ubc-vision/segmentation-sg)
* [Context-aware Scene Graph Generation with Seq2Seq Transformer](https://openaccess.thecvf.com/content/ICCV2021/papers/Lu_Context-Aware_Scene_Graph_Generation_With_Seq2Seq_Transformers_ICCV_2021_paper.pdf)<br>:star:[code]( https://github.com/layer6ai-labs/SGG-Seq2Seq)
* [A Simple Baseline for Weakly-Supervised Scene Graph Generation](https://openaccess.thecvf.com/content/ICCV2021/papers/Shi_A_Simple_Baseline_for_Weakly-Supervised_Scene_Graph_Generation_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/jshi31/WS-SGG)
* 场景合成
  * [Self-Supervised Real-to-Sim Scene Generation](https://arxiv.org/abs/2011.14488)<br>:house:[project](https://research.nvidia.com/publication/2021-08_Sim2SG)
  * [Unconstrained Scene Generation With Locally Conditioned Radiance Fields](https://arxiv.org/abs/2104.00670)<br>:star:[code](https://apple.github.io/ml-gsn/):house:[project](https://github.com/apple/ml-gsn)
  * [Visual Distant Supervision for Scene Graph Generation](https://arxiv.org/abs/2103.15365)<br>:star:[code](https://github.com/thunlp/VisualDS)

<a name="6"/>

## 6.Point Cloud(点云)
* [AdaFit: Rethinking Learning-based Normal Estimation on Point Clouds](https://arxiv.org/abs/2108.05836)<br>:star:[code](https://github.com/Runsong123/AdaFit):house:[project](https://runsong123.github.io/AdaFit/)
* [Adaptive Graph Convolution for Point Cloud Analysis](https://arxiv.org/abs/2108.08035)<br>:star:[code](https://github.com/hrzhou2/AdaptConv-master)
* [Learning Inner-Group Relations on Point Clouds](https://arxiv.org/abs/2108.12468)
* [CPFN: Cascaded Primitive Fitting Networks for High-Resolution Point Clouds](https://arxiv.org/abs/2109.00113)<br>:star:[code](https://github.com/erictuanle/CPFN)
* [Cloud Transformers: A Universal Approach To Point Cloud Processing Tasks](https://arxiv.org/abs/2007.11679)<br>:star:[code](https://github.com/saic-vul/cloud_transformers):tv:[video](https://www.youtube.com/watch?v=lYTzLhy-ybw) 
* [PCAM: Product of Cross-Attention Matrices for Rigid Registration of Point Clouds](https://arxiv.org/abs/2110.01269)<br>:star:[code](https://github.com/valeoai/PCAM)
* [3DVG-Transformer: Relation Modeling for Visual Grounding on Point Clouds](https://openaccess.thecvf.com/content/ICCV2021/papers/Zhao_3DVG-Transformer_Relation_Modeling_for_Visual_Grounding_on_Point_Clouds_ICCV_2021_paper.pdf)
* [Differentiable Convolution Search for Point Cloud Processing](http://arxiv.org/abs/2108.12856)
* [Superpoint Network for Point Cloud Oversegmentation](https://openaccess.thecvf.com/content/ICCV2021/papers/Hui_Superpoint_Network_for_Point_Cloud_Oversegmentation_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/fpthink/SPNet)
* [PU-EVA: An Edge-Vector Based Approximation Solution for Flexible-Scale Point Cloud Upsampling](https://openaccess.thecvf.com/content/ICCV2021/papers/Luo_PU-EVA_An_Edge-Vector_Based_Approximation_Solution_for_Flexible-Scale_Point_Cloud_ICCV_2021_paper.pdf)
* [SGMNet: Learning Rotation-Invariant Point Cloud Representations via Sorted Gram Matrix](https://openaccess.thecvf.com/content/ICCV2021/papers/Xu_SGMNet_Learning_Rotation-Invariant_Point_Cloud_Representations_via_Sorted_Gram_Matrix_ICCV_2021_paper.pdf)
* [DWKS: A Local Descriptor of Deformations Between Meshes and Point Clouds](https://openaccess.thecvf.com/content/ICCV2021/papers/Magnet_DWKS_A_Local_Descriptor_of_Deformations_Between_Meshes_and_Point_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/RobinMagnet/DWKS)
* 点云去噪
  * [Score-Based Point Cloud Denoising](https://arxiv.org/abs/2107.10981)<br>:star:[code](https://github.com/luost26/score-denoise)
* 点云配准
  * [HRegNet: A Hierarchical Network for Large-scale Outdoor LiDAR Point Cloud Registration](https://arxiv.org/abs/2107.11992)<br>:star:[code](https://github.com/ispc-lab/HRegNet):house:[project](https://ispc-group.github.io/hregnet)
  * [(Just) A Spoonful of Refinements Helps the Registration Error Go Down](https://arxiv.org/abs/2108.03257https://arxiv.org/abs/2108.03257)<br>:open_mouth:oral:star:[code](https://github.com/SergioRAgostinho/just-a-spoonful)
  * [A Robust Loss for Point Cloud Registration](https://arxiv.org/abs/2108.11682)
  * [Deep Hough Voting for Robust Global Registration](https://arxiv.org/abs/2109.04310)
  * [Sampling Network Guided Cross-Entropy Method for Unsupervised Point Cloud Registration](https://arxiv.org/abs/2109.06619)<br>:star:[code](https://github.com/Jiang-HB/CEMNet)
  * [Feature Interactive Representation for Point Cloud Registration](https://openaccess.thecvf.com/content/ICCV2021/papers/Wu_Feature_Interactive_Representation_for_Point_Cloud_Registration_ICCV_2021_paper.pdf)
  * [LSG-CPD: Coherent Point Drift With Local Surface Geometry for Point Cloud Registration](https://openaccess.thecvf.com/content/ICCV2021/papers/Liu_LSG-CPD_Coherent_Point_Drift_With_Local_Surface_Geometry_for_Point_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/ChirikjianLab/LSG-CPD):tv:[video](https://www.youtube.com/watch?v=1lxz9Uu-GXI)
  * [OMNet: Learning Overlapping Mask for Partial-to-Partial Point Cloud Registration](https://arxiv.org/abs/2103.00937)<br>:star:[code](https://github.com/megvii-research/OMNet)
  * [DeepPRO: Deep Partial Point Cloud Registration of Objects](https://openaccess.thecvf.com/content/ICCV2021/papers/Lee_DeepPRO_Deep_Partial_Point_Cloud_Registration_of_Objects_ICCV_2021_paper.pdf)
  * [Provably Approximated Point Cloud Registration](https://openaccess.thecvf.com/content/ICCV2021/papers/Jubran_Provably_Approximated_Point_Cloud_Registration_ICCV_2021_paper.pdf)
  * [Bootstrap Your Own Correspondences](https://arxiv.org/abs/2106.00677)点云配准
* 3D点云
  * [Unsupervised Learning of Fine Structure Generation for 3D Point Clouds by 2D Projection Matching](https://arxiv.org/abs/2108.03746)<br>:star:[code](https://github.com/chenchao15/2D_projection_matching)
  * [Spatio-temporal Self-Supervised Representation Learning for 3D Point Clouds](https://arxiv.org/abs/2109.00179)<br>:star:[code](https://github.com/yichen928/STRL):house:[project](https://siyuanhuang.com/STRL/)
  * [Unsupervised Learning of Fine Structure Generation for 3D Point Clouds by 2D Projections Matching](https://arxiv.org/abs/2108.03746)<br>:star:[code](https://github.com/chenchao15/2D_projection_matching)
* 点云补全
  * [SnowflakeNet: Point Cloud Completion by Snowflake Point Deconvolution with Skip-Transformer](https://arxiv.org/abs/2108.04444)<br>:open_mouth:oral:star:[code](https://github.com/AllenXiangX/SnowflakeNet)
  * [ME-PCN: Point Completion Conditioned on Mask Emptiness](https://arxiv.org/abs/2108.08187)
  * [PoinTr: Diverse Point Cloud Completion with Geometry-Aware Transformers](https://arxiv.org/abs/2108.08839)<br>:open_mouth:oral:star:[code](https://github.com/yuxumin/PoinTr)
  * [Voxel-based Network for Shape Completion by Leveraging Edge Generation](https://arxiv.org/abs/2108.09936)<br>:star:[code](https://github.com/xiaogangw/VE-PCN)
  * [RFNet: Recurrent Forward Network for Dense Point Cloud Completion](https://arxiv.org/abs/2104.00820)
* 点云增强
  * [Point Cloud Augmentation with Weighted Local Transformations](https://arxiv.org/abs/2110.05379)<br>:star:[code](https://github.com/mlvlab/PointWOLF)
* 点云形状分析
  * [Walk in the Cloud: Learning Curves for Point Clouds Shape Analysis](https://arxiv.org/abs/2105.01288)<br>:star:[code](https://github.com/tiangexiang/CurveNet):house:[project](https://curvenet.github.io/)
* 点云分析
  * [A Closer Look at Rotation-Invariant Deep Point Cloud Analysis](https://openaccess.thecvf.com/content/ICCV2021/papers/Li_A_Closer_Look_at_Rotation-Invariant_Deep_Point_Cloud_Analysis_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/SILI1994/rotation-invariant-pointcloud-analysis)
* 3D点云分类
  * [A Backdoor Attack Against 3D Point Cloud Classifiers](http://arxiv.org/abs/2104.05808)<br>:star:[code](https://github.com/zhenxianglance/PCBA)
* 3D点云生成与补全
  * [3D Shape Generation and Completion through Point-Voxel Diffusion](https://arxiv.org/abs/2104.03670)<br>:house:[project](https://alexzhou907.github.io/pvd):tv:[video](https://youtu.be/64jl79i6HNY)


<a name="5"/>

## 5.Few-Shot/Zero-Shot Learning;Domain Generalization/Adaptation(小/零样本学习;域适应/泛化)
* 域适应
  * [Transporting Causal Mechanisms for Unsupervised Domain Adaptation](https://arxiv.org/abs/2107.11055)<br>:open_mouth:oral<br>:star:[code](https://github.com/yue-zhongqi/tcm)
  * [Generalized Source-free Domain Adaptation](https://arxiv.org/abs/2108.01614)<br>:star:[code](https://github.com/Albert0147/G-SFDA)
  * [Semantic Concentration for Domain Adaptation](https://arxiv.org/abs/2108.05720)<br>:star:[code](https://github.com/BIT-DA)
  * [PIT: Position-Invariant Transform for Cross-FoV Domain Adaptation](https://arxiv.org/abs/2108.07142)<br>:star:[code](https://github.com/sheepooo/PIT-Position-Invariant-Transform)
  * [Learning Cross-modal Contrastive Features for Video Domain Adaptation](https://arxiv.org/abs/2108.11974)
  * [Zero-Shot Day-Night Domain Adaptation With a Physics Prior](https://openaccess.thecvf.com/content/ICCV2021/papers/Lengyel_Zero-Shot_Day-Night_Domain_Adaptation_With_a_Physics_Prior_ICCV_2021_paper.pdf)<br>:open_mouth:oral:star:[code](https://github.com/Attila94/CIConv)
  * [Active Universal Domain Adaptation](https://openaccess.thecvf.com/content/ICCV2021/papers/Ma_Active_Universal_Domain_Adaptation_ICCV_2021_paper.pdf)
  * [Re-energizing Domain Discriminator with Sample Relabeling for Adversarial Domain Adaptation](https://arxiv.org/abs/2103.11661)
  * [OVANet: One-vs-All Network for Universal Domain Adaptation](https://arxiv.org/abs/2104.03344)<br>:star:[code](https://github.com/VisionLearningGroup/OVANet)
  * [Collaborative Optimization and Aggregation for Decentralized Domain Generalization and Adaptation](https://openaccess.thecvf.com/content/ICCV2021/papers/Wu_Collaborative_Optimization_and_Aggregation_for_Decentralized_Domain_Generalization_and_Adaptation_ICCV_2021_paper.pdf)
  * [Partial Video Domain Adaptation with Partial Adversarial Temporal Attentive Network](https://arxiv.org/abs/2107.04941)<br>:star:[code](https://xuyu0010.github.io/pvda.html)
  * [Information-Theoretic Regularization for Multi-Source Domain Adaptation](https://arxiv.org/abs/2104.01568)
  * [Gradient Distribution Alignment Certificates Better Adversarial Domain Adaptation](https://openaccess.thecvf.com/content/ICCV2021/papers/Gao_Gradient_Distribution_Alignment_Certificates_Better_Adversarial_Domain_Adaptation_ICCV_2021_paper.pdf)
  * 无监督域适应
    * [Adversarial Unsupervised Domain Adaptation with Conditional and Label Shift: Infer, Align and Iterate](https://arxiv.org/abs/2107.13469)
    * [Recursively Conditional Gaussian for Ordinal Unsupervised Domain Adaptation](https://arxiv.org/abs/2107.13467)<br>:open_mouth:oral
    * [Tune it the Right Way: Unsupervised Validation of Domain Adaptation via Soft Neighborhood Density](https://arxiv.org/abs/2108.10860)
    * [Adversarial Robustness for Unsupervised Domain Adaptation](https://arxiv.org/abs/2109.00946)<br>:house:[project](http://deepawais.com/robust_uda/)
  * 零样本域适应
    * [Zero-Shot Domain Adaptation with a Physics Prior](https://arxiv.org/abs/2108.05137)<br>:open_mouth:oral:star:[code](https://github.com/Attila94/CIConv)
    * [Collaborative Learning with Disentangled Features for Zero-shot Domain Adaptation](https://openaccess.thecvf.com/content/ICCV2021/papers/Jhoo_Collaborative_Learning_With_Disentangled_Features_for_Zero-Shot_Domain_Adaptation_ICCV_2021_paper.pdf)
* 域泛化
  * [Domain Generalization via Gradient Surgery](https://arxiv.org/abs/2108.01621)<br>:star:[code](https://github.com/lucasmansilla/DGvGS)
  * [Learning to Diversify for Single Domain Generalization](https://arxiv.org/abs/2108.11726)<br>:star:[code](https://github.com/BUserName/Learning_to_diversify)
  * [Shape-Biased Domain Generalization via Shock Graph Embeddings](https://arxiv.org/abs/2109.05671)
  * [SelfReg: Self-Supervised Contrastive Regularization for Domain Generalization](https://openaccess.thecvf.com/content/ICCV2021/papers/Kim_SelfReg_Self-Supervised_Contrastive_Regularization_for_Domain_Generalization_ICCV_2021_paper.pdf)
  * [A Style and Semantic Memory Mechanism for Domain Generalization](https://openaccess.thecvf.com/content/ICCV2021/papers/Chen_A_Style_and_Semantic_Memory_Mechanism_for_Domain_Generalization_ICCV_2021_paper.pdf)
* 小样本
  * [Boosting the Generalization Capability in Cross-Domain Few-shot Learning via Noise-enhanced Supervised Autoencoder](https://arxiv.org/abs/2108.05028)
  * [Generalized and Incremental Few-Shot Learning by Explicit Learning and Calibration without Forgetting](https://arxiv.org/abs/2108.08165)<br>:star:[code](https://github.com/annusha/LCwoF)
  * [Meta Navigator: Search for a Good Adaptation Policy for Few-shot Learning](https://arxiv.org/abs/2109.05749)
  * [Meta-Learning with Task-Adaptive Loss Function for Few-Shot Learning](https://arxiv.org/abs/2110.03909)<br>:open_mouth:oral:star:[code](https://github.com/baiksung)
  * [Z-Score Normalization, Hubness, and Few-Shot Learning](https://openaccess.thecvf.com/content/ICCV2021/papers/Fei_Z-Score_Normalization_Hubness_and_Few-Shot_Learning_ICCV_2021_paper.pdf)
  * [Pseudo-Loss Confidence Metric for Semi-Supervised Few-Shot Learning](https://openaccess.thecvf.com/content/ICCV2021/papers/Huang_Pseudo-Loss_Confidence_Metric_for_Semi-Supervised_Few-Shot_Learning_ICCV_2021_paper.pdf)
  * [Curvature Generation in Curved Spaces for Few-Shot Learning](https://openaccess.thecvf.com/content/ICCV2021/papers/Gao_Curvature_Generation_in_Curved_Spaces_for_Few-Shot_Learning_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/ZhiGaomcislab/CurvatureGeneration_FSL)
  * 小样本异常检测
    * [A Hierarchical Transformation-Discriminating Generative Model for Few Shot Anomaly Detection](https://arxiv.org/abs/2104.14535)
* Zero-Shot Learning(零样本学习)
  * [Discriminative Region-based Multi-Label Zero-Shot Learning](https://arxiv.org/abs/2108.09301)<br>:star:[code](https://github.com/akshitac8/BiAM)
  * [Field-Guide-Inspired Zero-Shot Learning](https://arxiv.org/abs/2108.10967)
  * Generalized Zero-Shot Learning(广义零样本学习)
    * [FREE: Feature Refinement for Generalized Zero-Shot Learning](https://arxiv.org/abs/2107.13807)<br>:star:[code](https://github.com/shiming-chen/FREE)
    * [Semantics Disentangling for Generalized Zero-Shot Learning](https://arxiv.org/abs/2101.07978)<br>:star:[code](https://github.com/uqzhichen/SDGZSL)
 
<a name="4"/>

## 4.Neural rendering(神经渲染)
* [In-Place Scene Labelling and Understanding with Implicit Scene Representation](https://arxiv.org/abs/2103.15875)<br>:open_mouth:oral:house:[project](https://shuaifengzhi.com/Semantic-NeRF/):tv:[video](https://www.youtube.com/watch?v=FpShWO7LVbM)
* [Differentiable Surface Rendering via Non-Differentiable Sampling](https://arxiv.org/abs/2108.04886)
* [Self-Calibrating Neural Radiance Fields](https://arxiv.org/abs/2108.13826)<br>:star:[code](https://github.com/POSTECH-CVLab/SCNeRF)
* [NerfingMVS: Guided Optimization of Neural Radiance Fields for Indoor Multi-view Stereo](https://arxiv.org/abs/2109.01129)<br>:open_mouth:oral:star:[code](https://github.com/weiyithu/NerfingMVS):house:[project](https://weiyithu.github.io/NerfingMVS/)
* [Learning Object-Compositional Neural Radiance Field for Editable Scene Rendering](https://arxiv.org/abs/2109.01847)<br>:star:[code](https://github.com/zju3dv/object_nerf):house:[project](https://zju3dv.github.io/object_nerf/)
* [CodeNeRF: Disentangled Neural Radiance Fields for Object Categories](https://arxiv.org/abs/2109.01750)<br>:star:[code](https://github.com/wayne1123/code-nerf)
* [MVSNeRF: Fast Generalizable Radiance Field Reconstruction from Multi-View Stereo](https://arxiv.org/abs/2103.15595)<br>:star:[code](https://github.com/apchenstu/mvsnerf):house:[project](https://apchenstu.github.io/mvsnerf/):tv:[video](https://youtu.be/3M3edNiaGsA)
* [PlenOctrees for Real-Time Rendering of Neural Radiance Fields](https://arxiv.org/abs/2103.14024)<br>:open_mouth:oral:star:[Conversion Code](https://github.com/sxyu/plenoctree):star:[Viewer Code](https://github.com/sxyu/volrend):house:[project](https://alexyu.net/plenoctrees/):tv:[video](https://youtu.be/obrmH1T5mfI)
* [Neural Radiance Flow for 4D View Synthesis and Video Processing](https://arxiv.org/abs/2012.09790)<br>:star:[code](https://github.com/yilundu/nerflow):house:[project](https://yilundu.github.io/nerflow/)
* [Animatable Neural Radiance Fields for Modeling Dynamic Human Bodies](https://openaccess.thecvf.com/content/ICCV2021/papers/Peng_Animatable_Neural_Radiance_Fields_for_Modeling_Dynamic_Human_Bodies_ICCV_2021_paper.pdf)star:[code](https://github.com/zju3dv/animatable_nerf):house:[project](https://zju3dv.github.io/animatable_nerf/):tv:[video](https://zju3dv.github.io/animatable_nerf/)
* [GNeRF: GAN-Based Neural Radiance Field Without Posed Camera](https://arxiv.org/abs/2103.15606)<br>:open_mouth:oral
* [BARF: Bundle-Adjusting Neural Radiance Fields](https://arxiv.org/abs/2104.06405)<br>:open_mouth:oral:star:[code](https://github.com/chenhsuanlin/bundle-adjusting-NeRF):house:[project](https://chenhsuanlin.bitbucket.io/bundle-adjusting-NeRF/)
* [FastNeRF: High-Fidelity Neural Rendering at 200FPS](https://arxiv.org/abs/2103.10380)<br>:house:[project](https://microsoft.github.io/FastNeRF/):tv:[video](https://youtu.be/JS5H-Usiphg)
* [PIRenderer: Controllable Portrait Image Generation via Semantic Neural Rendering](https://arxiv.org/abs/2109.08379)<br>:star:[code](https://github.com/RenYurui/PIRender):tv:[video](https://www.youtube.com/watch?v=gDhcRcPI1JU&feature=youtu.be)
* [NeRD: Neural Reflectance Decomposition from Image Collections](https://arxiv.org/abs/2012.03918)<br>:star:[code](https://github.com/cgtuebingen/NeRD-Neural-Reflectance-Decomposition):house:[project](https://markboss.me/publication/2021-nerd/):tv:[video](https://www.youtube.com/watch?v=JL-qMTXw9VU)
* 3D photography(3D 相片)
  * [SLIDE: Single Image 3D Photography with Soft Layering and Depth-aware Inpainting](https://arxiv.org/abs/2109.01068)<br>:open_mouth:oral:house:[project](https://varunjampani.github.io/slide/):tv:[video](https://www.youtube.com/watch?v=RQio7q-ueY8)

<a name="3"/>

## 3.Image Clustering(图像聚类)
* [Clustering by Maximizing Mutual Information Across Views](https://arxiv.org/abs/2107.11635)
* [Learning Hierarchical Graph Neural Networks for Image Clustering](https://arxiv.org/abs/2107.01319)<br>:star:[code](https://github.com/dmlc/dgl/tree/master/examples/pytorch/hilander)
* [One-Pass Multi-View Clustering for Large-Scale Data](https://openaccess.thecvf.com/content/ICCV2021/papers/Liu_One-Pass_Multi-View_Clustering_for_Large-Scale_Data_ICCV_2021_paper.pdf)
* 人脸聚类
  * [Learn To Cluster Faces via Pairwise Classification](https://openaccess.thecvf.com/content/ICCV2021/papers/Liu_Learn_To_Cluster_Faces_via_Pairwise_Classification_ICCV_2021_paper.pdf)

<a name="2"/>

## 2.Sign Language(手语识别)
* [Mixed SIGNals: Sign Language Production via a Mixture of Motion Primitives](https://arxiv.org/abs/2107.11317)
* [SignBERT: Pre-Training of Hand-Model-Aware Representation for Sign Language Recognition](https://arxiv.org/abs/2110.05382)
* [Self-Mutual Distillation Learning for Continuous Sign Language Recognition](https://openaccess.thecvf.com/content/ICCV2021/papers/Hao_Self-Mutual_Distillation_Learning_for_Continuous_Sign_Language_Recognition_ICCV_2021_paper.pdf)
* [Visual Alignment Constraint for Continuous Sign Language Recognition](https://arxiv.org/abs/2104.02330)<br>:star:[code](https://github.com/ycmin95/VAC_CSLR)
* 手语翻译
  * [Stochastic Transformer Networks With Linear Competing Units: Application To End-to-End SL Translation](https://arxiv.org/abs/2109.13318)

<a name="1"/>

## 1.Other(其它)
* [Bias Loss for Mobile Neural Networks](https://arxiv.org/abs/2107.11170)<br>:star:[code](https://github.com/lusinlu/biasloss_skipblocknet)
* [Improve Unsupervised Pretraining for Few-label Transfer](https://arxiv.org/abs/2107.12369)
* [Temporal-wise Attention Spiking Neural Networks for Event Streams Classification](https://arxiv.org/abs/2107.11711)
* [Accelerating Atmospheric Turbulence Simulation via Learned Phase-to-Space Transform](https://arxiv.org/abs/2107.11627)
* [Energy-Based Open-World Uncertainty Modeling for Confidence Calibration](https://arxiv.org/abs/2107.12628)
* [Robustness via Cross-Domain Ensembles](https://arxiv.org/abs/2103.10919)<br>:open_mouth:oral:star:[code](https://github.com/EPFL-VILAB/XDEnsembles):house:[project](https://crossdomain-ensembles.epfl.ch/):tv:[video](https://youtu.be/h0FI5Sp7y7g)
* [Warp Consistency for Unsupervised Learning of Dense Correspondences](https://arxiv.org/abs/2104.03308)<br>:open_mouth:oral:star:[code](https://github.com/PruneTruong/DenseMatching)
* [Few-Shot and Continual Learning with Attentive Independent Mechanisms](https://arxiv.org/abs/2107.14053)<br>:star:[code](https://github.com/huang50213/AIM-Fewshot-Continual)
* [Out-of-Core Surface Reconstruction via Global TGV Minimization](https://arxiv.org/abs/2107.14790)
* [ELLIPSDF: Joint Object Pose and Shape Optimization with a Bi-level Ellipsoid and Signed Distance Function Description](https://arxiv.org/abs/2108.00355)
* [Multi-scale Matching Networks for Semantic Correspondence](https://arxiv.org/abs/2108.00211)<br>:star:[code](https://github.com/wintersun661/MMNet)
* [Learning with Noisy Labels via Sparse Regularization](https://arxiv.org/abs/2108.00192)<br>:star:[code](https://github.com/hitcszx/lnl_sr)
* [CanvasVAE: Learning to Generate Vector Graphic Documents](https://arxiv.org/abs/2108.01249)
* [Toward Spatially Unbiased Generative Models](https://arxiv.org/abs/2108.01285)<br>:star:[code](https://github.com/jychoi118/toward_spatial_unbiased)
* [Learning Compatible Embeddings](https://arxiv.org/abs/2108.01958)<br>:star:[code](https://github.com/IrvingMeng/LCE)
* [Instance Similarity Learning for Unsupervised Feature Representation](https://arxiv.org/abs/2108.02721)<br>:star:[code](https://github.com/ZiweiWangTHU/ISL)
* [Generalizable Mixed-Precision Quantization via Attribution Rank Preservation](https://arxiv.org/abs/2108.02720)<br>:star:[code](https://github.com/ZiweiWangTHU/GMPQ)
* [Unifying Nonlocal Blocks for Neural Networks](https://arxiv.org/abs/2108.02451)<br>:star:[code](https://github.com/zh460045050/SNL_ICCV2021)
* [Impact of Aliasing on Generalization in Deep Convolutional Networks](https://arxiv.org/abs/2108.03489)
* [NASOA: Towards Faster Task-oriented Online Fine-tuning with a Zoo of Models](https://arxiv.org/abs/2108.03434)<br>:star:[code](https://github.com/NAS-OA/NASOA)
* [ProAI: An Efficient Embedded AI Hardware for Automotive Applications - a Benchmark Study](https://arxiv.org/abs/2108.05170)
* [m-RevNet: Deep Reversible Neural Networks with Momentum](https://arxiv.org/abs/2108.05862) [涉嫌学术不端，已申请撤稿](https://www.zhihu.com/question/480075870/answer/2064860328)
* [Continual Neural Mapping: Learning An Implicit Scene Representation from Sequential Observations](https://arxiv.org/abs/2108.05851)
* [MT-ORL: Multi-Task Occlusion Relationship Learning](https://arxiv.org/abs/2108.05722)<br>:star:[code](https://github.com/fengpanhe/MT-ORL) 
* [Finding Representative Interpretations on Convolutional Neural Networks](https://arxiv.org/abs/2108.06384)
* 异常检测
  * [Weakly Supervised Temporal Anomaly Segmentation with Dynamic Time Warping](https://arxiv.org/abs/2108.06816)
* [Orthogonal Jacobian Regularization for Unsupervised Disentanglement in Image Generation](https://arxiv.org/abs/2108.07668)<br>:star:[code](https://github.com/csyxwei/OroJaR)
* [PR-RRN: Pairwise-Regularized Residual-Recursive Networks for Non-rigid Structure-from-Motion](https://arxiv.org/abs/2108.07506)
* [Instance Segmentation in 3D Scenes using Semantic Superpoint Tree Networks](https://arxiv.org/abs/2108.07478)<br>:star:[code](https://github.com/Gorilla-Lab-SCUT)
* [Learning RAW-to-sRGB Mappings with Inaccurately Aligned Supervision](https://arxiv.org/abs/2108.08119)<br>:star:[code](https://github.com/cszhilu1998/RAW-to-sRGB)
* [Structured Outdoor Architecture Reconstruction by Exploration and Classification](https://arxiv.org/abs/2108.07990)
* [Global Pooling, More than Meets the Eye: Position Information is Encoded Channel-Wise in CNNs](https://arxiv.org/abs/2108.07884)<br>:star:[code](https://github.com/islamamirul/PermuteNet)
* [A New Journey from SDRTV to HDRTV](https://arxiv.org/abs/2108.07978)<br>:star:[code](https://github.com/chxy95/HDRTVNet)
* [A Simple Framework for 3D Lensless Imaging with Programmable Masks](https://arxiv.org/abs/2108.07966)<br>:star:[code](https://github.com/CSIPlab/Programmable3Dcam)
* [Causal Attention for Unbiased Visual Recognition](https://arxiv.org/abs/2108.08782)<br>:star:[code](https://github.com/Wangt-CN/CaaM)
* [Learning to Match Features with Seeded Graph Matching Network](https://arxiv.org/abs/2108.08771)<br>:star:[code](https://github.com/vdvchen/SGMNet)
* [Amplitude-Phase Recombination: Rethinking Robustness of Convolutional Neural Networks in Frequency Domain](https://arxiv.org/abs/2108.08487)
* [PatchMatch-RL: Deep MVS with Pixelwise Depth, Normal, and Visibility](https://arxiv.org/abs/2108.08943)<br>:open_mouth:oral
* [Towards Understanding the Generative Capability of Adversarially Robust Classifiers](https://arxiv.org/abs/2108.09093)<br>:open_mouth:oral
* [Ranking Models in Unlabeled New Environments](https://arxiv.org/abs/2108.10310)<br>:star:[code](https://github.com/sxzrt/Proxy-Set)
* [Learning of Visual Relations: The Devil is in the Tails](https://arxiv.org/abs/2108.09668)
* [BlockCopy: High-Resolution Video Processing with Block-Sparse Feature Propagation and Online Policies](https://arxiv.org/abs/2108.09376)<br>:star:[code](https://github.com/thomasverelst/blockcopy-video-processing-pytorch)
* [Patch2CAD: Patchwise Embedding Learning for In-the-Wild Shape Retrieval from a Single Image](https://arxiv.org/abs/2108.09368)
* 去偏差
  * [BiaSwap: Removing dataset bias with bias-tailored swapping augmentation](https://arxiv.org/abs/2108.10008)
* [Full-Velocity Radar Returns by Radar-Camera Fusion](https://arxiv.org/abs/2108.10637)
* [CSG-Stump: A Learning Friendly CSG-Like Representation for Interpretable Shape Parsing](https://arxiv.org/abs/2108.11305)<br>:star:[code](https://github.com/kimren227/CSGStumpNet):house:[project](https://kimren227.github.io/projects/CSGStump/)
* [NGC: A Unified Framework for Learning with Open-World Noisy Data](https://arxiv.org/abs/2108.11035)
* [LocTex: Learning Data-Efficient Visual Representations from Localized Textual Supervision](https://arxiv.org/abs/2108.11950)<br>:house:[project](https://loctex.mit.edu/)
* [Unsupervised Dense Deformation Embedding Network for Template-Free Shape Correspondence](https://arxiv.org/abs/2108.11609)
* [Lifelong Infinite Mixture Model Based on Knowledge-Driven Dirichlet Process](https://arxiv.org/abs/2108.12278)<br>:star:[code](https://github.com/dtuzi123/Lifelong-infinite-mixture-model)
* [Digging into Uncertainty in Self-supervised Multi-view Stereo](https://arxiv.org/abs/2108.12966)
* [Learning to Discover Reflection Symmetry via Polar Matching Convolution](https://arxiv.org/abs/2108.12952)
* [A Dual Adversarial Calibration Framework for Automatic Fetal Brain Biometry](https://arxiv.org/abs/2108.12719)
* [The Functional Correspondence Problem](https://arxiv.org/abs/2109.01097)
* [The Animation Transformer: Visual Correspondence via Segment Matching](https://arxiv.org/abs/2109.02614)
* [Parsing Table Structures in the Wild](https://arxiv.org/abs/2109.02199)
* [Square Root Marginalization for Sliding-Window Bundle Adjustment](https://arxiv.org/abs/2109.02182)
* [Hierarchical Object-to-Zone Graph for Object Navigation](https://arxiv.org/abs/2109.02066)<br>:star:[code](https://github.com/sx-zhang/HOZ):tv:[video](https://drive.google.com/file/d/1UtTcFRhFZLkqgalKom6_9GpQmsJfXAZC/view)
* [Dual Transfer Learning for Event-based End-task Prediction via Pluggable Event to Image Translation](https://arxiv.org/abs/2109.01801)
* [Robustness and Generalization via Generative Adversarial Training](https://arxiv.org/abs/2109.02765)
* [Learning Fast Sample Re-weighting Without Reward Data](https://arxiv.org/abs/2109.03216)<br>:star:[code](https://github.com/google-research/google-research/tree/master/ieg)
* [ReconfigISP: Reconfigurable Camera Image Processing Pipeline](https://arxiv.org/abs/2109.04760)<br>:house:[project](https://www.mmlab-ntu.com/project/reconfigisp/)
* [Learning Indoor Inverse Rendering with 3D Spatially-Varying Lighting](https://arxiv.org/abs/2109.06061)<br>:open_mouth:oral
* [Low-Shot Validation: Active Importance Sampling for Estimating Classifier Performance on Rare Categories](https://arxiv.org/abs/2109.05720)
* [DisUnknown: Distilling Unknown Factors for Disentanglement Learning](https://arxiv.org/abs/2109.08090)<br>:star:[code](https://github.com/stormraiser/disunknown):house:[project](https://stormraiser.github.io/disunknown/)
* [S3VAADA: Submodular Subset Selection for Virtual Adversarial Active Domain Adaptation](https://arxiv.org/abs/2109.08901)<br>:house:[project](https://sites.google.com/iisc.ac.in/s3vaada-iccv2021)
* [ALADIN: All Layer Adaptive Instance Normalization for Fine-grained Style Similarity](https://arxiv.org/abs/2103.09776)<br>:tv:[video](https://www.youtube.com/watch?v=TxE1_juIHqY)
* [Photon-Starved Scene Inference using Single Photon Cameras](https://arxiv.org/abs/2107.11001)<br>:tv:[video](https://www.youtube.com/watch?v=r1YvHnGbi6k)
* [OSCAR-Net: Object-centric Scene Graph Attention for Image Attribution](https://arxiv.org/abs/2108.03541)<br>:star:[code](https://github.com/exnx/oscar):house:[project](https://exnx.github.io/oscar/)
* [Learning to Estimate Hidden Motions with Global Motion Aggregation](https://arxiv.org/abs/2104.02409)<br>:star:[code](https://github.com/zacjiang/GMA):tv:[video](https://www.youtube.com/watch?v=cBNSQ8ZFKSE)
* [Modelling Neighbor Relation in Joint Space-Time Graph for Video Correspondence Learning](https://arxiv.org/abs/2109.13499)
* [Meta Learning on a Sequence of Imbalanced Domains with Difficulty Awareness](https://arxiv.org/abs/2109.14120)<br>:star:[code](https://github.com/joey-wang123/Imbalancemeta) 
* [Procedure Planning in Instructional Videosvia Contextual Modeling and Model-based Policy Learning](https://arxiv.org/abs/2110.01770)<br>:open_mouth:oral
* [Extensions of Karger's Algorithm: Why They Fail in Theory and How They Are Useful in Practice](https://arxiv.org/abs/2110.02750)<br>:open_mouth:oral:star:[code](https://github.com/ejnnr)
* [Neural Strokes: Stylized Line Drawing of 3D Shapes](https://arxiv.org/abs/2110.03900)<br>:star:[code](https://github.com/DifanLiu/NeuralStrokes)
* [Learning Realistic Human Reposing using Cyclic Self-Supervision with 3D Shape, Pose, and Appearance Consistency](https://arxiv.org/abs/2110.05458)
* [Omnidata: A Scalable Pipeline for Making Multi-Task Mid-Level Vision Datasets from 3D Scans](https://arxiv.org/abs/2110.04994)<br>:house:[project](https://omnidata.vision/)
* [Cherry-Picking Gradients: Learning Low-Rank Embeddings of Visual Data via Differentiable Cross-Approximation](https://openaccess.thecvf.com/content/ICCV2021/papers/Usvyatsov_Cherry-Picking_Gradients_Learning_Low-Rank_Embeddings_of_Visual_Data_via_Differentiable_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/aelphy/c-pic)
* [Exploiting Explanations for Model Inversion Attacks](https://arxiv.org/abs/2104.12669)
* [Learning Bias-Invariant Representation by Cross-Sample Mutual Information Minimization](https://arxiv.org/abs/2108.05449)
* [RDI-Net: Relational Dynamic Inference Networks](https://openaccess.thecvf.com/content/ICCV2021/papers/Wang_RDI-Net_Relational_Dynamic_Inference_Networks_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/huanyuhello/RDI-Net)
* [ARAPReg: An As-Rigid-As Possible Regularization Loss for Learning Deformable Shape Generators](https://arxiv.org/abs/2108.09432)<br>:star:[code](https://github.com/GitBoSun/ARAPReg)
* [T-Net: Effective Permutation-Equivariant Network for Two-View Correspondence Learning](https://openaccess.thecvf.com/content/ICCV2021/papers/Zhong_T-Net_Effective_Permutation-Equivariant_Network_for_Two-View_Correspondence_Learning_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/x-gb/T-Net)
* [Learning To Stylize Novel Views](http://arxiv.org/abs/2105.13509)<br>:star:[code](https://github.com/hhsinping/stylescene):house:[project](https://hhsinping.github.io/3d_scene_stylization/)
* [A Lazy Approach to Long-Horizon Gradient-Based Meta-Learning](https://openaccess.thecvf.com/content/ICCV2021/papers/Zheng_Exploring_Temporal_Coherence_for_More_General_Video_Face_Forgery_Detection_ICCV_2021_paper.pdf)
* [Viewing Graph Solvability via Cycle Consistency](https://openaccess.thecvf.com/content/ICCV2021/papers/Arrigoni_Viewing_Graph_Solvability_via_Cycle_Consistency_ICCV_2021_paper.pdf)<br>:open_mouth:oral:star:[code](https://github.com/federica-arrigoni/solvability)<br>:trophy:Best paper honorable mention
* [SACoD: Sensor Algorithm Co-Design Towards Efficient CNN-Powered Intelligent PhlatCam](https://openaccess.thecvf.com/content/ICCV2021/papers/Fu_SACoD_Sensor_Algorithm_Co-Design_Towards_Efficient_CNN-Powered_Intelligent_PhlatCam_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/RICE-EIC/SACoD)
* [Rethinking 360° Image Visual Attention Modelling with Unsupervised Learning](https://openaccess.thecvf.com/content/ICCV2021/papers/Djilali_Rethinking_360deg_Image_Visual_Attention_Modelling_With_Unsupervised_Learning._ICCV_2021_paper.pdf)
* [Motion Basis Learning for Unsupervised Deep Homography Estimation with Subspace Projection](https://arxiv.org/abs/2103.15346)<br>:star:[code](https://github.com/megvii-research/BasesHomo)
* [Batch Normalization Increases Adversarial Vulnerability and Decreases Adversarial Transferability: A Non-Robust Feature Perspective](https://openaccess.thecvf.com/content/ICCV2021/papers/Benz_Batch_Normalization_Increases_Adversarial_Vulnerability_and_Decreases_Adversarial_Transferability_A_ICCV_2021_paper.pdf)
* [DeepCAD: A Deep Generative Network for Computer-Aided Design Models](https://arxiv.org/abs/2105.09492)<br>:house:[project](http://www.cs.columbia.edu/cg/deepcad/)
* [Better Aggregation in Test-Time Augmentation](https://openaccess.thecvf.com/content/ICCV2021/papers/Shanmugam_Better_Aggregation_in_Test-Time_Augmentation_ICCV_2021_paper.pdf)
* [Self-Born Wiring for Neural Trees](https://openaccess.thecvf.com/content/ICCV2021/papers/Chen_Self-Born_Wiring_for_Neural_Trees_ICCV_2021_paper.pdf)
* [Detector-Free Weakly Supervised Grounding by Separation](https://arxiv.org/abs/2104.09829)
* [Motion-Aware Dynamic Architecture for Efficient Frame Interpolation](https://openaccess.thecvf.com/content/ICCV2021/papers/Choi_Motion-Aware_Dynamic_Architecture_for_Efficient_Frame_Interpolation_ICCV_2021_paper.pdf)
* [Relating Adversarially Robust Generalization to Flat Minima](https://arxiv.org/abs/2104.04448)
* [Bit-Mixer: Mixed-Precision Networks With Runtime Bit-Width Selection](https://openaccess.thecvf.com/content/ICCV2021/papers/Bulat_Bit-Mixer_Mixed-Precision_Networks_With_Runtime_Bit-Width_Selection_ICCV_2021_paper.pdf)
* [AINet: Association Implantation for Superpixel Segmentation](https://arxiv.org/abs/2101.10696)<br>:star:[code](https://github.com/wangyxxjtu/AINet-ICCV2021)
* [Orthogonal Projection Loss](https://arxiv.org/abs/2103.14021)<br>:star:[code](https://github.com/kahnchana/opl)
* [Knowledge-Enriched Distributional Model Inversion Attacks](https://arxiv.org/abs/2010.04092)<br>:star:[code](https://github.com/SCccc21/Knowledge-Enriched-DMI)
* [Architecture Disentanglement for Deep Neural Networks](https://arxiv.org/abs/2003.13268)<br>:star:[code](https://github.com/hujiecpp/NAD)
* [On Equivariant and Invariant Learning of Object Landmark Representations](https://arxiv.org/abs/2006.14787)<br>:star:[code](https://github.com/cvl-umass/ContrastLandmark):house:[project](https://people.cs.umass.edu/~zezhoucheng/contrastive_landmark/)
* [Predicting with Confidence on Unseen Distributions](https://arxiv.org/abs/2107.03315)
* [Embed Me If You Can: A Geometric Perceptron](https://arxiv.org/abs/2006.06507)<br>:star:[code](https://github.com/pavlo-melnyk/mlgp-embedme)
* [Persistent Homology Based Graph Convolution Network for Fine-Grained 3D Shape Segmentation](https://openaccess.thecvf.com/content/ICCV2021/papers/Wong_Persistent_Homology_Based_Graph_Convolution_Network_for_Fine-Grained_3D_Shape_ICCV_2021_paper.pdf)
* [HIRE-SNN: Harnessing the Inherent Robustness of Energy-Efficient Deep Spiking Neural Networks by Training With Crafted Input Noise](https://openaccess.thecvf.com/content/ICCV2021/papers/Kundu_HIRE-SNN_Harnessing_the_Inherent_Robustness_of_Energy-Efficient_Deep_Spiking_Neural_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/ksouvik52/hiresnn2021)
* [Towards Memory-Efficient Neural Networks via Multi-Level In Situ Generation](https://arxiv.org/abs/2108.11430)
* [From Culture to Clothing: Discovering the World Events Behind a Century of Fashion Images](https://arxiv.org/abs/2102.01690)<br>:house:[project](http://vision.cs.utexas.edu/projects/CultureClothing/)
* [MBA-VO: Motion Blur Aware Visual Odometry](https://openaccess.thecvf.com/content/ICCV2021/papers/Liu_MBA-VO_Motion_Blur_Aware_Visual_Odometry_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/ethliup/MBA-VO)
* [STR-GQN: Scene Representation and Rendering for Unknown Cameras Based on Spatial Transformation Routing](https://openaccess.thecvf.com/content/ICCV2021/papers/Chen_STR-GQN_Scene_Representation_and_Rendering_for_Unknown_Cameras_Based_on_ICCV_2021_paper.pdf)
* [Explaining Local, Global, And Higher-Order Interactions In Deep Learning](https://arxiv.org/abs/2006.08601)
* [Beyond Trivial Counterfactual Explanations with Diverse Valuable Explanations](https://arxiv.org/abs/2103.10226)<br>:star:[code](https://github.com/ElementAI/beyond-trivial-explanations)
* [Homogeneous Architecture Augmentation for Neural Predictor](https://arxiv.org/abs/2107.13153)<br>:star:[code](https://github.com/lyq998/HAAP)
* [SS-IL: Separated Softmax for Incremental Learning](https://openaccess.thecvf.com/content/ICCV2021/papers/Ahn_SS-IL_Separated_Softmax_for_Incremental_Learning_ICCV_2021_paper.pdf)
* [VSAC: Efficient and Accurate Estimator for H and F](https://arxiv.org/abs/2106.10240)
* [Fusion Moves for Graph Matching](https://arxiv.org/abs/2101.12085)<br>:star:[code](https://github.com/vislearn/libmpopt):house:[project](https://vislearn.github.io/libmpopt/iccv2021/)
* [Geometric Granularity Aware Pixel-To-Mesh](https://openaccess.thecvf.com/content/ICCV2021/papers/Shi_Geometric_Granularity_Aware_Pixel-To-Mesh_ICCV_2021_paper.pdf)
* [Modulated Periodic Activations for Generalizable Local Functional Representations](https://arxiv.org/abs/2104.03960)<br>:house:[project](https://ishit.github.io/modsine/)
* [Interpretation of Emergent Communication in Heterogeneous Collaborative Embodied Agents](https://openaccess.thecvf.com/content/ICCV2021/papers/Patel_Interpretation_of_Emergent_Communication_in_Heterogeneous_Collaborative_Embodied_Agents_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/saimwani/CoMON):house:[project](https://shivanshpatel35.github.io/comon/):tv:[video](https://youtu.be/kLv2rxO9t0g)
* [A Dark Flash Normal Camera](https://arxiv.org/abs/2012.06125)<br>:house:[project](https://darkflashnormalpaper.github.io/):tv:[video](https://www.youtube.com/watch?v=RboGUBqYQec)
* [Pri3D: Can 3D Priors Help 2D Representation Learning?](https://arxiv.org/abs/2104.11225)<br>:star:[code](https://github.com/Sekunde/Pri3D):tv:[video](https://www.youtube.com/watch?v=S2VodtyfQbQ)
* [Membership Inference Attacks Are Easier on Difficult Problems](https://arxiv.org/abs/2102.07762)
* [Auxiliary Tasks and Exploration Enable ObjectGoal Navigation](https://openaccess.thecvf.com/content/ICCV2021/papers/Ye_Auxiliary_Tasks_and_Exploration_Enable_ObjectGoal_Navigation_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/joel99/objectnav):house:[project](https://joel99.github.io/objectnav/)
* [MixMo: Mixing Multiple Inputs for Multiple Outputs via Deep Subnetworks](https://arxiv.org/abs/2103.06132)
* [Act the Part: Learning Interaction Strategies for Articulated Object Part Discovery](https://arxiv.org/abs/2105.01047)<br>:house:[project](https://atp.cs.columbia.edu/)
* [DCT-SNN: Using DCT To Distribute Spatial Information Over Time for Low-Latency Spiking Neural Networks](https://openaccess.thecvf.com/content/ICCV2021/papers/Garg_DCT-SNN_Using_DCT_To_Distribute_Spatial_Information_Over_Time_for_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/SayeedChowdhury/dct-snn)
* [Learning To Resize Images for Computer Vision Tasks](https://arxiv.org/abs/2103.09950)
* [Field of Junctions: Extracting Boundary Structure at Low SNR](https://arxiv.org/abs/2011.13866)
* [DeepGaze IIE: Calibrated prediction in and out-of-domain for state-of-the-art saliency modeling](https://arxiv.org/abs/2105.12441)
* [Learning To Reduce Defocus Blur by Realistically Modeling Dual-Pixel Data](https://arxiv.org/abs/2012.03255)<br>:star:[code](https://github.com/Abdullah-Abuolaim/recurrent-defocus-deblurring-synth-dual-pixel)
* [Graph-based Asynchronous Event Processing for Rapid Object Recognitio](https://openaccess.thecvf.com/content/ICCV2021/papers/Li_Graph-Based_Asynchronous_Event_Processing_for_Rapid_Object_Recognition_ICCV_2021_paper.pdf)
* [Ranking Models in Unlabeled New Environments](https://openaccess.thecvf.com/content/ICCV2021/papers/Sun_Ranking_Models_in_Unlabeled_New_Environments_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/sxzrt/Proxy-Set)
* [A Hybrid Frequency-Spatial Domain Model for Sparse Image Reconstruction in Scanning Transmission Electron Microscopy](https://openaccess.thecvf.com/content/ICCV2021/papers/He_A_Hybrid_Frequency-Spatial_Domain_Model_for_Sparse_Image_Reconstruction_in_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/icthrm/Sparse-Sampling-Reconstruction)
* [MixMix: All You Need for Data-Free Compression Are Feature and Data Mixing](https://arxiv.org/abs/2011.09899)
* [Efficient Large Scale Inlier Voting for Geometric Vision Problems](https://arxiv.org/abs/2107.11810)<br>:star:[code](https://github.com/google-research/google-research/tree/master/large_scale_voting)
* [Aggregation With Feature Detection](https://openaccess.thecvf.com/content/ICCV2021/papers/Sun_Aggregation_With_Feature_Detection_ICCV_2021_paper.pdf)
* [ReCU: Reviving the Dead Weights in Binary Neural Networks](http://arxiv.org/abs/2103.12369)<br>:star:[code](https://github.com/z-hXu/ReCU)
* [Deep Halftoning With Reversible Binary Pattern](https://openaccess.thecvf.com/content/ICCV2021/papers/Xia_Deep_Halftoning_With_Reversible_Binary_Pattern_ICCV_2021_paper.pdf)
* [FFT-OT: A Fast Algorithm for Optimal Transportation](https://openaccess.thecvf.com/content/ICCV2021/papers/Lei_FFT-OT_A_Fast_Algorithm_for_Optimal_Transportation_ICCV_2021_paper.pdf)
* [Progressive Correspondence Pruning by Consensus Learning](https://arxiv.org/abs/2101.00591)<br>:star:[code](https://github.com/sailor-z/CLNet):house:[project](https://sailor-z.github.io/projects/CLNet)<br>:newspaper:解读:[基于一致性学习的渐进式匹配筛选 (ICCV 2021)](https://zhuanlan.zhihu.com/p/394483122)
* [Multispectral Illumination Estimation Using Deep Unrolling Network](https://openaccess.thecvf.com/content/ICCV2021/papers/Li_Multispectral_Illumination_Estimation_Using_Deep_Unrolling_Network_ICCV_2021_paper.pdf)
* [Distilling Global and Local Logits With Densely Connected Relations](https://openaccess.thecvf.com/content/ICCV2021/papers/Kim_Distilling_Global_and_Local_Logits_With_Densely_Connected_Relations_ICCV_2021_paper.pdf)
* [Learning specialized activation functions with the Piecewise Linear Unit](https://arxiv.org/abs/2104.03693)
* [Adaptive Convolutions With Per-Pixel Dynamic Filter Atom](https://arxiv.org/abs/2108.07895)
* [Deep Matching Prior: Test-Time Optimization for Dense Correspondence](https://arxiv.org/abs/2106.03090)<br>:star:[code](https://github.com/SunghwanHong/Deep-Matching-Prior)
* [Calibrated and Partially Calibrated Semi-Generalized Homographies](https://arxiv.org/abs/2103.06535)<br>:star:[code](https://github.com/snehalbhayani/SemiGeneralizedHomography)
* [The Spatio-Temporal Poisson Point Process: A Simple Model for the Alignment of Event Camera Data](https://openaccess.thecvf.com/content/ICCV2021/papers/Gu_The_Spatio-Temporal_Poisson_Point_Process_A_Simple_Model_for_the_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/pbideau/Event-ST-PPP)
* [EC-DARTS: Inducing Equalized and Consistent Optimization Into DARTS](https://openaccess.thecvf.com/content/ICCV2021/papers/Zhou_EC-DARTS_Inducing_Equalized_and_Consistent_Optimization_Into_DARTS_ICCV_2021_paper.pdf)
* [Refining activation downsampling with SoftPool](https://arxiv.org/abs/2101.00440)
* [FATNN: Fast and Accurate Ternary Neural Networks](https://arxiv.org/abs/2008.05101)<br>:star:[code](https://github.com/MonashAI/QTool)
* [GTT-Net: Learned Generalized Trajectory Triangulation](https://openaccess.thecvf.com/content/ICCV2021/papers/Xu_GTT-Net_Learned_Generalized_Trajectory_Triangulation_ICCV_2021_paper.pdf) 
* [Deep Permutation Equivariant Structure from Motion](https://arxiv.org/abs/2104.06703)<br>:star:[code](https://github.com/drormoran/Equivariant-SFM)
* [Extending Neural P-frame Codecs for B-frame Codin](https://openaccess.thecvf.com/content/ICCV2021/papers/Pourreza_Extending_Neural_P-Frame_Codecs_for_B-Frame_Coding_ICCV_2021_paper.pdf)
* [Hierarchical Graph Attention Network for Few-Shot Visual-Semantic Learning](https://openaccess.thecvf.com/content/ICCV2021/papers/Yin_Hierarchical_Graph_Attention_Network_for_Few-Shot_Visual-Semantic_Learning_ICCV_2021_paper.pdf)
* [SA-ConvONet: Sign-Agnostic Optimization of Convolutional Occupancy Networks](https://openaccess.thecvf.com/content/ICCV2021/papers/Tang_SA-ConvONet_Sign-Agnostic_Optimization_of_Convolutional_Occupancy_Networks_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/tangjiapeng/SA-ConvONet)
* [AA-RMVSNet: Adaptive Aggregation Recurrent Multi-View Stereo Network](https://openaccess.thecvf.com/content/ICCV2021/papers/Wei_AA-RMVSNet_Adaptive_Aggregation_Recurrent_Multi-View_Stereo_Network_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/QT-Zhu/AA-RMVSNet)
* [Rethinking the Backdoor Attacks' Triggers: A Frequency Perspective](https://arxiv.org/abs/2104.03413)<br>:star:[code](https://github.com/YiZeng623/frequency-backdoor)
* [Orthographic-Perspective Epipolar Geometry](https://openaccess.thecvf.com/content/ICCV2021/papers/Larsson_Orthographic-Perspective_Epipolar_Geometry_ICCV_2021_paper.pdf)
* [Why Approximate Matrix Square Root Outperforms Accurate SVD in Global Covariance Pooling?](https://arxiv.org/abs/2105.02498)<br>:star:[code](https://github.com/KingJamesSong/DifferentiableSVD)
* [PixelPyramids: Exact Inference Models From Lossless Image Pyramids](https://openaccess.thecvf.com/content/ICCV2021/papers/Mahajan_PixelPyramids_Exact_Inference_Models_From_Lossless_Image_Pyramids_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/visinf/pixelpyramids)
* [SurfaceNet: Adversarial SVBRDF Estimation from a Single Image](https://arxiv.org/abs/2107.11298)<br>:star:[code](https://github.com/perceivelab/surfacenet)
* [Adaptive Curriculum Learning](https://openaccess.thecvf.com/content/ICCV2021/papers/Kong_Adaptive_Curriculum_Learning_ICCV_2021_paper.pdf)
* [Sparse-Shot Learning With Exclusive Cross-Entropy for Extremely Many Localisations](https://arxiv.org/abs/2104.10425)
* [Graspness Discovery in Clutters for Fast and Accurate Grasp Detection](https://openaccess.thecvf.com/content/ICCV2021/papers/Wang_Graspness_Discovery_in_Clutters_for_Fast_and_Accurate_Grasp_Detection_ICCV_2021_paper.pdf)
* [RobustNav: Towards Benchmarking Robustness in Embodied Navigation](https://arxiv.org/abs/2106.04531)<br>:star:[code](https://github.com/allenai/robustnav)
* [Generating Attribution Maps With Disentangled Masked Backpropagation](https://openaccess.thecvf.com/content/ICCV2021/papers/Ruiz_Generating_Attribution_Maps_With_Disentangled_Masked_Backpropagation_ICCV_2021_paper.pdf)
* [Spectral Leakage and Rethinking the Kernel Size in CNNs](https://arxiv.org/abs/2101.10143)<br>:star:[code](https://github.com/ntomen/Windowed-Convolutions-for-CNNs)
* [What You Can Learn by Staring at a Blank Wall](https://arxiv.org/abs/2108.13027)
* [Neural TMDlayer: Modeling Instantaneous Flow of Features via SDE Generators](https://arxiv.org/abs/2108.08891)
* [CLEAR: Clean-up Sample-Targeted Backdoor in Neural Networks](https://openaccess.thecvf.com/content/ICCV2021/papers/Zhu_CLEAR_Clean-Up_Sample-Targeted_Backdoor_in_Neural_Networks_ICCV_2021_paper.pdf)
* [Learning To Hallucinate Examples From Extrinsic and Intrinsic Supervision](https://openaccess.thecvf.com/content/ICCV2021/papers/Gui_Learning_To_Hallucinate_Examples_From_Extrinsic_and_Intrinsic_Supervision_ICCV_2021_paper.pdf)
* [Single-shot Hyperspectral-Depth Imaging with Learned Diffractive Optics](https://arxiv.org/abs/2009.00463)
* [GridToPix: Training Embodied Agents With Minimal Supervision](https://arxiv.org/abs/2105.00931)<br>:house:[project](https://unnat.github.io/gridtopix/):tv:[video](https://youtu.be/CT6n32pa1EI)
* [Differentiable Dynamic Wirings for Neural Networks](https://openaccess.thecvf.com/content/ICCV2021/papers/Yuan_Differentiable_Dynamic_Wirings_for_Neural_Networks_ICCV_2021_paper.pdf)
* [JEM++: Improved Techniques for Training JEM](https://openaccess.thecvf.com/content/ICCV2021/papers/Yang_JEM_Improved_Techniques_for_Training_JEM_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/sndnyang/JEMPP)
* [X-World: Accessibility, Vision, and Autonomy Meet](https://openaccess.thecvf.com/content/ICCV2021/papers/Zhang_X-World_Accessibility_Vision_and_Autonomy_Meet_ICCV_2021_paper.pdf)
* [Memory-augmented Dynamic Neural Relational Inference](https://openaccess.thecvf.com/content/ICCV2021/papers/Gong_Memory-Augmented_Dynamic_Neural_Relational_Inference_ICCV_2021_paper.pdf)
* [Physics-based Differentiable Depth Sensor Simulation](https://arxiv.org/abs/2103.16563)
* [Hypergraph Neural Networks for Hypergraph Matching](https://openaccess.thecvf.com/content/ICCV2021/papers/Liao_Hypergraph_Neural_Networks_for_Hypergraph_Matching_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/xwliao/HNN-HM)
* Visual Grounding
  * [SAT: 2D Semantics Assisted Training for 3D Visual Grounding](https://arxiv.org/abs/2105.11450)
