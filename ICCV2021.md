# ICCV2021最新信息及已接收论文/代码(持续更新)

<div align="center">
  <img src="image/ICCV2021.png"/>
</div>

官网链接：http://iccv2021.thecvf.com/home<br>
开会时间：2021年10月11日至17日<br>

# 📗📗📗在【我爱计算机视觉】微信公众号后台回复“paper”，即可收到 ICCV 2021 已收录论文的打包下载。至10月12日已公开 520+14 篇。

### :exclamation::exclamation::exclamation::star2::star2::star2:ICCV 2021收录论文，10月12日新增 14 篇。
* 手语识别
  * [SignBERT: Pre-Training of Hand-Model-Aware Representation for Sign Language Recognition](https://arxiv.org/abs/2110.05382)
* 点云
  * [Point Cloud Augmentation with Weighted Local Transformations](https://arxiv.org/abs/2110.05379)<br>:star:[code](https://github.com/mlvlab/PointWOLF)
* 姿态
  * [The Center of Attention: Center-Keypoint Grouping via Attention for Multi-Person Pose Estimation](https://arxiv.org/abs/2110.05132)<br>:star:[code](https://github.com/dvl-tum)
* A-VQA
  * [Pano-AVQA: Grounded Audio-Visual Question Answering on 360∘ Videos](https://arxiv.org/abs/2110.05122)
* 图像到图像翻译
  * [Bridging the Gap between Label- and Reference-based Synthesis in Multi-attribute Image-to-Image Translation](https://arxiv.org/abs/2110.05055)<br>:star:[code](https://github.com/huangqiusheng/BridgeGAN)
* 三维
  * [BuildingNet: Learning to Label 3D Buildings](https://arxiv.org/abs/2110.04955)<br>:open_mouth:oral:star:[code](https://github.com/buildingnet/buildingnet_dataset):house:[project](https://buildingnet.org/)
* 目标检测
  * [Morphable Detector for Object Detection on Demand](https://arxiv.org/abs/2110.04917)<br>:star:[code](https://github.com/Zhaoxiangyun/Morphable-Detector)
* 人脸
  * [Self-Supervised 3D Face Reconstruction via Conditional Estimation](https://arxiv.org/abs/2110.04800)
* 分类
  * [Transformer-based Dual Relation Graph for Multi-label Image Recognition](https://arxiv.org/abs/2110.04722)
* 数据集
  * [Beyond Road Extraction: A Dataset for Map Update using Aerial Images](https://arxiv.org/abs/2110.04690)<br>:star:[code](https://github.com/favyen/muno21):house:[project](https://favyen.com/muno21/)<br>用于使用航拍图像更新地图的数据集
* 人体运动捕捉
  * [SOMA: Solving Optical Marker-Based MoCap Automatically](https://arxiv.org/abs/2110.04431)<br>:star:[code](https://github.com/nghorbani/soma):house:[project](https://soma.is.tue.mpg.de/):tv:[video](https://youtu.be/BEFCqIefLA8)
* 去噪
  * [Rethinking Noise Synthesis and Modeling in Raw Denoising](https://arxiv.org/abs/2110.04756)<br>:star:[code](https://github.com/zhangyi-3/noise-synthesis)
* 其它
  * [Learning Realistic Human Reposing using Cyclic Self-Supervision with 3D Shape, Pose, and Appearance Consistency](https://arxiv.org/abs/2110.05458)
  * [Omnidata: A Scalable Pipeline for Making Multi-Task Mid-Level Vision Datasets from 3D Scans](https://arxiv.org/abs/2110.04994)<br>:house:[project](https://omnidata.vision/)


### 10月11日新增 4 篇。

* GAN
  * [Toward a Visual Concept Vocabulary for GAN Latent Space](https://arxiv.org/abs/2110.04292)
  * [Collaging Class-specific GANs for Semantic Image Synthesis](https://arxiv.org/abs/2110.04281)<br>:house:[project](https://yuheng-li.github.io/CollageGAN/)
* 其它
  * [Neural Strokes: Stylized Line Drawing of 3D Shapes](https://arxiv.org/abs/2110.03900)<br>:star:[code](https://github.com/DifanLiu/NeuralStrokes)
* 小样本
  * [Meta-Learning with Task-Adaptive Loss Function for Few-Shot Learning](https://arxiv.org/abs/2110.03909)<br>:open_mouth:oral:star:[code](https://github.com/baiksung)

# 目录

|:dog:|:mouse:|:hamster:|:tiger:|
|------|------|------|------|
|[57.Image Matching(图像匹配)](#57)|[58.Computational Photography(光学、几何、光场成像、计算摄影)](#58)|[59.Graph Neural Networks(图神经网络)](#59)|
|[53.Vision Localization(视觉定位)](#53)|[54.Sketch recognition(草图)](#54)|[55.Activity Recognition(活动识别)](#55)|[56.Dataset(数据集)](#56)|
|[49.Human-Object Interaction(人物交互)](#49)|[50.Continual Learning(持续学习)](#50)|[51.View Synthesis(视图合成)](#51)|[52.Vision-and-Language(视觉语言)](#52)|
|[45.Image Caption(图像字幕)](#45)|[46.Defect Detection(缺陷检测)](#46)|[47.NAS](#47)|[48.6DoF](#48)|
|[41.Out-of-Distribution Detection(OOD)](#41)|[42.Visual Representations Learning(视觉表征学习)](#42)|[43.Dense Prediction(密集预测)](#43)|[44.Human motion prediction(人体运动预测)](#44)|
|[37.Multitask Learning(多任务学习)](#37)|[38.Weakly/Semi-Supervised/Self-supervised/Unsupervised Learning(自/半/弱监督学习)](#38)|[39.Incremental Learning(增量学习)](#39)|[40.Metric Learning(度量学习)](#40)|
|[33.Remote Sensing Images(遥感影像)](#33)|[34.Image Super-Resolution(图像超分辨率)](#34)|[35.Quantization/Pruning/Knowledge Distillation/Model Compression(量化、剪枝、蒸馏、模型压缩/扩展与优化)](#35)|[36.SLAM/AR/VR/机器人](#36)|
|[29.Image Retrieval(图像检索)](#29)|[30.Image Generation/synthesis(图像生成/合成)](#30)|[31.Style Transfer(风格迁移)](#31)|[32.语音](#32)|
|[25.Medical Image(医学影像)](#25)|[26.Image Processing(图像处理)](#26)|[27.Multi-label image recognition(多标签图像识别)](#27)|[28.Contrastive Learning(对比学习)](#28)]
|[21.Active Learning(主动学习)](#21)|[22.GAN](#22)|[23.Gaze Estimation(视线估计)](#23)|[24.Face(人脸)](#24)|
|[17.3D(三维视觉)](#17)|[18.Transformers](#18)|[19.Self-Driving Vehicles(自动驾驶)](#19)|[20.Adversarial Learning(对抗学习)](#20)|
|[13.Image Segmentation(图像分割)](#13)|[14.Object Detection(目标检测)](#13)|[15.Object Tracking(目标跟踪)](#15)|[16.Person Re-Identification(人员重识别)](#16)|
|[9.Video](#9)|[10.OCR](10)|[11.Visual Question Answering(视觉问答)](#11)|[12.Image/Fine-Grained Classification(图像/细粒度分类)](#12)|
|[5.Few-Shot/Zero-Shot Learning;Domain Generalization/Adaptation(小/零样本学习;域适应/泛化)](#5)|[6.Point Cloud(点云)](#6)|[7.Scene Graph Generation(场景图生成)](#7)|[8.Human Pose Estimation(人体姿态估计)](#8)|
|[1.Other(其它)](#1)|[2.Sign Language(手语识别)](#2)|[3.Image Clustering(图像聚类)](#3)|[4.Neural rendering(神经渲染)](#4)|

<a name="59"/>

## 59.Graph Neural Networks(图神经网络)
* [Meta-Aggregator: Learning to Aggregate for 1-bit Graph Neural Networks](https://arxiv.org/abs/2109.12872) 

<a name="58"/>

## 58.Computational Photography(光学、几何、光场成像、计算摄影)
* Snapshot compressive imaging(快照压缩成像)
  * [Dense Deep Unfolding Network with 3D-CNN Prior for Snapshot Compressive Imaging](https://arxiv.org/abs/2109.06548)<br>:star:[code](https://github.com/jianzhangcs/SCI3D)
* 光场
  * [Light Field Saliency Detection with Dual Local Graph Learning andReciprocative Guidance](https://arxiv.org/abs/2110.00698)

<a name="57"/>

## 57.Image Matching(图像匹配)
* [Matching in the Dark: A Dataset for Matching Image Pairs of Low-light Scenes](https://arxiv.org/abs/2109.03585)
 
<a name="56"/>

## 56.Dataset(数据集)
* 生物医学图像
  * [BioFors: A Large Biomedical Image Forensics Dataset](https://arxiv.org/abs/2108.12961)
* 3D重建
  * [Common Objects in 3D: Large-Scale Learning and Evaluation of Real-life 3D Category Reconstruction](https://arxiv.org/abs/2109.00512)<br>:sunflower:[dataset](https://github.com/facebookresearch/co3d)
* 航空影像数据集
  * [Beyond Road Extraction: A Dataset for Map Update using Aerial Images](https://arxiv.org/abs/2110.04690)<br>:star:[code](https://github.com/favyen/muno21):house:[project](https://favyen.com/muno21/)<br>用于使用航拍图像更新地图的数据集

<a name="55"/>

## 55.Activity Recognition(活动识别)
* 小组活动识别
  * [Spatio-Temporal Dynamic Inference Network for Group Activity Recognition](https://arxiv.org/abs/2108.11743)<br>:star:[code](https://github.com/JacobYuan7/DIN_GAR)
  * [GroupFormer: Group Activity Recognition with Clustered Spatial-Temporal Transformer](https://arxiv.org/abs/2108.12630)<br>:star:[code](https://github.com/xueyee/GroupFormer) 
 

<a name="54"/>

## 54.Sketch recognition(草图)
* [SketchLattice: Latticed Representation for Sketch Manipulation](https://arxiv.org/abs/2108.11636)

<a name="53"/>

## 53.Vision Localization(视觉定位)
  * [Continual Learning for Image-Based Camera Localization](https://arxiv.org/abs/2108.09112)<br>:star:[code](https://github.com/AaltoVision/CL_HSCNet)

<a name="52"/>

## 52.Vision-and-Language(视觉语言)
* [YouRefIt: Embodied Reference Understanding with Language and Gesture](https://arxiv.org/abs/2109.03413)<br>:open_mouth:oral:house:[project](https://yixchen.github.io/YouRefIt/)
* 视觉语言导航
  * [Airbert: In-domain Pretraining for Vision-and-Language Navigation](https://arxiv.org/abs/2108.09105)<br>:house:[project](https://airbert-vln.github.io/)
  * [Waypoint Models for Instruction-guided Navigation in Continuous Environments](https://arxiv.org/abs/2110.02207)<br>:open_mouth:oral:star:[code](https://github.com/jacobkrantz/VLN-CE):house:[project](https://jacobkrantz.github.io/waypoint-vlnce/):tv:[video](https://youtu.be/hrHj9-1xoio)

<a name="51"/>

## 51.View Synthesis(视图合成)
* [Out-of-boundary View Synthesis Towards Full-Frame Video Stabilization](https://arxiv.org/abs/2108.09041)
* [Deep 3D Mask Volume for View Synthesis of Dynamic Scenes](https://arxiv.org/abs/2108.13408)<br>:house:[project](https://cseweb.ucsd.edu//~viscomp/projects/ICCV21Deep/)
* [Embedding Novel Views in a Single JPEG Image](https://arxiv.org/abs/2108.13003)
* [Video Autoencoder: self-supervised disentanglement of static 3D structure and motion](https://arxiv.org/abs/2110.02951)<br>:open_mouth:oral:star:[code](https://github.com/zlai0/VideoAutoencoder/):house:[project](https://zlai0.github.io/VideoAutoencoder/#method_video):tv:[video](https://www.youtube.com/watch?v=UaJZd4FrM8E)

<a name="50"/>

## 50.Continual Learning(持续学习)
  * [Online Continual Learning with Natural Distribution Shifts: An Empirical Study with Visual Data](https://arxiv.org/abs/2108.09020)

  
<a name="49"/>

## 49.Human-Object Interaction(人物交互)
* [Exploiting Scene Graphs for Human-Object Interaction Detection](https://arxiv.org/abs/2108.08584)
* [Spatially Conditioned Graphs for Detecting Human-Object Interactions](https://arxiv.org/abs/2012.06060)<br>:star:[code](https://github.com/fredzzhang/spatially-conditioned-graphs):tv:[video](https://www.youtube.com/watch?v=gkBWi_rWedU)
* [Virtual Multi-Modality Self-Supervised Foreground Matting for Human-Object Interaction](https://arxiv.org/abs/2110.03278)

<a name="48"/>

## 48.6DoF
* [SO-Pose: Exploiting Self-Occlusion for Direct 6D Pose Estimation](https://arxiv.org/abs/2108.08367)<br>:star:[code](https://github.com/shangbuhuan13/SO-Pose)
* [StereOBJ-1M: Large-scale Stereo Image Dataset for 6D Object Pose Estimation](https://arxiv.org/abs/2109.10115)

<a name="47"/>

## 47.NAS
* [BN-NAS: Neural Architecture Search with Batch Normalization](https://arxiv.org/abs/2108.07375)<br>:star:[code](https://github.com/bychen515/BNNAS)
* [RANK-NOSH: Efficient Predictor-Based Architecture Search via Non-Uniform Successive Halving](https://arxiv.org/abs/2108.08019)
* [Pi-NAS: Improving Neural Architecture Search by Reducing Supernet Training Consistency Shift](https://arxiv.org/abs/2108.09671)<br>:star:[code](https://github.com/Ernie1/Pi-NAS)
* [Evolving Search Space for Neural Architecture Search](https://arxiv.org/abs/2011.10904)<br>:star:[code](https://github.com/orashi/NSE_NAS):tv:[video](https://www.youtube.com/watch?v=fq21WBaumRc)

<a name="46"/>

## 46.Defect Detection(缺陷检测)
* [DRÆM -- A discriminatively trained reconstruction embedding for surface anomaly detection](https://arxiv.org/abs/2108.07610)

<a name="45"/>

## 45.Image Caption(图像字幕)
* [Who's Waldo? Linking People Across Text and Images](https://arxiv.org/abs/2108.07253)<br>:open_mouth:oral:house:[project](https://whoswaldo.github.io/)<br>:newspaper:解读:[ICCV2021 Oral-新任务！新数据集！康奈尔大学提出了类似VG但又不是VG的PVG任务](https://mp.weixin.qq.com/s/QC1UQRmZKgS0dctTXQ77Bg)
* art description generation(艺术描述生成)
  * [Explain Me the Painting: Multi-Topic Knowledgeable Art Description Generation](https://arxiv.org/abs/2109.05743)<br>:star:[code](https://github.com/noagarcia/explain-paintings)

<a name="44"/>

## 44.Human motion prediction(人体运动预测)
* [MSR-GCN: Multi-Scale Residual Graph Convolution Networks for Human Motion Prediction](https://arxiv.org/abs/2108.07152)<br>:star:[code](https://github.com/Droliven/MSRGCN)
* [Stochastic Scene-Aware Motion Prediction](https://arxiv.org/abs/2108.08284)<br>:star:[code](https://github.com/mohamedhassanmus/SAMP):house:[project](https://samp.is.tue.mpg.de/)  
* [Generating Smooth Pose Sequences for Diverse Human Motion Prediction](https://arxiv.org/abs/2108.08422)<br>:open_mouth:oral:star:[code](https://github.com/wei-mao-2019/gsps)

<a name="43"/>

## 43.Dense Prediction(密集预测)
* [FaPN: Feature-aligned Pyramid Network for Dense Image Prediction](https://arxiv.org/abs/2108.07058)<br>:star:[code](https://github.com/EMI-Group/FaPN)

<a name="42"/>

## 42.Visual Representations Learning(视觉表征学习)
* [Self-Supervised Visual Representations Learning by Contrastive Mask Prediction](https://arxiv.org/abs/2108.07954)
* [Temporal Knowledge Consistency for Unsupervised Visual Representation Learning](https://arxiv.org/abs/2108.10668)

<a name="41"/>

## 41.Out-of-Distribution Detection(OOD)
* [CODEs: Chamfer Out-of-Distribution Examples against Overconfidence Issue](https://arxiv.org/abs/2108.06024)
* [Semantically Coherent Out-of-Distribution Detection](https://arxiv.org/abs/2108.11941)<br>:star:[code](https://github.com/jingkang50/ICCV21_SCOOD):house:[project](https://jingkang50.github.io/projects/scood)

<a name="40"/>

## 40.Metric Learning(度量学习)
* [Towards Interpretable Deep Metric Learning with Structural Matching](https://arxiv.org/abs/2108.05889)<br>:star:[code](https://github.com/wl-zhao/DIML)
* [Deep Relational Metric Learning](https://arxiv.org/abs/2108.10026)<br>:star:[code](https://github.com/zbr17/DRML)
* [LoOp: Looking for Optimal Hard Negative Embeddings for Deep Metric Learning](https://arxiv.org/abs/2108.09335)<br>:star:[code](https://github.com/puneesh00/LoOp)

<a name="39"/>

## 39.Incremental Learning(增量学习)
* 类增量学习
  * [Always Be Dreaming: A New Approach for Data-Free Class-Incremental Learning](https://arxiv.org/abs/2106.09701)<br>:newspaper:解读:[让模型实现“终生学习”，佐治亚理工学院提出Data-Free的增量学习](https://mp.weixin.qq.com/s/Fm9ufPD6rzL2VzaqpdFpjg)

<a name="38"/>

## 38.Weakly/Semi-Supervised/Self-supervised/Unsupervised Learning(自/半/弱监督学习)
* 半监督
  * [Trash to Treasure: Harvesting OOD Data with Cross-Modal Matching for Open-Set Semi-Supervised Learning](https://arxiv.org/abs/2108.05617)
* 自监督
  * [Focus on the Positives: Self-Supervised Learning for Biodiversity Monitoring](https://arxiv.org/abs/2108.06435)<br>:star:[code](https://github.com/omipan/camera_traps_self_supervised)
  * [Self-supervised Neural Networks for Spectral Snapshot Compressive Imaging](https://arxiv.org/abs/2108.12654)<br>:star:[code](https://github.com/mengziyi64/CASSI-Self-Supervised)

<a name="37"/>

## 37.Multitask Learning(多任务学习)
* [MultiTask-CenterNet (MCN): Efficient and Diverse Multitask Learning using an Anchor Free Approach](https://arxiv.org/abs/2108.05060)<br>:newspaper:解读:[ICCV2021《MultiTask CenterNet》CV多任务新进展！一节更比三节强](https://mp.weixin.qq.com/s/toAZS0OHdW4MG30P1wAAUA)
* [Multi-Task Self-Training for Learning General Representations](https://arxiv.org/abs/2108.11353)<br>:newspaper:解读:[ICCV2021 MuST：还在特定任务里为刷点而苦苦挣扎？谷歌的大佬们都已经开始玩多任务训练了](https://mp.weixin.qq.com/s/nhv1l9xBSaZceibIn5fhqw)

<a name="36"/>

## 36.SLAM/AR/VR/机器人
* 机器人
  * 室内导航
    * [The Surprising Effectiveness of Visual Odometry Techniques for Embodied PointGoal Navigation](https://arxiv.org/abs/2108.11550)<br>:star:[code](https://github.com/Xiaoming-Zhao/PointNav-VO):house:[project](https://xiaoming-zhao.github.io/projects/pointnav-vo/)
* VR/AR
  * [The Power of Points for Modeling Humans in Clothing](https://arxiv.org/abs/2109.01137)<br>:star:[code](https://github.com/qianlim/POP):house:[project](https://qianlim.github.io/POP):tv:[video](https://youtu.be/5M4F9zSWIEE)
  * 虚拟试穿  
    * [M3D-VTON: A Monocular-to-3D Virtual Try-On Network](https://arxiv.org/abs/2108.05126)
    * [ZFlow: Gated Appearance Flow-based Virtual Try-on with 3D Priors](https://arxiv.org/abs/2109.07001)
* SLAM
  * [On the Limits of Pseudo Ground Truth in Visual Camera Re-localisation](https://arxiv.org/abs/2109.00524)<br>:star:[code](https://github.com/tsattler/visloc_pseudo_gt_limitations/)

<a name="35"/>

## 35.Quantization/Pruning/Knowledge Distillation/Model Compression(量化、剪枝、蒸馏、模型压缩/扩展与优化)
* 知识蒸馏
  * [Distilling Holistic Knowledge with Graph Neural Networks](https://arxiv.org/abs/2108.05507)<br>:star:[code](https://github.com/wyc-ruiker/HKD)
  * [Lipschitz Continuity Guided Knowledge Distillation](https://arxiv.org/abs/2108.12905)<br>:star:[code](https://github.com/42Shawn/LONDON/tree/master)
* 量化
  * [Distance-aware Quantization](https://arxiv.org/abs/2108.06983)<br>:star:[code](https://github.com/cvlab-yonsei/DAQ):house:[project](https://cvlab.yonsei.ac.kr/projects/DAQ/) 
  * [Dynamic Network Quantization for Efficient Video Inference](https://arxiv.org/abs/2108.10394)<br>:star:[code](https://github.com/sunxm2357/VideoIQ):house:[project](https://cs-people.bu.edu/sunxm/VideoIQ/project.html)
  * [Cluster-Promoting Quantization with Bit-Drop for Minimizing Network Quantization Loss](https://arxiv.org/abs/2109.02100)
* 模型压缩
  * [GDP: Stabilized Neural Network Pruning via Gates with Differentiable Polarization](https://arxiv.org/abs/2109.02220)

<a name="34"/>

## 34.Image Super-Resolution(图像超分辨率)
* [Mutual Affine Network for Spatially Variant Kernel Estimation in Blind Image Super-Resolution](https://arxiv.org/abs/2108.05302)<br>:star:[code](https://github.com/JingyunLiang/MANet)
* [Hierarchical Conditional Flow: A Unified Framework for Image Super-Resolution and Image Rescaling](https://arxiv.org/abs/2108.05301)<br>:star:[code](https://github.com/JingyunLiang/HCFlow)
* [Deep Reparametrization of Multi-Frame Super-Resolution and Denoising](https://arxiv.org/abs/2108.08286)<br>:open_mouth:oral
* [Dual-Camera Super-Resolution with Aligned Attention Modules](https://arxiv.org/abs/2109.01349)<br>:star:[code](https://github.com/Tengfei-Wang/DualCameraSR):house:[project](https://tengfei-wang.github.io/Dual-Camera-SR/index.html):tv:[video](https://www.youtube.com/watch?v=5TiUfAcNvuw)

<a name="33"/>

## 33.Remote Sensing Images(遥感影像)
* [SUNet: Symmetric Undistortion Network for Rolling Shutter Correction](https://arxiv.org/abs/2108.04775)
* [Change is Everywhere: Single-Temporal Supervised Object Change Detection in Remote Sensing Imagery](https://arxiv.org/abs/2108.07002)<br>:star:[code](https://github.com/Z-Zheng/ChangeStar)

<a name="32"/>

## 32.语音
* [The Right to Talk: An Audio-Visual Transformer Approach](https://arxiv.org/abs/2108.03256)<br>:star:[code](https://github.com/uark-cviu/Right2Talk)
* 音频分离
  * [Visual Scene Graphs for Audio Source Separation](https://arxiv.org/abs/2109.11955)

<a name="31"/>

## 31.Style Transfer(风格迁移)
* [AdaAttN: Revisit Attention Mechanism in Arbitrary Neural Style Transfer](https://arxiv.org/abs/2108.03647)<br>:star:[code](https://github.com/Huage001/AdaAttN)
* [Domain-Aware Universal Style Transfer](https://arxiv.org/abs/2108.04441)<br>:star:[code](https://github.com/Kibeom-Hong/Domain-Aware-Style-Transfer)

<a name="30"/>

## 30.Image Generation/synthesis(图像生成/合成)
* [ILVR: Conditioning Method for Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2108.02938)<br>:open_mouth:oral
* [Image Synthesis via Semantic Composition](https://arxiv.org/abs/2109.07053)<br>:star:[code](https://github.com/dvlab-research/SCGAN):house:[project](https://shepnerd.github.io/scg/)

<a name="29"/>

## 29.Image Retrieval(图像检索)
  * [DOLG: Single-Stage Image Retrieval with Deep Orthogonal Fusion of Local and Global Features](https://arxiv.org/abs/2108.02927)<br>:star:[code](https://github.com/feymanpriv/DOLG)
  * [Image Retrieval on Real-life Images with Pre-trained Vision-and-Language Models](https://arxiv.org/abs/2108.04024)<br>:star:[code](https://github.com/Cuberick-Orion/CIRR):house:[project](https://cuberick-orion.github.io/CIRR/)
  * [Self-supervised Product Quantization for Deep Unsupervised Image Retrieval](https://arxiv.org/abs/2109.02244)
* 跨域检索
  * [Universal Cross-Domain Retrieval: Generalizing Across Classes and Domains](https://arxiv.org/abs/2108.08356)
* Visual Geolocalization
  * [Viewpoint Invariant Dense Matching for Visual Geolocalization](https://arxiv.org/abs/2109.09827)<br>:star:[code](https://github.com/gmberton/geo_warp)


<a name="28"/>

## 28.Contrastive Learning(对比学习)
* [Improving Contrastive Learning by Visualizing Feature Transformation](https://arxiv.org/abs/2108.02982)<br>:open_mouth:oral:star:[code](https://github.com/DTennant/CL-Visualizing-Feature-Transformation)
* [TACo: Token-aware Cascade Contrastive Learning for Video-Text Alignment](https://arxiv.org/abs/2108.09980)<br>:newspaper:解读:[ICCV2021-TOCo-微软&CMU提出Token感知的级联对比学习方法，在视频文本对齐任务上“吊打”其他SOTA方法](https://mp.weixin.qq.com/s/sNwvYL1qsgyVrRe3-QmzhA)

<a name="27"/>

## 27.Multi-label image recognition(多标签图像识别)
* [Residual Attention: A Simple but Effective Method for Multi-Label Recognition](https://arxiv.org/abs/2108.02456)
* [Transformer-based Dual Relation Graph for Multi-label Image Recognition](https://arxiv.org/abs/2110.04722)



<a name="26"/>

## 26.Image Processing(图像处理)
* 图像形状操纵
  * [Image Shape Manipulation from a Single Augmented Training Sample](https://arxiv.org/abs/2109.06151)<br>:open_mouth:oral:star:[code](https://github.com/eliahuhorwitz/DeepSIM):house:[project](http://www.vision.huji.ac.il/deepsim/)
* 边缘检测
  * [RINDNet: Edge Detection for Discontinuity in Reflectance, Illumination, Normal and Depth](https://arxiv.org/abs/2108.00616)<br>:open_mouth:oral:star:[code](https://github.com/MengyangPu/RINDNet)
  * [Pixel Difference Networks for Efficient Edge Detection](https://arxiv.org/abs/2108.07009)<br>:star:[code](https://github.com/zhuoinoulu/pidinet)
* 图像识别
  * [MicroNet: Improving Image Recognition with Extremely Low FLOPs](https://arxiv.org/abs/2108.05894)<br>:star:[code](https://github.com/liyunsheng13/micronet)
* 图像去模糊
  * [Rethinking Coarse-to-Fine Approach in Single Image Deblurring](https://arxiv.org/abs/2108.05054)<br>:star:[code](https://github.com/chosj95/MIMO-UNet)
  * [Single Image Defocus Deblurring Using Kernel-Sharing Parallel Atrous Convolutions](https://arxiv.org/abs/2108.09108)
* Image quality assessment(图像质量评估IQA)
  * [MUSIQ: Multi-scale Image Quality Transformer](https://arxiv.org/abs/2108.05997)<br>:star:[code](https://github.com/google-research/google-research/tree/master/musiq)
* Image Harmonization
  * [SSH: A Self-Supervised Framework for Image Harmonization](https://arxiv.org/abs/2108.06805)<br>:star:[code](https://github.com/VITA-Group/SSHarmonization)
* 去阴影
  * [CANet: A Context-Aware Network for Shadow Removal](https://arxiv.org/abs/2108.09894)
* 去噪
  * [Rethinking Deep Image Prior for Denoising](https://arxiv.org/abs/2108.12841)<br>:star:[code](https://github.com/gistvision/DIP-denosing)
  * [Rethinking Noise Synthesis and Modeling in Raw Denoising](https://arxiv.org/abs/2110.04756)<br>:star:[code](https://github.com/zhangyi-3/noise-synthesis)
* 图像着色
  * [Towards Vivid and Diverse Image Colorization with Generative Color Prior](https://arxiv.org/abs/2108.08826)
* 图像增强
  * [Real-time Image Enhancer via Learnable Spatial-aware 3D Lookup Tables](https://arxiv.org/abs/2108.08697)
  * [Adaptive Unfolding Total Variation Network for Low-Light Image Enhancement](https://arxiv.org/abs/2110.00984)<br>:star:[code](https://github.com/CharlieZCJ/UTVNet)
* 图像恢复
  * [Spatially-Adaptive Image Restoration using Distortion-Guided Networks](https://arxiv.org/abs/2108.08617)
  * [Dynamic Attentive Graph Learning for Image Restoration](https://arxiv.org/abs/2109.06620)<br>:star:[code](https://github.com/jianzhangcs/DAGL)
* 图像压缩
  * [Variable-Rate Deep Image Compression through Spatially-Adaptive Feature Transform](https://arxiv.org/abs/2108.09551)<br>:star:[code](https://github.com/micmic123/QmapCompression)
* 图像修复
  * [Image Inpainting via Conditional Texture and Structure Dual Generation](https://arxiv.org/abs/2108.09760)<br>:star:[code](https://github.com/Xiefan-Guo/CTSDG)
 * Image extrapolation
  * [SemIE: Semantically-aware Image Extrapolation](https://arxiv.org/abs/2108.13702)<br>:house:[project](https://semie-iccv.github.io/)
 * Reversible Image Conversion
  * [IICNet: A Generic Framework for Reversible Image Conversion](https://arxiv.org/abs/2109.04242)<br>:star:[code](https://github.com/felixcheng97/IICNet)
* 伪影去除
  * [Towards Flexible Blind JPEG Artifacts Removal](https://arxiv.org/abs/2109.14573)<br>:star:[code](https://github.com/jiaxi-jiang/FBCNN)
* De-rendering
  * [De-rendering Stylized Texts](https://arxiv.org/abs/2110.01890)<br>:star:[code](https://github.com/CyberAgentAILab/derendering-text):house:[project](https://cyberagentailab.github.io/derendering-text/)


<a name="25"/>

## 25.Medical Image(医学影像)
* 医学图像分割
  * [Recurrent Mask Refinement for Few-Shot Medical Image Segmentation](https://arxiv.org/abs/2108.00622)<br>:star:[code](https://github.com/uci-cbcl/RP-Net)
* 病理学图像表示
  * [A QuadTree Image Representation for Computational Pathology](https://arxiv.org/abs/2108.10873)
* 医学图像分析
  * [Preservational Learning Improves Self-supervised Medical Image Models by Reconstructing Diverse Contexts](https://arxiv.org/abs/2109.04379)<br>:star:[code](https://github.com/Luchixiang/PCRL)
* 医学图像去噪
  * [Eformer: Edge Enhancement based Transformer for Medical Image Denoising](https://arxiv.org/abs/2109.08044)
 
 
<a name="24"/>

## 24.Face(人脸)
* [VariTex: Variational Neural Face Textures](https://arxiv.org/abs/2104.05988)<br>:star:[code](https://github.com/mcbuehler/VariTex):house:[project](https://mcbuehler.github.io/VariTex/):tv:[video](https://www.youtube.com/watch?v=6-GFHcLkbik)
* 人脸造假检测
  * [OpenForensics: Large-Scale Challenging Dataset For Multi-Face Forgery Detection And Segmentation In-The-Wild](https://arxiv.org/abs/2107.14480)<br>:house:[project](https://sites.google.com/view/ltnghia/research/openforensics)
  * [Exploring Temporal Coherence for More General Video Face Forgery Detection](https://arxiv.org/abs/2108.06693)
* 人脸合成
  * [Disentangled Lifespan Face Synthesis](https://arxiv.org/abs/2108.02874)<br>:star:[code](https://github.com/SenHe/DLFS):house:[project](https://senhe.github.io/projects/iccv_2021_lifespan_face/):tv:[video](https://youtu.be/uklX03ns0m0)
* 人脸识别                                                                                      
  * [PASS: Protected Attribute Suppression System for Mitigating Bias in Face Recognition](https://arxiv.org/abs/2108.03764)
  * [SynFace: Face Recognition with Synthetic Data](https://arxiv.org/abs/2108.07960)
* Face perception面部感知
 * [Learning Facial Representations from the Cycle-consistency of Face](https://arxiv.org/abs/2108.03427)<br>:star:[code](https://github.com/JiaRenChang/FaceCycle)
* 说话人脸生成
  * [FACIAL: Synthesizing Dynamic Talking Face with Implicit Attribute Learning](https://arxiv.org/abs/2108.07938)
* 人脸表情识别
  * [Understanding and Mitigating Annotation Bias in Facial Expression Recognition](https://arxiv.org/abs/2108.08504)
  * [TransFER: Learning Relation-aware Facial Expression Representations with Transformers](https://arxiv.org/abs/2108.11116)
* 人脸呈现攻击检测
  * [Detection and Continual Learning of Novel Face Presentation Attacks](https://arxiv.org/abs/2108.12081)<br>:star:[code](https://github.com/mrostami1366)
* 人脸编辑
  * [Talk-to-Edit: Fine-Grained Facial Editing via Dialog](https://arxiv.org/abs/2109.04425)<br>:star:[code](https://github.com/yumingj/Talk-to-Edit):house:[project](https://www.mmlab-ntu.com/project/talkedit/)<br>:newspaper:解读:[ICCV2021 | 南洋理工大学、港中大提出Talk-to-Edit，对话实现高细粒度人脸编辑](https://mp.weixin.qq.com/s/48FsUqsppXaXUu-QMUIhCQ)
* 人脸对齐
  * [ADNet: Leveraging Error-Bias Towards Normal Direction in Face Alignment](https://arxiv.org/abs/2109.05721) 
* 人脸图像重建
  * [Focal Frequency Loss for Image Reconstruction and Synthesis](https://arxiv.org/abs/2012.12821)<br>:star:[code](https://github.com/EndlessSora/focal-frequency-loss):house:[project](https://www.mmlab-ntu.com/project/ffl/index.html):tv:[video](https://www.youtube.com/watch?v=RNTnDtKvcpc)
* 3D人脸重建
  * [Topologically Consistent Multi-View Face Inference Using Volumetric Sampling](https://arxiv.org/abs/2110.02948)<br>:star:[code](https://tianyeli.github.io/tofu) 
  * [Self-Supervised 3D Face Reconstruction via Conditional Estimation](https://arxiv.org/abs/2110.04800)



<a name="23"/>

## 23.Gaze Estimation(视线估计)
* [Generalizing Gaze Estimation with Outlier-guided Collaborative Adaptation](https://arxiv.org/abs/2107.13780)<br>:star:[code](https://github.com/DreamtaleCore/PnP-GA)

<a name="22"/>

## 22.GAN
* [Sketch Your Own GAN](https://arxiv.org/abs/2108.02774)<br>:star:[code](https://github.com/peterwang512/GANSketching):house:[project](https://peterwang512.github.io/GANSketching/)
* [Online Multi-Granularity Distillation for GAN Compression](https://arxiv.org/abs/2108.06908)<br>:star:[code](https://github.com/bytedance/OMGD)
* [Dual Projection Generative Adversarial Networks for Conditional Image Generation](https://arxiv.org/abs/2108.09016)<br>:star:[code](https://github.com/phymhan/P2GAN)
* [InSeGAN: A Generative Approach to Segmenting Identical Instances in Depth Images](https://arxiv.org/abs/2108.13865)
* [ReStyle: A Residual-Based StyleGAN Encoder via Iterative Refinement](https://arxiv.org/abs/2104.02699)<br>:star:[code](https://github.com/yuval-alaluf/restyle-encoder):house:[project](https://yuval-alaluf.github.io/restyle-encoder/):tv:[video](https://youtu.be/6pGzLECSIWM)
* [WarpedGANSpace: Finding non-linear RBF paths in GAN latent space](https://arxiv.org/abs/2109.13357)<br>:star:[code](https://github.com/chi0tzp/WarpedGANSpace)
* [Toward a Visual Concept Vocabulary for GAN Latent Space](https://arxiv.org/abs/2110.04292)
* [Collaging Class-specific GANs for Semantic Image Synthesis](https://arxiv.org/abs/2110.04281)<br>:house:[project](https://yuheng-li.github.io/CollageGAN/)
* GAN inversion(GAN逆映射)
  * [From Continuity to Editability: Inverting GANs with Consecutive Images](https://arxiv.org/abs/2107.13812)<br>:star:[code](https://github.com/cnnlstm/InvertingGANs_with_ConsecutiveImgs)
  * [GAN Inversion for Out-of-Range Images with Geometric Transformations](https://arxiv.org/abs/2108.08998)<br>:house:[project](https://kkang831.github.io/publication/ICCV_2021_BDInvert/)
* 图像到图像翻译
  * [Unaligned Image-to-Image Translation by Learning to Reweight](https://arxiv.org/abs/2109.11736)<br>:star:[code](https://github.com/Mid-Push/IrwGAN)
  * [Bridging the Gap between Label- and Reference-based Synthesis in Multi-attribute Image-to-Image Translation](https://arxiv.org/abs/2110.05055)<br>:star:[code](https://github.com/huangqiusheng/BridgeGAN)


<a name="21"/>

## 21.Active Learning(主动学习)
* [Semi-Supervised Active Learning with Temporal Output Discrepancy](https://arxiv.org/abs/2107.14153)<br>:star:[code](https://github.com/siyuhuang/TOD)
* [Influence Selection for Active Learning](https://arxiv.org/abs/2108.09331)
  

<a name="20"/>

## 20.Adversarial Learning(对抗学习)
* [Feature Importance-aware Transferable Adversarial Attacks](https://arxiv.org/abs/2107.14185)<br>:star:[code](https://github.com/hcguoO0/FIA)
* [TkML-AP: Adversarial Attacks to Top-k Multi-Label Learning](https://arxiv.org/abs/2108.00146)
* [Meta Gradient Adversarial Attack](https://arxiv.org/abs/2108.04204)
* [AGKD-BML: Defense Against Adversarial Attack by Attention Guided Knowledge Distillation and Bi-directional Metric Learning](https://arxiv.org/abs/2108.06017)<br>:star:[code](https://github.com/hongw579/AGKD-BML)
* [Exploiting Multi-Object Relationships for Detecting Adversarial Attacks in Complex Scenes](https://arxiv.org/abs/2108.08421)
* [AdvDrop: Adversarial Attack to DNNs by Dropping Information](https://arxiv.org/abs/2108.09034)<br>:star:[code](https://github.com/RjDuan/AdvDrop)

<a name="19"/>

## 19.Self-Driving Vehicles(自动驾驶)
* [End-to-End Urban Driving by Imitating a Reinforcement Learning Coach](https://arxiv.org/abs/2108.08265)<br>:star:[code](https://github.com/zhejz/carla-roach)  
* [MultiSiam: Self-supervised Multi-instance Siamese Representation Learning for Autonomous Driving](https://arxiv.org/abs/2108.12178)<br>:star:[code](https://github.com/KaiChen1998/MultiSiam)
* [NEAT: Neural Attention Fields for End-to-End Autonomous Driving](https://arxiv.org/abs/2109.04456)<br>:star:[code](https://github.com/autonomousvision/neat)
* [Safety-aware Motion Prediction with Unseen Vehicles for Autonomous Driving](https://arxiv.org/abs/2109.01510)<br>:star:[code](https://github.com/xrenaa/Safety-Aware-Motion-Prediction)
* [Social-NCE: Contrastive Learning of Socially-aware Motion Representations](https://arxiv.org/abs/2012.11717)<br>:star:[code](https://github.com/vita-epfl/social-nce-crowdnav):tv:[video](https://www.youtube.com/watch?v=s1khZWWiQfA)
* Human trajectory prediction(人体轨迹预测)
  * [Personalized Trajectory Prediction via Distribution Discrimination](https://arxiv.org/abs/2107.14204)<br>:star:[code](https://github.com/CHENGY12/DisDis)
  * [Human Trajectory Prediction via Counterfactual Analysis](https://arxiv.org/abs/2107.14202)(https://github.com/CHENGY12/CausalHTP)
* 轨迹预测
  * [Unlimited Neighborhood Interaction for Heterogeneous Trajectory Prediction](https://arxiv.org/abs/2108.00238)
  * [LOKI: Long Term and Key Intentions for Trajectory Prediction](https://arxiv.org/abs/2108.08236)
  * [MG-GAN: A Multi-Generator Model Preventing Out-of-Distribution Samples in Pedestrian Trajectory Prediction](https://arxiv.org/abs/2108.09274)<br>:star:[code](https://github.com/selflein/MG-GAN)
  * [DenseTNT: End-to-end Trajectory Prediction from Dense Goal Sets](https://arxiv.org/abs/2108.09640)
* 运动预测
  * [RAIN: Reinforced Hybrid Attention Inference Network for Motion Forecasting](https://arxiv.org/abs/2108.01316)<br>:house:[project](https://jiachenli94.github.io/publications/RAIN/)
  * [SLAMP: Stochastic Latent Appearance and Motion Prediction](https://arxiv.org/abs/2108.02760)<br>:house:[project](https://kuis-ai.github.io/slamp/)
  * [SlowFast Rolling-Unrolling LSTMs for Action Anticipation in Egocentric Videos](https://arxiv.org/abs/2109.00829)
* 自动导航
  * [FOVEA: Foveated Image Magnification for Autonomous Navigation](https://arxiv.org/abs/2108.12102)<br>:house:[project](https://www.cs.cmu.edu/~mengtial/proj/fovea/)  
* 交通场景理解
  * [Structured Bird's-Eye-View Traffic Scene Understanding from Onboard Images](https://arxiv.org/abs/2110.01997)<br>:star:[code](https://github.com/ybarancan/STSU)
* 车辆车牌识别
  * 车辆重识别
    * [Heterogeneous Relational Complement for Vehicle Re-identification](https://arxiv.org/abs/2109.07894)
 
<a name="18"/>

## 18.Transformers
* [Generic Attention-model Explainability for Interpreting Bi-Modal and Encoder-Decoder Transformers](https://arxiv.org/abs/2103.15679)<br>:open_mouth:oral:star:[code](https://github.com/hila-chefer/Transformer-MM-Explainability)<br>:newspaper:解读:[ICCV2021 Oral-TAU&Facebook提出了通用的Attention模型可解释性](https://mp.weixin.qq.com/s/3skb8XMiwM3QtdH7ZazoYg)
* [PlaneTR: Structure-Guided Transformers for 3D Plane Recovery](https://arxiv.org/abs/2107.13108)<br>:star:[code](https://github.com/IceTTTb/PlaneTR3D)
* [Rethinking and Improving Relative Position Encoding for Vision Transformer](https://arxiv.org/abs/2107.14222)<br>:star:[code](https://github.com/microsoft/AutoML/tree/main/iRPE)
* [Vision Transformer with Progressive Sampling](https://arxiv.org/abs/2108.01684)
* [Paint Transformer: Feed Forward Neural Painting with Stroke Prediction](https://arxiv.org/abs/2108.03798)<br>:open_mouth:oral:star:[code](https://github.com/Huage001/PaintTransformer)
* [Rethinking Spatial Dimensions of Vision Transformers](https://arxiv.org/abs/2103.16302)<br>:star:[code](https://github.com/naver-ai/pit)<br>:newspaper:解读:[ICCV2021-PiT-池化操作不是CNN的专属，ViT说：“我也可以”；南大提出池化视觉Transformer（PiT）](https://mp.weixin.qq.com/s/b051uw8SSu6x-5R27e8AMg)
* [PnP-DETR: Towards Efficient Visual Analysis with Transformers](https://arxiv.org/abs/2109.07036)<br>:star:[code](https://github.com/twangnh/pnp-detr)
* 密集预测
  * [Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions](https://arxiv.org/abs/2102.12122)<br>:open_mouth:oral:star:[code](https://github.com/whai362/PVT)<br>:newspaper:解读:[大白话Pyramid Vision Transformer](https://mp.weixin.qq.com/s/oJHZWmStQYYzEEOveiuQrQ)
* 3D人体纹理估计
  * [3D Human Texture Estimation from a Single Image with Transformers](https://arxiv.org/abs/2109.02563)<br>:open_mouth:oral:house:[project](https://www.mmlab-ntu.com/project/texformer/)


<a name="17"/>

## 17.3D(三维视觉)
* [Discovering 3D Parts from Image Collections](https://arxiv.org/abs/2107.13629)<br>:open_mouth:oral:star:[code](https://github.com/chhankyao/lpd):house:[project](https://chhankyao.github.io/lpd/):tv:[video](https://youtu.be/erNQANNwK6k)
* [PixelSynth: Generating a 3D-Consistent Experience from a Single Image](https://arxiv.org/abs/2108.05892)<br>:star:[code](https://github.com/crockwell/pixelsynth):house:[project](https://crockwell.github.io/pixelsynth/)
* [Towers of Babel: Combining Images, Language, and 3D Geometry for Learning Multimodal Vision](https://arxiv.org/abs/2108.05863)<br>:star:[code](https://github.com/tgxs002/wikiscenes):house:[project](https://www.cs.cornell.edu/projects/babel/)
* [Pixel-Perfect Structure-from-Motion with Featuremetric Refinement](https://arxiv.org/abs/2108.08291)<br>:star:[code](https://github.com/cvg/pixel-perfect-sfm)
* [Learning Anchored Unsigned Distance Functions with Gradient Direction Alignment for Single-view Garment Reconstruction](https://arxiv.org/abs/2108.08478)
* [LSD-StructureNet: Modeling Levels of Structural Detail in 3D Part Hierarchies](https://arxiv.org/abs/2108.13459)
* [Rational Polynomial Camera Model Warping for Deep Learning Based Satellite Multi-View Stereo Matching](https://arxiv.org/abs/2109.11121)<br>:star:[code](https://github.com/WHU-GPCV/SatMVS)
* [Where2Act: From Pixels to Actions for Articulated 3D Objects](https://arxiv.org/abs/2101.02692)<br>:tv:[video](https://www.youtube.com/watch?v=cdMSZru3Aa8)
* [BuildingNet: Learning to Label 3D Buildings](https://arxiv.org/abs/2110.04955)<br>:open_mouth:oral:star:[code](https://github.com/buildingnet/buildingnet_dataset):house:[project](https://buildingnet.org/)
* 深度估计
  * [StructDepth: Leveraging the structural regularities for self-supervised indoor depth estimation](https://arxiv.org/abs/2108.08574)<br>:star:[code](https://github.com/SJTU-ViSYS/StructDepth)
  * [Bridging Unsupervised and Supervised Depth from Focus via All-in-Focus Supervision](https://arxiv.org/abs/2108.10843)<br>:star:[code](https://github.com/albert100121/AiFDepthNet):house:[project](https://albert100121.github.io/AiFDepthNet/)
  * [Augmenting Depth Estimation with Geospatial Context](https://arxiv.org/abs/2109.09879)
  * Monocular Depth Estimation(单目深度估计)
    * [MonoIndoor: Towards Good Practice of Self-Supervised Monocular Depth Estimation for Indoor Environments](https://arxiv.org/abs/2107.12429)
    * [Regularizing Nighttime Weirdness: Efficient Self-supervised Monocular Depth Estimation in the Dark](https://arxiv.org/abs/2108.03830)<br>:star:[code](https://github.com/w2kun/RNW)
    * [Towards Interpretable Deep Networks for Monocular Depth Estimation](https://arxiv.org/abs/2108.05312)<br>:star:[code](https://github.com/youzunzhi/InterpretableMDE):tv:[video](https://www.youtube.com/watch?v=er7TF2CusWo)
    * [Self-supervised Monocular Depth Estimation for All Day Images using Domain Separation](https://arxiv.org/abs/2108.07628)<br>:star:[code](https://github.com/LINA-lln/ADDS-DepthNet)
    * [Fine-grained Semantics-aware Representation Enhancement for Self-supervised Monocular Depth Estimation](https://arxiv.org/abs/2108.08829)<br>:open_mouth:oral:star:[code](https://github.com/hyBlue/FSRE-Depth)
    * [Excavating the Potential Capacity of Self-Supervised Monocular Depth Estimation](https://arxiv.org/abs/2109.12484)<br>:star:[code](https://github.com/prstrive/EPCDepth)
* Omnidirectional Localization
  * [PICCOLO: Point Cloud-Centric Omnidirectional Localization](https://arxiv.org/abs/2108.06545)
* 三维重建
  * [Learning Signed Distance Field for Multi-view Surface Reconstruction](https://arxiv.org/abs/2108.09964)<br>:open_mouth:oral
  * [3DStyleNet: Creating 3D Shapes with Geometric and Texture Style Variations](https://arxiv.org/abs/2108.12958)<br>:open_mouth:oral:house:[project](https://nv-tlabs.github.io/3DStyleNet/)
  * [DensePose 3D: Lifting Canonical Surface Maps of Articulated Objects to the Third Dimension](https://arxiv.org/abs/2109.00033)
  * [In-the-Wild Single Camera 3D Reconstruction Through Moving Water Surfaces](https://vccimaging.org/Publications/Xiong2021MovingWater/Xiong2021MovingWater.pdf)<br>:open_mouth:oral:star:[code](https://github.com/vccimaging/Reconstrution_Through_Moving_Water):tv:[video](https://www.youtube.com/watch?v=F6R52hfAs6s)
  * 三维场景重建
    * [Graph-to-3D: End-to-End Generation and Manipulation of 3D Scenes Using Scene Graphs](https://arxiv.org/abs/2108.08841)
    * [VolumeFusion: Deep Depth Fusion for 3D Scene Reconstruction](https://arxiv.org/abs/2108.08623)
  * 三维形状重建  
    * [3DIAS: 3D Shape Reconstruction with Implicit Algebraic Surfaces](https://arxiv.org/abs/2108.08653)<br>:house:[project](https://myavartanoo.github.io/3dias/)
    * [Multiresolution Deep Implicit Functions for 3D Shape Representation](https://arxiv.org/abs/2109.05591)
    * [MVTN: Multi-View Transformation Network for 3D Shape Recognition](https://arxiv.org/abs/2011.13244)<br>:star:[code](https://github.com/ajhamdi/MVTN):tv:[video](https://www.youtube.com/watch?v=1zaHx8ztlhk)
  * 三维网格重建    
    * [Vis2Mesh: Efficient Mesh Reconstruction from Unstructured Point Clouds of Large Scenes with Learned Virtual View Visibility](https://arxiv.org/abs/2108.08378)<br>:star:[code](https://github.com/GDAOSU/vis2mesh) 
* 三维场景
  * [Scene Synthesis via Uncertainty-Driven Attribute Synchronization](https://arxiv.org/abs/2108.13499)<br>:star:[code](https://github.com/yanghtr/Sync2Gen)
* 相机校准
  * [CTRL-C: Camera calibration TRansformer with Line-Classification](https://arxiv.org/abs/2109.02259) 

<a name="16"/>

## 16.Person Re-Identification(人员重识别)
* [Spatio-Temporal Representation Factorization for Video-based Person Re-Identification](https://arxiv.org/abs/2107.11878)
* [Learning Instance-level Spatial-Temporal Patterns for Person Re-identification](https://arxiv.org/abs/2108.00171)<br>:star:[code](https://github.com/RenMin1991/cleaned-DukeMTMC-reID/)
* [Towards Discriminative Representation Learning for Unsupervised Person Re-identification](https://arxiv.org/abs/2108.03439)
* [Learning by Aligning: Visible-Infrared Person Re-identification using Cross-Modal Correspondences](https://arxiv.org/abs/2108.07422)<br>:star:[code](https://github.com/cvlab-yonsei/LbA):house:[project](https://cvlab.yonsei.ac.kr/projects/LbA/)
* [Video-based Person Re-identification with Spatial and Temporal Memory Networks](https://arxiv.org/abs/2108.09039)<br>:house:[project](https://cvlab.yonsei.ac.kr/projects/STMN/)
* [Multi-Expert Adversarial Attack Detection in Person Re-identification Using Context Inconsistency](https://arxiv.org/abs/2108.09891)
* 域适应人员重识别
  * [IDM: An Intermediate Domain Module for Domain Adaptive Person Re-ID](https://arxiv.org/abs/2108.02413)<br>:open_mouth:oral:star:[code](https://github.com/SikaStar/IDM)
* Crowd Counting(拥挤人群计数)
  * [Rethinking Counting and Localization in Crowds:A Purely Point-Based Framework](https://arxiv.org/abs/2107.12746)<br>:open_mouth:oral:star:[code](https://github.com/TencentYoutuResearch)
  * [Uniformity in Heterogeneity:Diving Deep into Count Interval Partition for Crowd Counting](https://arxiv.org/abs/2107.12619)<br>:star:[code](https://github.com/TencentYoutuResearch)
  * [Spatial Uncertainty-Aware Semi-Supervised Crowd Counting](https://arxiv.org/abs/2107.13271)
  * [Variational Attention: Propagating Domain-Specific Knowledge for Multi-Domain Learning in Crowd Counting](https://arxiv.org/abs/2108.08023)<br>:star:[code](https://github.com/Zhaoyi-Yan/DKPNet)
* Person Search
  * [ASMR: Learning Attribute-Based Person Search with Adaptive Semantic Margin Regularizer](https://arxiv.org/abs/2108.04533)<br>:house:[project](http://cvlab.postech.ac.kr/research/ASMR/)
* 行人检测
  * [MOTSynth: How Can Synthetic Data Help Pedestrian Detection and Tracking?](https://arxiv.org/abs/2108.09518)
* 行人属性识别
  * [Spatial and Semantic Consistency Regularizations for Pedestrian Attribute Recognition](https://arxiv.org/abs/2109.05686)
* Person Search(行人搜索)
  * [Weakly Supervised Person Search with Region Siamese Networks](https://arxiv.org/abs/2109.06109)

<a name="15"/>

## 15.Object Tracking(目标跟踪)
* [Saliency-Associated Object Tracking](https://arxiv.org/abs/2108.03637)
* [Box-Aware Feature Enhancement for Single Object Tracking on Point Clouds](https://arxiv.org/abs/2108.04728)<br>:star:[code](https://github.com/Ghostish/BAT)
* [Learning to Track Objects from Unlabeled Videos](https://arxiv.org/abs/2108.12711)<br>:star:[code](https://github.com/VISION-SJTU/USOT)
* [DepthTrack : Unveiling the Power of RGBD Tracking](https://arxiv.org/abs/2108.13962)<br>:star:[code](https://github.com/xiaozai/DeT)
* 视觉目标跟踪
  * [Learning to Adversarially Blur Visual Object Tracking](https://arxiv.org/abs/2107.12085)<br>:star:[code](https://github.com/tsingqguo/ABA)
  * [Learn to Match: Automatic Matching Network Design for Visual Tracking](https://arxiv.org/abs/2108.00803)<br>:star:[code](https://github.com/JudasDie/SOTS)
  * [Video Annotation for Visual Tracking via Selection and Refinement](https://arxiv.org/abs/2108.03821)<br>:star:[code](https://github.com/Daikenan/VASR)
* 卫星图像跟踪 
  * [HiFT: Hierarchical Feature Transformer for Aerial Tracking](https://arxiv.org/abs/2108.00202)<br>:star:[code](https://github.com/vision4robotics/HiFT)
* 3D多目标跟踪
  * [Exploring Simple 3D Multi-Object Tracking for Autonomous Driving](https://arxiv.org/abs/2108.10312)<br>:newspaper:解读:[ICCV 2021丨轻舟智航提出SimTrack: 3D多目标一体化检测与跟踪，简单又精确](https://mp.weixin.qq.com/s/7vCckbjGd65NMgW9evR4Ag)

<a name="14"/>

## 14.Object Detection(目标检测)
* [Rank & Sort Loss for Object Detection and Instance Segmentation](https://arxiv.org/abs/2107.11669)<br>:open_mouth:oral:star:[code](https://github.com/kemaloksuz/RankSortLoss)
* [MDETR : Modulated Detection for End-to-End Multi-Modal Understanding](https://arxiv.org/abs/2104.12763)<br>:open_mouth:oral:star:[code](https://github.com/ashkamath/mdetr)
* [SimROD: A Simple Adaptation Method for Robust Object Detection](https://arxiv.org/abs/2107.13389)<br>:open_mouth:oral
* [GraphFPN: Graph Feature Pyramid Network for Object Detection](https://arxiv.org/abs/2108.00580)
* [Fast Convergence of DETR with Spatially Modulated Co-Attention](https://arxiv.org/abs/2108.02404)<br>:star:[code](https://github.com/gaopengcuhk/SMCA-DETR)
* [Oriented R-CNN for Object Detection](https://arxiv.org/abs/2108.05699)<br>:star:[code](https://github.com/jbwang1997/OBBDetection) 
* [Conditional DETR for Fast Training Convergence](https://arxiv.org/abs/2108.06152)<br>:newspaper:解读:[通过显式寻找物体的 extremity 区域加快 DETR 的收敛：Conditional DETR](https://mp.weixin.qq.com/s/RbtdfFDczrSxi0F4apbx1w)
* [Vector-Decomposed Disentanglement for Domain-Invariant Object Detection](https://arxiv.org/abs/2108.06685)<br>:star:[code](https://github.com/AmingWu/VDD-DAOD)
* [G-DetKD: Towards General Distillation Framework for Object Detectors via Contrastive and Semantic-guided Feature Imitation](https://arxiv.org/abs/2108.07482)
* [ODAM: Object Detection, Association, and Mapping using Posed RGB Video](https://arxiv.org/abs/2108.10165)<br>:open_mouth:oral
* [Reconcile Prediction Consistency for Balanced Object Detection](https://arxiv.org/abs/2108.10809)
* [Deep Structured Instance Graph for Distilling Object Detectors](https://arxiv.org/abs/2109.12862)<br>:star:[code](https://github.com/dvlab-research/Dsig)
* [Towards Rotation Invariance in Object Detection](https://arxiv.org/abs/2109.13488)<br>:star:[code](https://github.com/akasha-imaging/ICCV2021)
* [Morphable Detector for Object Detection on Demand](https://arxiv.org/abs/2110.04917)<br>:star:[code](https://github.com/Zhaoxiangyun/Morphable-Detector)
* 3D目标检测
  * [Geometry Uncertainty Projection Network for Monocular 3D Object Detection](https://arxiv.org/abs/2107.13774)
  * [Fog Simulation on Real LiDAR Point Clouds for 3D Object Detection in Adverse Weather](https://arxiv.org/abs/2108.05249)<br>:star:[code](https://github.com/MartinHahner/LiDAR_fog_sim):house:[project](https://www.trace.ethz.ch/publications/2021/lidar_fog_simulation/)
  * [Is Pseudo-Lidar needed for Monocular 3D Object detection?](https://arxiv.org/abs/2108.06417)<br>:star:[code](https://github.com/TRI-ML/dd3d)
  * [RandomRooms: Unsupervised Pre-training from Synthetic Shapes and Randomized Layouts for 3D Object Detection](https://arxiv.org/abs/2108.07794)
  * [LIGA-Stereo: Learning LiDAR Geometry Aware Representations for Stereo-based 3D Detector](https://arxiv.org/abs/2108.08258)<br>:star:[code](https://github.com/xy-guo/LIGA-Stereo):house:[project](https://xy-guo.github.io/liga/)  
  * [Improving 3D Object Detection with Channel-wise Transformer](https://arxiv.org/abs/2108.10723)
  * [4D-Net for Learned Multi-Modal Alignment](https://arxiv.org/abs/2109.01066)
  * [Pyramid R-CNN: Towards Better Performance and Adaptability for 3D Object Detection](https://arxiv.org/abs/2109.02499)
  * [Voxel Transformer for 3D Object Detection](https://arxiv.org/abs/2109.02497)
  * [An End-to-End Transformer Model for 3D Object Detection](https://arxiv.org/abs/2109.08141)<br>:open_mouth:oral:star:[code](https://github.com/facebookresearch/3detr):house:[project](https://facebookresearch.github.io/3detr/)
* 目标定位
  * 弱监督目标定位
    * [Normalization Matters in Weakly Supervised Object Localization](https://arxiv.org/abs/2107.13221)
* Anomaly Detection(图像异常检测)
  * [Divide-and-Assemble: Learning Block-wise Memory for Unsupervised Anomaly Detection](https://arxiv.org/abs/2107.13118)
* 弱监督目标检测
  * [Boosting Weakly Supervised Object Detection via Learning Bounding Box Adjusters](https://arxiv.org/abs/2108.01499)<br>:star:[code](https://github.com/DongSky/lbba_boosted_wsod)
* OOD 检测
  * [Triggering Failures: Out-Of-Distribution detection by learning from local adversarial attacks in Semantic Segmentation](https://arxiv.org/abs/2108.01634)<br>:star:[code](https://github.com/valeoai/obsnet)
* 显著目标检测
  * [Disentangled High Quality Salient Object Detection](https://arxiv.org/abs/2108.03551)
  * [Specificity-preserving RGB-D Saliency Detection](https://arxiv.org/abs/2108.08162)<br>:star:[code](https://github.com/taozh2017/SPNet)
  * RGB-D显著目标检测
    * [RGB-D Saliency Detection via Cascaded Mutual Information Minimization](https://arxiv.org/abs/2109.07246)<br>:star:[code](https://github.com/JingZhang617/cascaded_rgbd_sod)
  * co-saliency detection
    * [Summarize and Search: Learning Consensus-aware Dynamic Convolution for Co-Saliency Detection](https://arxiv.org/abs/2110.00338)<br>:star:[code](https://github.com/nnizhang/CADC)
* 违禁物品检测
  * [Towards Real-World Prohibited Item Detection: A Large-Scale X-ray Benchmark](https://arxiv.org/abs/2108.07020)<br>:sunflower:[dataset](https://github.com/bywang2018/security-dataset)
* 小样本目标检测
  * [DeFRCN: Decoupled Faster R-CNN for Few-Shot Object Detection](https://arxiv.org/abs/2108.09017)<br>:star:[code](https://github.com/er-muyue/DeFRCN)
* 视觉关系协同定位
  * [Few-shot Visual Relationship Co-localization](https://arxiv.org/abs/2108.11618)<br>:star:[code](https://github.com/vl2g/VRC):house:[project](https://vl2g.github.io/projects/vrc/)
* 密集目标检测
  * [Mutual Supervision for Dense Object Detection](https://arxiv.org/abs/2109.05986)<br>:star:[code](https://github.com/MCG-NJU)
* 域适应目标检测
  * [Seeking Similarities over Differences: Similarity-based Domain Alignment for Adaptive Object Detection](https://arxiv.org/abs/2110.01428)

<a name="13"/>

## 13.Image Segmentation(图像分割)
* [Standardized Max Logits: A Simple yet Effective Approach for Identifying Unexpected Road Obstacles in Urban-Scene Segmentation](https://arxiv.org/abs/2107.11264)<br>:open_mouth:oral:star:[code](https://github.com/shjung13/Standardized-max-logits):tv:[video](https://www.youtube.com/watch?v=leBJZHzX6xM)
* [TransForensics: Image Forgery Localization with Dense Self-Attention](https://arxiv.org/abs/2108.03871)
* [From Contexts to Locality: Ultra-high Resolution Image Segmentation via Locality-aware Contextual Correlation](https://arxiv.org/abs/2109.02580)<br>:star:[code](https://github.com/liqiokkk/FCtL)
* [Labels4Free: Unsupervised Segmentation using StyleGAN](https://arxiv.org/abs/2103.14968)<br>:house:[project](https://rameenabdal.github.io/Labels4Free/):tv:[video](https://www.youtube.com/watch?v=_pHunGpvLVk)
* [Warp-Refine Propagation: Semi-Supervised Auto-labeling via Cycle-consistency](https://arxiv.org/abs/2109.13432)
* [Scaling up instance annotation via label propagation](https://arxiv.org/abs/2110.02277)<br>:star:[code](https://github.com/ethanweber/scaling-anno):house:[project](http://scaling-anno.csail.mit.edu/)
* 全景分割
  * [Panoptic Narrative Grounding](https://arxiv.org/abs/2109.04988)
* 语义分割
  * [Re-distributing Biased Pseudo Labels for Semi-supervised Semantic Segmentation: A Baseline Investigation](https://arxiv.org/abs/2107.11279)<br>:open_mouth:oral:star:[code](https://github.com/CVMI-Lab/DARS)
  * [Personalized Image Semantic Segmentation](https://arxiv.org/abs/2107.13978)<br>:star:[code](https://github.com/zhangyuygss/PIS)
  * [RECALL: Replay-based Continual Learning in Semantic Segmentation](https://arxiv.org/abs/2108.03673)
  * [Deep Metric Learning for Open World Semantic Segmentation](https://arxiv.org/abs/2108.04562)
  * [LabOR: Labeling Only if Required for Domain Adaptive Semantic Segmentation](https://arxiv.org/abs/2108.05570)<br>:open_mouth:oral
  * [Dual Path Learning for Domain Adaptation of Semantic Segmentation](https://arxiv.org/abs/2108.06337)<br>:star:[code](https://github.com/royee182/DPL)
  * [Multi-Target Adversarial Frameworks for Domain Adaptation in Semantic Segmentation](https://arxiv.org/abs/2108.06962)
  * [Exploiting a Joint Embedding Space for Generalized Zero-Shot Semantic Segmentation](https://arxiv.org/abs/2108.06536)<br>:star:[code](https://github.com/cvlab-yonsei/JoEm):house:[project](https://cvlab.yonsei.ac.kr/projects/JoEm/)
  * [Multi-Anchor Active Domain Adaptation for Semantic Segmentation](https://arxiv.org/abs/2108.08012)<br>:open_mouth:oral:star:[code](https://github.com/munanning/MADA)
  * [Pixel Contrastive-Consistent Semi-Supervised Semantic Segmentation](https://arxiv.org/abs/2108.09025)
  * [Self-Regulation for Semantic Segmentation](https://arxiv.org/abs/2108.09702)<br>:star:[code](https://github.com/dongzhang89/SR-SS)
  * [ShapeConv: Shape-aware Convolutional Layer for Indoor RGB-D Semantic Segmentation](https://arxiv.org/abs/2108.10528)
  * [Generalize then Adapt: Source-Free Domain Adaptive Semantic Segmentation](https://arxiv.org/abs/2108.11249)<br>:house:[project](https://sites.google.com/view/sfdaseg)
  * [Mining Contextual Information Beyond Image for Semantic Segmentation](https://arxiv.org/abs/2108.11819)<br>:star:[code](https://github.com/CharlesPikachu/mcibi)
  * [ISNet: Integrate Image-Level and Semantic-Level Context for Semantic Segmentation](https://arxiv.org/abs/2108.12382)<br>:star:[code](https://github.com/SegmentationBLWX/sssegmentation)
  * [Pseudo-mask Matters in Weakly-supervised Semantic Segmentation](https://arxiv.org/abs/2108.12995)
  * [SIGN: Spatial-information Incorporated Generative Network for Generalized Zero-shot Semantic Segmentation](https://arxiv.org/abs/2108.12517)
  * 小样本语义分割
    * [Simpler is Better: Few-shot Semantic Segmentation with Classifier Weight Transformer](https://arxiv.org/abs/2108.03032)<br>:star:[code](https://github.com/zhiheLu/CWT-for-FSS)
    * [Learning Meta-class Memory for Few-Shot Semantic Segmentation](https://arxiv.org/abs/2108.02958)
  * 3D语义分割
    * [VMNet: Voxel-Mesh Network for Geodesic-Aware 3D Semantic Segmentation](https://arxiv.org/abs/2107.13824)<br>:open_mouth:oral:star:[code](https://github.com/hzykent/VMNet)
  * 视频语义分割
    * [Domain Adaptive Video Segmentation via Temporal Consistency Regularization](https://arxiv.org/abs/2107.11004)<br>:star:[code](https://github.com/Dayan-Guan/DA-VSN)
  * 弱监督语义分割
    * [Leveraging Auxiliary Tasks with Affinity Learning for Weakly Supervised Semantic Segmentation](https://arxiv.org/abs/2107.11787)<br>:star:[code](https://github.com/xulianuwa/AuxSegNet)
    * [Complementary Patch for Weakly Supervised Semantic Segmentation](https://arxiv.org/abs/2108.03852)
  * 点云语义分割
    * [ReDAL: Region-based and Diversity-aware Active Learning for Point Cloud Semantic Segmentation](https://arxiv.org/abs/2107.11769)<br>:tv:[video](https://www.youtube.com/watch?v=XJeb9kMxs5E)
* 实例分割
  * [Rank & Sort Loss for Object Detection and Instance Segmentation](https://arxiv.org/abs/2107.11669)<br>:open_mouth:oral:star:[code](https://github.com/kemaloksuz/RankSortLoss)
  * [SOTR: Segmenting Objects with Transformers](https://arxiv.org/abs/2108.06747)<br>:star:[code](https://github.com/easton-cau/SOTR)
  * [A Weakly Supervised Amodal Segmenter with Boundary Uncertainty Estimation](https://arxiv.org/abs/2108.09897)
  * [Instances as Queries](https://arxiv.org/abs/2105.01928)<br>:star:[code](https://github.com/hustvl/QueryInst):tv:[video](https://www.youtube.com/watch?v=3Fqwvn6_oUQ)
  * [CrossVIS: Crossover Learning for Fast Online Video Instance Segmentation](https://arxiv.org/abs/2104.05970)<br>:star:[code](https://github.com/hustvl/CrossVIS):tv:[video](https://www.youtube.com/watch?v=tPvYYjTgaNs)
  * 视频实例分割
    * [Video Instance Segmentation with a Propose-Reduce Paradigm](https://arxiv.org/abs/2103.13746)<br>:star:[code](https://github.com/dvlab-research/ProposeReduce)
  * 3D实例分割
    * [Hierarchical Aggregation for 3D Instance Segmentation](https://arxiv.org/abs/2108.02350)<br>:star:[code](https://github.com/hustvl/HAIS)
* 小样本分割
  * [Mining Latent Classes for Few-shot Segmentation](https://arxiv.org/abs/2103.15402)<br>:open_mouth:oral:star:[code](https://github.com/LiheYoung/MiningFSS)
* Human Motion Segmentation(人体运动分割)
  * [Graph Constrained Data Representation Learning for Human Motion Segmentation](https://arxiv.org/abs/2107.13362)
* 点云分割
  * [Learning with Noisy Labels for Robust Point Cloud Segmentation](https://arxiv.org/abs/2107.14230)<br>:open_mouth:oral:star:[code](https://github.com/pleaseconnectwifi/PNAL):house:[project](https://shuquanye.com/PNAL_website/)
  * [DRINet: A Dual-Representation Iterative Learning Network for Point Cloud Segmentation](https://arxiv.org/abs/2108.04023)
* 视频目标分割(VOS)
  * [Full-Duplex Strategy for Video Object Segmentation](https://arxiv.org/abs/2108.03151)<br>:house:[project](http://dpfan.net/FSNet/)
  * [Joint Inductive and Transductive Learning for Video Object Segmentation](https://arxiv.org/abs/2108.03679)<br>:star:[code](https://github.com/maoyunyao/JOINT)
  * [Hierarchical Memory Matching Network for Video Object Segmentation](https://arxiv.org/abs/2109.11404)<br>:star:[code](https://github.com/Hongje/HMMN)
  * [Self-supervised Video Object Segmentation by Motion Grouping](https://arxiv.org/abs/2104.07658)<br>:star:[code](https://github.com/charigyang/motiongrouping):house:[project](https://charigyang.github.io/motiongroup/):tv:[video](https://www.youtube.com/watch?v=Q0dLExLXZIw)
* 语义场景分割 
  * [BiMaL: Bijective Maximum Likelihood Approach to Domain Adaptation in Semantic Scene Segmentation](https://arxiv.org/abs/2108.03267)<br>:star:[code](https://github.com/uark-cviu/BiMaL)
* Referring Segmentation(基于文本的分割) 
  * [Vision-Language Transformer and Query Generation for Referring Segmentation](https://arxiv.org/abs/2108.05565)<br>:star:[code](https://github.com/henghuiding/Vision-Language-Transformer)
* 场景理解
  * [DeepPanoContext: Panoramic 3D Scene Understanding with Holistic Scene Context Graph and Relation-based Optimization](https://arxiv.org/abs/2108.10743)

<a name="12"/>

## 12.Image/Fine-Grained Classification(图像/细粒度分类) 
* [DiagViB-6: A Diagnostic Benchmark Suite for Vision Models in the Presence of Shortcut and Generalization Opportunities](https://arxiv.org/abs/2108.05779)
* [Online Continual Learning For Visual Food Classification](https://arxiv.org/abs/2108.06781)
* [A Unified Objective for Novel Class Discovery](https://arxiv.org/abs/2108.08536)<br>:open_mouth:oral:star:[code](https://github.com/DonkeyShot21/UNO):house:[project](https://ncd-uno.github.io/)<br>:newspaper:解读:[ICCV2021 Oral | UNO：用于“新类发现”的统一目标函数，简化训练流程！已开源！](https://mp.weixin.qq.com/s/3aQ5AUKOAnO7kDtsxhRJ3Q)
* [Improving Generalization of Batch Whitening by Convolutional Unit Optimization](https://arxiv.org/abs/2108.10629)<br>:star:[code](https://github.com/YooshinCho/pytorch_ConvUnitOptimization)
* [Towards Learning Spatially Discriminative Feature Representations](https://arxiv.org/abs/2109.01359)
* [CrossViT: Cross-Attention Multi-Scale Vision Transformer for Image Classification](https://arxiv.org/abs/2103.14899)<br>:star:[code](https://github.com/IBM/CrossViT)<br>:newspaper:解读:[ICCV2021 MIT-IBM沃森开源CrossViT：Transformer走向多分支、多尺度](https://mp.weixin.qq.com/s/aqDaF4Iy96Nx1pvX__6vHg)
* [SCOUTER: Slot Attention-based Classifier for Explainable Image Recognition](https://arxiv.org/abs/2009.06138)<br>:star:[code](https://github.com/wbw520/scouter)
* [Influence-Balanced Loss for Imbalanced Visual Classification](https://arxiv.org/abs/2110.02444)<br>:star:[code](https://github.com/pseulki/IB-Loss) 
* 长尾识别
  * [Parametric Contrastive Learning](https://arxiv.org/abs/2107.12028)<br>:star:[code](https://github.com/jiequancui/Parametric-Contrastive-Learning)
  * [ACE: Ally Complementary Experts for Solving Long-Tailed Recognition in One-Shot](https://arxiv.org/abs/2108.02385)<br>:open_mouth:oral:star:[code](https://github.com/jrcai/ACE)
  * [Self Supervision to Distillation for Long-Tailed Visual Recognition](https://arxiv.org/abs/2109.04075)<br>:star:[code](https://github.com/MCG-NJU)
* 细粒度
  * [Webly Supervised Fine-Grained Recognition: Benchmark Datasets and An Approach](https://arxiv.org/abs/2108.02399)<br>:star:[code](https://github.com/NUST-Machine-Intelligence-Laboratory/weblyFG-dataset)
  * [Learning Canonical 3D Object Representation for Fine-Grained Recognition](https://arxiv.org/abs/2108.04628)
  * [Counterfactual Attention Learning for Fine-Grained Visual Categorization and Re-identification](https://arxiv.org/abs/2108.08728)<br>:star:[code](https://github.com/raoyongming/CAL)
* 小样本分类
  * [Transductive Few-Shot Classification on the Oblique Manifold](https://arxiv.org/abs/2108.04009)
  * [Relational Embedding for Few-Shot Classification](https://arxiv.org/abs/2108.09666)
  * [Binocular Mutual Learning for Improving Few-shot Classification](https://arxiv.org/abs/2108.12104)<br>:star:[code](https://github.com/ZZQzzq/BML)
  * [Partner-Assisted Learning for Few-Shot Image Classification](https://arxiv.org/abs/2109.07607)
  * [On the Importance of Distractors for Few-Shot Classification](https://arxiv.org/abs/2109.09883)<br>:star:[code](https://github.com/quantacode/Contrastive-Finetuning)
* 多标签分类
  * [Asymmetric Loss For Multi-Label Classification](https://arxiv.org/abs/2009.14119)<br>:star:[code](https://github.com/Alibaba-MIIL/ASL)
  * [Semantic Diversity Learning for Zero-Shot Multi-label Classification](https://arxiv.org/abs/2105.05926)<br>:star:[code](https://github.com/Alibaba-MIIL/ZS_SDL)

  
<a name="11"/>

## 11.Visual Question Answering(视觉问答)
* [Greedy Gradient Ensemble for Robust Visual Question Answering](https://arxiv.org/abs/2107.12651)<br>:star:[code](https://github.com/GeraldHan/GGE)
* [Weakly Supervised Relative Spatial Reasoning for Visual Question Answering](https://arxiv.org/abs/2109.01934)<br>:star:[code](https://github.com/pratyay-banerjee/weak_sup_vqa)
* [Calibrating Concepts and Operations: Towards Symbolic Reasoning on Real Images](https://arxiv.org/abs/2110.00519)<br>:star:[code](https://github.com/Lizw14/CaliCO)
* video question answering
  * [Just Ask: Learning to Answer Questions from Millions of Narrated Videos](https://arxiv.org/abs/2012.00451)<br>:open_mouth:oral:star:[code](https://github.com/antoyang/just-ask):house:[project](https://antoyang.github.io/just-ask.html)
* A-VQA
  * [Pano-AVQA: Grounded Audio-Visual Question Answering on 360∘ Videos](https://arxiv.org/abs/2110.05122)


<a name="10"/>

## 10.OCR
* [Joint Visual Semantic Reasoning: Multi-Stage Decoder for Text Recognition](https://arxiv.org/abs/2107.12090)<br>:tv:[video](https://www.youtube.com/watch?v=GPk-O3ZqIoI)
* [Text is Text, No Matter What: Unifying Text Recognition using Knowledge Distillation](https://arxiv.org/abs/2107.12087)<br>:tv:[video](https://www.youtube.com/watch?v=8VLkaf_hGdQ)
* [Towards the Unseen: Iterative Text Recognition by Distilling from Errors](https://arxiv.org/abs/2107.12081)<br>:tv:[video](https://www.youtube.com/watch?v=ywaGXFZIiDI)
* 任意形状文本检测
  * [Adaptive Boundary Proposal Network for Arbitrary Shape Text Detection](https://arxiv.org/abs/2107.12664)<br>:star:[code](https://github.com/GXYM/TextBPN)
* 场景文本识别
  * [From Two to One: A New Scene Text Recognizer with Visual Language Modeling Network](https://arxiv.org/abs/2108.09661)<br>:star:[code](https://github.com/wangyuxin87/VisionLAN)
* 场景文本替换
  * [STRIVE: Scene Text Replacement In Videos](https://arxiv.org/abs/2109.02762)<br>:house:[project](https://striveiccv2021.github.io/STRIVE-ICCV2021/)  
  

  
<a name="9"/>

## 9.Video
* Action Detection and Recognition(人体动作检测与识别)
  * [Channel-wise Topology Refinement Graph Convolution for Skeleton-Based Action Recognition](https://arxiv.org/abs/2107.12213)<br>:star:[code](https://github.com/Uason-Chen/CTR-GCN)
  * [MGSampler: An Explainable Sampling Strategy for Video Action Recognition](https://arxiv.org/abs/2104.09952)
  * [Skeleton Cloud Colorization for Unsupervised 3D Action Representation Learning](https://arxiv.org/abs/2108.01959)
  * [Video Pose Distillation for Few-Shot, Fine-Grained Sports Action Recognition](https://arxiv.org/abs/2109.01305)
  * [Class Semantics-based Attention for Action Detection](https://arxiv.org/abs/2109.02613)
  * [MultiSports: A Multi-Person Video Dataset of Spatio-Temporally Localized Sports Actions](https://arxiv.org/pdf/2105.07404.pdf)<br>:star:[code](https://github.com/MCG-NJU/MultiSports)
  * 零样本动作识别
    * [Elaborative Rehearsal for Zero-shot Action Recognition](https://arxiv.org/abs/2108.02833)<br>:star:[code](https://github.com/DeLightCMU/ElaborativeRehearsal)
  * Temporal Action Localization(时序动作定位)
    * [Enriching Local and Global Contexts for Temporal Action Localization](https://arxiv.org/abs/2107.12960)
    * [Learning Action Completeness from Points for Weakly-supervised Temporal Action Localization](https://arxiv.org/abs/2108.05029)<br>:open_mouth:oral:star:[code](https://github.com/Pilhyeon)
    * [Foreground-Action Consistency Network for Weakly Supervised Temporal Action Localization](https://arxiv.org/abs/2108.06524)<br>:star:[code](https://github.com/LeonHLJ/FAC-Net)
  * Temporal Action Proposal Generation(时序动作提案生成)
    * [Relaxed Transformer Decoders for Direct Action Proposal Generation](https://arxiv.org/abs/2102.01894)<br>:star:[code](https://github.com/MCG-NJU/RTD-Action)
* Action Quality Assessment(行动质量评估)
  * [Group-aware Contrastive Regression for Action Quality Assessment](https://arxiv.org/abs/2108.07797)
* Video Rescaling
  * [Self-Conditioned Probabilistic Learning of Video Rescaling](https://arxiv.org/abs/2107.11639)
* Video activity localisation
  * [Cross-Sentence Temporal and Semantic Relations in Video Activity Localisation](https://arxiv.org/abs/2107.11443)<br>:newspaper:解读:[ICCV2021 | 如何高效视频定位？QMUL&北大&Adobe强强联手提出弱监督CRM，性能SOTA](https://mp.weixin.qq.com/s/tlGzpUU56HWjVqDtVOkdWg)
* 视频修复
  * [Internal Video Inpainting by Implicit Long-range Propagation](https://arxiv.org/abs/2108.01912)<br>:star:[code](https://github.com/Tengfei-Wang/Implicit-Internal-Video-Inpainting):house:[project](https://tengfei-wang.github.io/Implicit-Internal-Video-Inpainting/)
  * [Occlusion-Aware Video Object Inpainting](https://arxiv.org/abs/2108.06765)<br>:house:[project](http://www.kelei.site/voin/)
  * [FuseFormer: Fusing Fine-Grained Information in Transformers for Video Inpainting](https://arxiv.org/abs/2109.02974)<br>:star:[code](https://github.com/ruiliu-ai/FuseFormer)
* 视频分析
  * 视频表征学习
    * [Enhancing Self-supervised Video Representation Learning via Multi-level Feature Optimization](https://arxiv.org/abs/2108.02183)
    * [Self-Supervised Video Representation Learning with Meta-Contrastive Network](https://arxiv.org/abs/2108.08426)
    * [Long Short View Feature Decomposition via Contrastive Video Representation Learning](https://arxiv.org/abs/2109.11593)
* 视频剪辑
  * [Learning to Cut by Watching Movies](https://arxiv.org/abs/2108.04294)<br>:star:[code](https://github.com/PardoAlejo/LearningToCut):house:[project](https://www.alejandropardo.net/publication/learning-to-cut/) 
* 视频字幕
  * Dense Video Captioning
    * [End-to-End Dense Video Captioning with Parallel Decoding](https://arxiv.org/abs/2108.07781)<br>:star:[code](https://github.com/ttengwang/PDVC)
* 视频编码
  * [Overfitting the Data: Compact Neural Video Delivery via Content-aware Feature Modulation](https://arxiv.org/abs/2108.08202)<br>:star:[code](https://github.com/Neural-video-delivery/CaFM-Pytorch-ICCV2021)<br>:newspaper:解读:[ICCV2021—工业界中的神经网络视频传输超分算法](https://mp.weixin.qq.com/s/dQCZaZNz0oCMHQQEdV79aA)
* 视频生成
  * [Click to Move: Controlling Video Generation with Sparse Motion](https://arxiv.org/abs/2108.08815)<br>:star:[code](https://github.com/PierfrancescoArdino/C2M)
* Video Relation Detection(视频关系检测)
  * [Social Fabric: Tubelet Compositions for Video Relation Detection](https://arxiv.org/abs/2108.08363)<br>:star:[code](https://github.com/shanshuo/Social-Fabric)
* Video Grounding
  * [Support-Set Based Cross-Supervision for Video Grounding](https://arxiv.org/abs/2108.10576)
* 视频精彩片段检测
  * [Cross-category Video Highlight Detection via Set-based Learning](https://arxiv.org/abs/2108.11770)<br>:star:[code](https://github.com/ChrisAllenMing/Cross_Category_Video_Highlight)
  * [PR-Net: Preference Reasoning for Personalized Video Highlight Detection](https://arxiv.org/abs/2109.01799)
  * [HighlightMe: Detecting Highlights from Human-Centric Videos](https://arxiv.org/abs/2110.01774)
* 视频识别
  * [Searching for Two-Stream Models in Multivariate Space for Video Recognition](https://arxiv.org/abs/2108.12957)
* Motion Retargeting(运动重定位)
  * [Contact-Aware Retargeting of Skinned Motion](https://arxiv.org/abs/2109.07431)<br>:tv:[video](https://www.youtube.com/watch?v=qQ4HO2Hibsk)
* 视频预测
  * [A Hierarchical Variational Neural Uncertainty Model for Stochastic Video Prediction](https://arxiv.org/abs/2110.03446)<br>:open_mouth:oral

<a name="8"/>

## 8.Human Pose Estimation(人体姿态估计)
* [Human Pose Regression with Residual Log-likelihood Estimation](https://arxiv.org/abs/2107.11291)<br>:open_mouth:oral:star:[code](https://github.com/Jeff-sjtu/res-loglikelihood-regression)
* [Online Knowledge Distillation for Efficient Pose Estimation](https://arxiv.org/abs/2108.02092)
* [DECA: Deep viewpoint-Equivariant human pose estimation using Capsule Autoencoders](https://arxiv.org/abs/2108.08557)<br>:open_mouth:oral:star:[code](https://github.com/mmlab-cv/DECA)
* [Estimating and Exploiting the Aleatoric Uncertainty in Surface Normal Estimation](https://arxiv.org/abs/2109.09881)<br>:open_mouth:oral:star:[code](https://github.com/baegwangbin/surface_normal_uncertainty)
* [Dynamical Pose Estimation](https://arxiv.org/abs/2103.06182)<br>:star:[code](https://github.com/hankyang94/DAMP):tv:[video](https://www.youtube.com/watch?v=S6L0h-d0IYM)
* 3D 人体姿态估计
  * [PyMAF: 3D Human Pose and Shape Regression with Pyramidal Mesh Alignment Feedback Loop](https://arxiv.org/abs/2103.16507)<br>:open_mouth:oral:star:[code](https://github.com/HongwenZhang/PyMAF):house:[project](https://hongwenzhang.github.io/pymaf/)
  * [HuMoR: 3D Human Motion Model for Robust Pose Estimation](https://arxiv.org/abs/2105.04668)<br>:open_mouth:oral:house:[project](https://geometry.stanford.edu/projects/humor/):tv:[video](https://youtu.be/5VWirxUHG0Y)
  * [Probabilistic Monocular 3D Human Pose Estimation with Normalizing Flows](https://arxiv.org/abs/2107.13788)<br>:star:[code](https://github.com/twehrbein/Probabilistic-Monocular-3D-Human-Pose-Estimation-with-Normalizing-Flows):tv:[video](https://www.youtube.com/watch?v=gaNX5CIl1L8)
  * [Learning Skeletal Graph Neural Networks for Hard 3D Pose Estimation](https://arxiv.org/abs/2108.07181)<br>:star:[code](https://github.com/ailingzengzzz/Skeletal-GNN)
  * [EventHPE: Event-based 3D Human Pose and Shape Estimation](https://arxiv.org/abs/2108.06819)<br>:star:[code](https://github.com/JimmyZou/EventHPE)
  * [imGHUM: Implicit Generative Models of 3D Human Shape and Articulated Pose](https://arxiv.org/abs/2108.10842)
  * [Graph-Based 3D Multi-Person Pose Estimation Using Multi-View Images](https://arxiv.org/abs/2109.05885)
  * [Unsupervised 3D Pose Estimation for Hierarchical Dance Video Recognition](https://arxiv.org/abs/2109.09166)
  * [Learning to Regress Bodies from Images using Differentiable Semantic Rendering](https://arxiv.org/abs/2110.03480)<br>:house:[project](https://dsr.is.tue.mpg.de/)
  * [Hierarchical Kinematic Probability Distributions for 3D Human Shape and Pose Estimation from Images in the Wild](https://arxiv.org/abs/2110.00990)<br>:star:[code](https://github.com/akashsengupta1997/HierarchicalProbabilistic3DHuman)
* 手势识别
  * [Hand Image Understanding via Deep Multi-Task Learning](https://arxiv.org/abs/2107.11646)
* 3D 手部姿态
  * [HandFoldingNet: A 3D Hand Pose Estimation Network Using Multiscale-Feature Guided Folding of a 2D Hand Skeleton](https://arxiv.org/abs/2108.05545)<br>:star:[code](https://github.com/cwc1260/HandFold)
* 3D姿势迁移
  * [Unsupervised Geodesic-preserved Generative Adversarial Networks for Unconstrained 3D Pose Transfer](https://arxiv.org/abs/2108.07520)<br>:star:[code](https://github.com/mikecheninoulu/Unsupervised_IEPGAN)
* 手势合成
  * [Speech Drives Templates: Co-Speech Gesture Synthesis with Learned Templates](https://arxiv.org/abs/2108.08020)(https://github.com/ShenhanQian/SpeechDrivesTemplates)
* 三维网格合成
  * [Deep Hybrid Self-Prior for Full 3D Mesh Generation](https://arxiv.org/abs/2108.08017)<br>:house:[project](https://yqdch.github.io/DHSP3D/)
* 人体重建
  * [ARCH++: Animation-Ready Clothed Human Reconstruction Revisited](https://arxiv.org/abs/2108.07845)<br>:tv:[video](https://www.youtube.com/watch?v=kNtlheGLSR8)
  * 3D 人体重建
    * [Probabilistic Modeling for Human Mesh Recovery](https://arxiv.org/abs/2108.11944)<br>:star:[code](https://github.com/nkolot/ProHMR):house:[project](https://www.seas.upenn.edu/~nkolot/projects/prohmr/)
* 三维人体重建
  * [Gravity-Aware Monocular 3D Human-Object Reconstruction](https://arxiv.org/abs/2108.08844)<br>:house:[project](http://4dqv.mpi-inf.mpg.de/GraviCap/)
* 4D人体捕捉
  * [Learning Motion Priors for 4D Human Body Capture in 3D Scenes](https://arxiv.org/abs/2108.10399)<br>:star:[code](https://github.com/sanweiliti/LEMO):house:[project](https://sanweiliti.github.io/LEMO/LEMO.html):tv:[video](https://youtu.be/ly8UaeFqFhw) 
* 人体姿态估计与合成
  * [Physics-based Human Motion Estimation and Synthesis from Videos](https://arxiv.org/abs/2109.09913)
* 多人姿态估计 
  * [Shape-aware Multi-Person Pose Estimation from Multi-View Images](https://arxiv.org/abs/2110.02330)<br>:star:[code](https://github.com/zj-dong/Multi-Person-Pose-Estimation):house:[project](https://ait.ethz.ch/projects/2021/multi-human-pose/):tv:[video](https://www.youtube.com/watch?v=KE5Jpnyqmh4)<br>论文公开
  * [The Center of Attention: Center-Keypoint Grouping via Attention for Multi-Person Pose Estimation](https://arxiv.org/abs/2110.05132)<br>:star:[code](https://github.com/dvl-tum)
* 人/物体姿态关键点检测
  * [Keypoint Communities](https://arxiv.org/abs/2110.00988)<br>:star:[code](https://github.com/DuncanZauss/Keypoint_Communities)
* 人体运动捕捉
  * [SOMA: Solving Optical Marker-Based MoCap Automatically](https://arxiv.org/abs/2110.04431)<br>:star:[code](https://github.com/nghorbani/soma):house:[project](https://soma.is.tue.mpg.de/):tv:[video](https://youtu.be/BEFCqIefLA8)


<a name="7"/>

## 7.Scene Graph Generation(场景图生成)
* [Spatial-Temporal Transformer for Dynamic Scene Graph Generation](https://arxiv.org/abs/2107.12309)
* [Unconditional Scene Graph Generation](https://arxiv.org/abs/2108.05884)<br>:house:[project](https://scenegraphgen.github.io/)
* [Target Adaptive Context Aggregation for Video Scene Graph Generation](https://arxiv.org/abs/2108.08121)<br>:star:[code](https://github.com/MCG-NJU/TRACE)
* [Learning to Generate Scene Graph from Natural Language Supervision](https://arxiv.org/pdf/2109.02227)<br>:star:[code](https://github.com/YiwuZhong/SGG_from_NLS)


<a name="6"/>

## 6.Point Cloud(点云)
* [AdaFit: Rethinking Learning-based Normal Estimation on Point Clouds](https://arxiv.org/abs/2108.05836)<br>:star:[code](https://github.com/Runsong123/AdaFit):house:[project](https://runsong123.github.io/AdaFit/)
* [Adaptive Graph Convolution for Point Cloud Analysis](https://arxiv.org/abs/2108.08035)<br>:star:[code](https://github.com/hrzhou2/AdaptConv-master)
* [Learning Inner-Group Relations on Point Clouds](https://arxiv.org/abs/2108.12468)
* [CPFN: Cascaded Primitive Fitting Networks for High-Resolution Point Clouds](https://arxiv.org/abs/2109.00113)<br>:star:[code](https://github.com/erictuanle/CPFN)
* [Cloud Transformers: A Universal Approach To Point Cloud Processing Tasks](https://arxiv.org/abs/2007.11679)<br>:star:[code](https://github.com/saic-vul/cloud_transformers):tv:[video](https://www.youtube.com/watch?v=lYTzLhy-ybw) 
* [PCAM: Product of Cross-Attention Matrices for Rigid Registration of Point Clouds](https://arxiv.org/abs/2110.01269)<br>:star:[code](https://github.com/valeoai/PCAM)
* 点云去噪
  * [Score-Based Point Cloud Denoising](https://arxiv.org/abs/2107.10981)
* 点云配准
  * [HRegNet: A Hierarchical Network for Large-scale Outdoor LiDAR Point Cloud Registration](https://arxiv.org/abs/2107.11992)<br>:star:[code](https://github.com/ispc-lab/HRegNet):house:[project](https://ispc-group.github.io/hregnet)
  * [(Just) A Spoonful of Refinements Helps the Registration Error Go Down](https://arxiv.org/abs/2108.03257https://arxiv.org/abs/2108.03257)<br>:open_mouth:oral:star:[code](https://github.com/SergioRAgostinho/just-a-spoonful)
  * [A Robust Loss for Point Cloud Registration](https://arxiv.org/abs/2108.11682)
  * [Deep Hough Voting for Robust Global Registration](https://arxiv.org/abs/2109.04310)
  * [Sampling Network Guided Cross-Entropy Method for Unsupervised Point Cloud Registration](https://arxiv.org/abs/2109.06619)<br>:star:[code](https://github.com/Jiang-HB/CEMNet)
* 3D点云
  * [Unsupervised Learning of Fine Structure Generation for 3D Point Clouds by 2D Projection Matching](https://arxiv.org/abs/2108.03746)<br>:star:[code](https://github.com/chenchao15/2D_projection_matching)
  * [Spatio-temporal Self-Supervised Representation Learning for 3D Point Clouds](https://arxiv.org/abs/2109.00179)<br>:star:[code](https://github.com/yichen928/STRL):house:[project](https://siyuanhuang.com/STRL/)
* 点云补全
  * [SnowflakeNet: Point Cloud Completion by Snowflake Point Deconvolution with Skip-Transformer](https://arxiv.org/abs/2108.04444)
  * [ME-PCN: Point Completion Conditioned on Mask Emptiness](https://arxiv.org/abs/2108.08187)
  * [PoinTr: Diverse Point Cloud Completion with Geometry-Aware Transformers](https://arxiv.org/abs/2108.08839)<br>:open_mouth:oral:star:[code](https://github.com/yuxumin/PoinTr)
  * [Voxel-based Network for Shape Completion by Leveraging Edge Generation](https://arxiv.org/abs/2108.09936)<br>:star:[code](https://github.com/xiaogangw/VE-PCN)
* 点云增强
  * [Point Cloud Augmentation with Weighted Local Transformations](https://arxiv.org/abs/2110.05379)<br>:star:[code](https://github.com/mlvlab/PointWOLF)

<a name="5"/>

## 5.Few-Shot/Zero-Shot Learning;Domain Generalization/Adaptation(小/零样本学习;域适应/泛化)
* 域适应
  * [Transporting Causal Mechanisms for Unsupervised Domain Adaptation](https://arxiv.org/abs/2107.11055)<br>:open_mouth:oral
  * [Generalized Source-free Domain Adaptation](https://arxiv.org/abs/2108.01614)<br>:star:[code](https://github.com/Albert0147/G-SFDA)
  * [Semantic Concentration for Domain Adaptation](https://arxiv.org/abs/2108.05720)<br>:star:[code](https://github.com/BIT-DA)
  * [PIT: Position-Invariant Transform for Cross-FoV Domain Adaptation](https://arxiv.org/abs/2108.07142)<br>:star:[code](https://github.com/sheepooo/PIT-Position-Invariant-Transform)
  * [Learning Cross-modal Contrastive Features for Video Domain Adaptation](https://arxiv.org/abs/2108.11974)
  * 无监督域适应
    * [Adversarial Unsupervised Domain Adaptation with Conditional and Label Shift: Infer, Align and Iterate](https://arxiv.org/abs/2107.13469)
    * [Recursively Conditional Gaussian for Ordinal Unsupervised Domain Adaptation](https://arxiv.org/abs/2107.13467)<br>:open_mouth:oral
    * [Tune it the Right Way: Unsupervised Validation of Domain Adaptation via Soft Neighborhood Density](https://arxiv.org/abs/2108.10860)
    * [Adversarial Robustness for Unsupervised Domain Adaptation](https://arxiv.org/abs/2109.00946)<br>:house:[project](http://deepawais.com/robust_uda/)
  * 零样本域适应
    * [Zero-Shot Domain Adaptation with a Physics Prior](https://arxiv.org/abs/2108.05137)<br>:open_mouth:oral:star:[code](https://github.com/Attila94/CIConv)
* 域泛化
  * [Domain Generalization via Gradient Surgery](https://arxiv.org/abs/2108.01621)<br>:star:[code](https://github.com/lucasmansilla/DGvGS)
  * [Learning to Diversify for Single Domain Generalization](https://arxiv.org/abs/2108.11726)<br>:star:[code](https://github.com/BUserName/Learning_to_diversify)
  * [Shape-Biased Domain Generalization via Shock Graph Embeddings](https://arxiv.org/abs/2109.05671)
* 小样本
  * [Boosting the Generalization Capability in Cross-Domain Few-shot Learning via Noise-enhanced Supervised Autoencoder](https://arxiv.org/abs/2108.05028)
  * [Generalized and Incremental Few-Shot Learning by Explicit Learning and Calibration without Forgetting](https://arxiv.org/abs/2108.08165)<br>:star:[code](https://github.com/annusha/LCwoF)
  * [Meta Navigator: Search for a Good Adaptation Policy for Few-shot Learning](https://arxiv.org/abs/2109.05749)
  * [Meta-Learning with Task-Adaptive Loss Function for Few-Shot Learning](https://arxiv.org/abs/2110.03909)<br>:open_mouth:oral:star:[code](https://github.com/baiksung)
* Zero-Shot Learning(零样本学习)
  * [Discriminative Region-based Multi-Label Zero-Shot Learning](https://arxiv.org/abs/2108.09301)<br>:star:[code](https://github.com/akshitac8/BiAM)
  * [Field-Guide-Inspired Zero-Shot Learning](https://arxiv.org/abs/2108.10967)
  * Generalized Zero-Shot Learning(广义零样本学习)
    * [FREE: Feature Refinement for Generalized Zero-Shot Learning](https://arxiv.org/abs/2107.13807)<br>:star:[code](https://github.com/shiming-chen/FREE)
 
<a name="4"/>

## 4.Neural rendering(神经渲染)
* [In-Place Scene Labelling and Understanding with Implicit Scene Representation](https://arxiv.org/abs/2103.15875)<br>:open_mouth:oral:house:[project](https://shuaifengzhi.com/Semantic-NeRF/):tv:[video](https://www.youtube.com/watch?v=FpShWO7LVbM)
* [Differentiable Surface Rendering via Non-Differentiable Sampling](https://arxiv.org/abs/2108.04886)
* [Self-Calibrating Neural Radiance Fields](https://arxiv.org/abs/2108.13826)<br>:star:[code](https://github.com/POSTECH-CVLab/SCNeRF)
* [NerfingMVS: Guided Optimization of Neural Radiance Fields for Indoor Multi-view Stereo](https://arxiv.org/abs/2109.01129)<br>:open_mouth:oral:star:[code](https://github.com/weiyithu/NerfingMVS):house:[project](https://weiyithu.github.io/NerfingMVS/)
* [Learning Object-Compositional Neural Radiance Field for Editable Scene Rendering](https://arxiv.org/abs/2109.01847)<br>:star:[code](https://github.com/zju3dv/object_nerf):house:[project](https://zju3dv.github.io/object_nerf/)
* [CodeNeRF: Disentangled Neural Radiance Fields for Object Categories](https://arxiv.org/abs/2109.01750)<br>:star:[code](https://github.com/wayne1123/code-nerf)
* [MVSNeRF: Fast Generalizable Radiance Field Reconstruction from Multi-View Stereo](https://arxiv.org/abs/2103.15595)<br>:star:[code](https://github.com/apchenstu/mvsnerf):house:[project](https://apchenstu.github.io/mvsnerf/):tv:[video](https://youtu.be/3M3edNiaGsA)
* 3D photography(3D 相片)
  * [SLIDE: Single Image 3D Photography with Soft Layering and Depth-aware Inpainting](https://arxiv.org/abs/2109.01068)<br>:open_mouth:oral:house:[project](https://varunjampani.github.io/slide/):tv:[video](https://www.youtube.com/watch?v=RQio7q-ueY8)

<a name="3"/>

## 3.Image Clustering(图像聚类)
* [Clustering by Maximizing Mutual Information Across Views](https://arxiv.org/abs/2107.11635)

<a name="2"/>

## 2.Sign Language(手语识别)
* [Mixed SIGNals: Sign Language Production via a Mixture of Motion Primitives](https://arxiv.org/abs/2107.11317)
* [SignBERT: Pre-Training of Hand-Model-Aware Representation for Sign Language Recognition](https://arxiv.org/abs/2110.05382)

<a name="1"/>

## 1.Other(其它)
* [Bias Loss for Mobile Neural Networks](https://arxiv.org/abs/2107.11170)
* [Improve Unsupervised Pretraining for Few-label Transfer](https://arxiv.org/abs/2107.12369)
* [Temporal-wise Attention Spiking Neural Networks for Event Streams Classification](https://arxiv.org/abs/2107.11711)
* [Accelerating Atmospheric Turbulence Simulation via Learned Phase-to-Space Transform](https://arxiv.org/abs/2107.11627)
* [Energy-Based Open-World Uncertainty Modeling for Confidence Calibration](https://arxiv.org/abs/2107.12628)
* [Robustness via Cross-Domain Ensembles](https://arxiv.org/abs/2103.10919)<br>:open_mouth:oral:star:[code](https://github.com/EPFL-VILAB/XDEnsembles):house:[project](https://crossdomain-ensembles.epfl.ch/):tv:[video](https://youtu.be/h0FI5Sp7y7g)
* [Warp Consistency for Unsupervised Learning of Dense Correspondences](https://arxiv.org/abs/2104.03308)<br>:open_mouth:oral:star:[code](https://github.com/PruneTruong/DenseMatching)
* [Few-Shot and Continual Learning with Attentive Independent Mechanisms](https://arxiv.org/abs/2107.14053)<br>:star:[code](https://github.com/huang50213/AIM-Fewshot-Continual)
* [Out-of-Core Surface Reconstruction via Global TGV Minimization](https://arxiv.org/abs/2107.14790)
* [ELLIPSDF: Joint Object Pose and Shape Optimization with a Bi-level Ellipsoid and Signed Distance Function Description](https://arxiv.org/abs/2108.00355)
* [Multi-scale Matching Networks for Semantic Correspondence](https://arxiv.org/abs/2108.00211)
* [Learning with Noisy Labels via Sparse Regularization](https://arxiv.org/abs/2108.00192)<br>:star:[code](https://github.com/hitcszx/lnl_sr)
* [CanvasVAE: Learning to Generate Vector Graphic Documents](https://arxiv.org/abs/2108.01249)
* [Toward Spatially Unbiased Generative Models](https://arxiv.org/abs/2108.01285)<br>:star:[code](https://github.com/jychoi118/toward_spatial_unbiased)
* [Learning Compatible Embeddings](https://arxiv.org/abs/2108.01958)<br>:star:[code](https://github.com/IrvingMeng/LCE)
* [Instance Similarity Learning for Unsupervised Feature Representation](https://arxiv.org/abs/2108.02721)<br>:star:[code](https://github.com/ZiweiWangTHU/ISL)
* [Generalizable Mixed-Precision Quantization via Attribution Rank Preservation](https://arxiv.org/abs/2108.02720)<br>:star:[code](https://github.com/ZiweiWangTHU/GMPQ)
* [Unifying Nonlocal Blocks for Neural Networks](https://arxiv.org/abs/2108.02451)<br>:star:[code](https://github.com/zh460045050/SNL_ICCV2021)
* [Impact of Aliasing on Generalization in Deep Convolutional Networks](https://arxiv.org/abs/2108.03489)
* [NASOA: Towards Faster Task-oriented Online Fine-tuning with a Zoo of Models](https://arxiv.org/abs/2108.03434)<br>:star:[code](https://github.com/NAS-OA/NASOA)
* [ProAI: An Efficient Embedded AI Hardware for Automotive Applications - a Benchmark Study](https://arxiv.org/abs/2108.05170)
* [m-RevNet: Deep Reversible Neural Networks with Momentum](https://arxiv.org/abs/2108.05862) [涉嫌学术不端，已申请撤稿](https://www.zhihu.com/question/480075870/answer/2064860328)
* [Continual Neural Mapping: Learning An Implicit Scene Representation from Sequential Observations](https://arxiv.org/abs/2108.05851)
* [MT-ORL: Multi-Task Occlusion Relationship Learning](https://arxiv.org/abs/2108.05722)<br>:star:[code](https://github.com/fengpanhe/MT-ORL) 
* [Finding Representative Interpretations on Convolutional Neural Networks](https://arxiv.org/abs/2108.06384)
* [Collaborative Unsupervised Visual Representation Learning from Decentralized Data](https://arxiv.org/abs/2108.06492)
* 异常检测
  * [Weakly Supervised Temporal Anomaly Segmentation with Dynamic Time Warping](https://arxiv.org/abs/2108.06816)
* [Orthogonal Jacobian Regularization for Unsupervised Disentanglement in Image Generation](https://arxiv.org/abs/2108.07668)<br>:star:[code](https://github.com/csyxwei/OroJaR)
* [PR-RRN: Pairwise-Regularized Residual-Recursive Networks for Non-rigid Structure-from-Motion](https://arxiv.org/abs/2108.07506)
* [Instance Segmentation in 3D Scenes using Semantic Superpoint Tree Networks](https://arxiv.org/abs/2108.07478)<br>:star:[code](https://github.com/Gorilla-Lab-SCUT)
* [Learning RAW-to-sRGB Mappings with Inaccurately Aligned Supervision](https://arxiv.org/abs/2108.08119)<br>:star:[code](https://github.com/cszhilu1998/RAW-to-sRGB)
* [Structured Outdoor Architecture Reconstruction by Exploration and Classification](https://arxiv.org/abs/2108.07990)
* [Global Pooling, More than Meets the Eye: Position Information is Encoded Channel-Wise in CNNs](https://arxiv.org/abs/2108.07884)<br>:star:[code](https://github.com/islamamirul/PermuteNet)
* [A New Journey from SDRTV to HDRTV](https://arxiv.org/abs/2108.07978)<br>:star:[code](https://github.com/chxy95/HDRTVNet)
* [A Simple Framework for 3D Lensless Imaging with Programmable Masks](https://arxiv.org/abs/2108.07966)<br>:star:[code](https://github.com/CSIPlab/Programmable3Dcam)
* [Causal Attention for Unbiased Visual Recognition](https://arxiv.org/abs/2108.08782)<br>:star:[code](https://github.com/Wangt-CN/CaaM)
* [Learning to Match Features with Seeded Graph Matching Network](https://arxiv.org/abs/2108.08771)<br>:star:[code](https://github.com/vdvchen/SGMNet)
* [Amplitude-Phase Recombination: Rethinking Robustness of Convolutional Neural Networks in Frequency Domain](https://arxiv.org/abs/2108.08487)
* [PatchMatch-RL: Deep MVS with Pixelwise Depth, Normal, and Visibility](https://arxiv.org/abs/2108.08943)<br>:open_mouth:oral
* [Towards Understanding the Generative Capability of Adversarially Robust Classifiers](https://arxiv.org/abs/2108.09093)<br>:open_mouth:oral
* [Ranking Models in Unlabeled New Environments](https://arxiv.org/abs/2108.10310)<br>:star:[code](https://github.com/sxzrt/Proxy-Set)
* [Learning of Visual Relations: The Devil is in the Tails](https://arxiv.org/abs/2108.09668)
* [BlockCopy: High-Resolution Video Processing with Block-Sparse Feature Propagation and Online Policies](https://arxiv.org/abs/2108.09376)
* [Patch2CAD: Patchwise Embedding Learning for In-the-Wild Shape Retrieval from a Single Image](https://arxiv.org/abs/2108.09368)
* 去偏差
  * [BiaSwap: Removing dataset bias with bias-tailored swapping augmentation](https://arxiv.org/abs/2108.10008)
* [Full-Velocity Radar Returns by Radar-Camera Fusion](https://arxiv.org/abs/2108.10637)
* [CSG-Stump: A Learning Friendly CSG-Like Representation for Interpretable Shape Parsing](https://arxiv.org/abs/2108.11305)<br>:star:[code](https://github.com/kimren227/CSGStumpNet):house:[project](https://kimren227.github.io/projects/CSGStump/)
* [NGC: A Unified Framework for Learning with Open-World Noisy Data](https://arxiv.org/abs/2108.11035)
* [LocTex: Learning Data-Efficient Visual Representations from Localized Textual Supervision](https://arxiv.org/abs/2108.11950)<br>:house:[project](https://loctex.mit.edu/)
* [Unsupervised Dense Deformation Embedding Network for Template-Free Shape Correspondence](https://arxiv.org/abs/2108.11609)
* [Lifelong Infinite Mixture Model Based on Knowledge-Driven Dirichlet Process](https://arxiv.org/abs/2108.12278)<br>:star:[code](https://github.com/dtuzi123/Lifelong-infinite-mixture-model)
* [Digging into Uncertainty in Self-supervised Multi-view Stereo](https://arxiv.org/abs/2108.12966)
* [Learning to Discover Reflection Symmetry via Polar Matching Convolution](https://arxiv.org/abs/2108.12952)
* [A Dual Adversarial Calibration Framework for Automatic Fetal Brain Biometry](https://arxiv.org/abs/2108.12719)
* [The Functional Correspondence Problem](https://arxiv.org/abs/2109.01097)
* [The Animation Transformer: Visual Correspondence via Segment Matching](https://arxiv.org/abs/2109.02614)
* [Parsing Table Structures in the Wild](https://arxiv.org/abs/2109.02199)
* [Square Root Marginalization for Sliding-Window Bundle Adjustment](https://arxiv.org/abs/2109.02182)
* [Hierarchical Object-to-Zone Graph for Object Navigation](https://arxiv.org/abs/2109.02066)
* [Dual Transfer Learning for Event-based End-task Prediction via Pluggable Event to Image Translation](https://arxiv.org/abs/2109.01801)
* [Robustness and Generalization via Generative Adversarial Training](https://arxiv.org/abs/2109.02765)
* [Learning Fast Sample Re-weighting Without Reward Data](https://arxiv.org/abs/2109.03216)<br>:star:[code](https://github.com/google-research/google-research/tree/master/ieg)
* [ReconfigISP: Reconfigurable Camera Image Processing Pipeline](https://arxiv.org/abs/2109.04760)<br>:house:[project](https://www.mmlab-ntu.com/project/reconfigisp/)
* [Learning Indoor Inverse Rendering with 3D Spatially-Varying Lighting](https://arxiv.org/abs/2109.06061)<br>:open_mouth:oral
* [Low-Shot Validation: Active Importance Sampling for Estimating Classifier Performance on Rare Categories](https://arxiv.org/abs/2109.05720)
* [DisUnknown: Distilling Unknown Factors for Disentanglement Learning](https://arxiv.org/abs/2109.08090)<br>:star:[code](https://github.com/stormraiser/disunknown):house:[project](https://stormraiser.github.io/disunknown/)
* [S3VAADA: Submodular Subset Selection for Virtual Adversarial Active Domain Adaptation](https://arxiv.org/abs/2109.08901)<br>:house:[project](https://sites.google.com/iisc.ac.in/s3vaada-iccv2021)
* [ALADIN: All Layer Adaptive Instance Normalization for Fine-grained Style Similarity](https://arxiv.org/abs/2103.09776)<br>:tv:[video](https://www.youtube.com/watch?v=TxE1_juIHqY)
* [Photon-Starved Scene Inference using Single Photon Cameras](https://arxiv.org/abs/2107.11001)<br>:tv:[video](https://www.youtube.com/watch?v=r1YvHnGbi6k)
* [OSCAR-Net: Object-centric Scene Graph Attention for Image Attribution](https://arxiv.org/abs/2108.03541)<br>:star:[code](https://github.com/exnx/oscar):house:[project](https://exnx.github.io/oscar/)
* [Learning to Estimate Hidden Motions with Global Motion Aggregation](https://arxiv.org/abs/2104.02409)<br>:star:[code](https://github.com/zacjiang/GMA):tv:[video](https://www.youtube.com/watch?v=cBNSQ8ZFKSE)
* [Modelling Neighbor Relation in Joint Space-Time Graph for Video Correspondence Learning](https://arxiv.org/abs/2109.13499)
* [Meta Learning on a Sequence of Imbalanced Domains with Difficulty Awareness](https://arxiv.org/abs/2109.14120)<br>:star:[code](https://github.com/joey-wang123/Imbalancemeta) 
* [Procedure Planning in Instructional Videosvia Contextual Modeling and Model-based Policy Learning](https://arxiv.org/abs/2110.01770)<br>:open_mouth:oral:
* [Extensions of Karger's Algorithm: Why They Fail in Theory and How They Are Useful in Practice](https://arxiv.org/abs/2110.02750)<br>:open_mouth:oral:
* [Neural Strokes: Stylized Line Drawing of 3D Shapes](https://arxiv.org/abs/2110.03900)<br>:star:[code](https://github.com/DifanLiu/NeuralStrokes)
* [Learning Realistic Human Reposing using Cyclic Self-Supervision with 3D Shape, Pose, and Appearance Consistency](https://arxiv.org/abs/2110.05458)
* [Omnidata: A Scalable Pipeline for Making Multi-Task Mid-Level Vision Datasets from 3D Scans](https://arxiv.org/abs/2110.04994)<br>:house:[project](https://omnidata.vision/)



