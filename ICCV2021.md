# ICCV2021最新信息及已接收论文/代码(持续更新)

<div align="center">
  <img src="image/ICCV2021.png"/>
</div>

官网链接：http://iccv2021.thecvf.com/home<br>
开会时间：2021年10月11日至17日<br>

# :exclamation::exclamation::exclamation::star2::star2::star2:📗📗📗ICCV 2021收录论文已全部公布，下载可在【我爱计算机视觉】后台回复“paper”，即可收到。共计 1612 篇。

### :exclamation::exclamation::exclamation::star2::star2::star2:ICCV 2021收录论文，10 月 14 日整理如下：

* 自监督
  * [Contrast and Order Representations for Video Self-Supervised Learning](https://openaccess.thecvf.com/content/ICCV2021/papers/Hu_Contrast_and_Order_Representations_for_Video_Self-Supervised_Learning_ICCV_2021_paper.pdf)
  * [On Feature Decorrelation in Self-Supervised Learning](https://arxiv.org/abs/2105.00470)<br>:open_mouth:oral
* GAN
  * [Latent Transformations via NeuralODEs for GAN-Based Image Editing](https://openaccess.thecvf.com/content/ICCV2021/papers/Khrulkov_Latent_Transformations_via_NeuralODEs_for_GAN-Based_Image_Editing_ICCV_2021_paper.pdf)
  * [Reality Transform Adversarial Generators for Image Splicing Forgery Detection and Localization](https://openaccess.thecvf.com/content/ICCV2021/papers/Bi_Reality_Transform_Adversarial_Generators_for_Image_Splicing_Forgery_Detection_and_ICCV_2021_paper.pdf)
* 视图合成
  * [Geometry-Free View Synthesis: Transformers and No 3D Priors](https://arxiv.org/abs/2104.07652)<br>:star:[code](https://github.com/CompVis/geometry-free-view-synthesis)
* Image translation
  * [Scaling-up Disentanglement for Image Translation](https://arxiv.org/abs/2103.14017)<br>:star:[code](https://github.com/avivga/overlord):house:[project](http://www.vision.huji.ac.il/overlord/)
* 三维人脸动画
  * [MeshTalk: 3D Face Animation From Speech Using Cross-Modality Disentanglement](https://openaccess.thecvf.com/content/ICCV2021/papers/Richard_MeshTalk_3D_Face_Animation_From_Speech_Using_Cross-Modality_Disentanglement_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/facebookresearch/meshtalk):tv:[video](https://research.fb.com/wp-content/uploads/2021/04/mesh_talk.mp4)
* SR
  * [Learning a Single Network for Scale-Arbitrary Super-Resolution](https://arxiv.org/abs/2004.03791)
* Salient Object Ranking
  * [Salient Object Ranking with Position-Preserved Attention](https://arxiv.org/abs/2106.05047)<br>:star:[code](https://github.com/EricFH/SOR)
* 目标检测
  * [DetCo: Unsupervised Contrastive Learning for Object Detection](https://arxiv.org/abs/2102.04803)<br>:star:[code](https://github.com/xieenze/DetCo)
* 视频 SR
  * [Omniscient Video Super-Resolution](https://arxiv.org/abs/2103.15683)<br>:star:[code](https://github.com/psychopa4/OVSR)
* Reid
  * [Clothing Status Awareness for Long-Term Person Re-Identification](https://openaccess.thecvf.com/content/ICCV2021/papers/Huang_Clothing_Status_Awareness_for_Long-Term_Person_Re-Identification_ICCV_2021_paper.pdf)
* 图像增强
  * [Representative Color Transform for Image Enhancement](https://openaccess.thecvf.com/content/ICCV2021/papers/Kim_Representative_Color_Transform_for_Image_Enhancement_ICCV_2021_paper.pdf)
* 表明重建
  * [Temporally-Coherent Surface Reconstruction via Metric-Consistent Atlases](https://arxiv.org/abs/2104.06950)
* Transformer
  * [Multi-Scale Vision Longformer: A New Vision Transformer for High-Resolution Image Encoding](https://arxiv.org/abs/2103.15358)<br>:star:[code](https://github.com/microsoft/vision-longformer)
  * [LeViT: A Vision Transformer in ConvNet's Clothing for Faster Inference](https://openaccess.thecvf.com/content/ICCV2021/papers/Graham_LeViT_A_Vision_Transformer_in_ConvNets_Clothing_for_Faster_Inference_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/facebookresearch/LeViT)
  * [DocFormer: End-to-End Transformer for Document Understanding](https://arxiv.org/abs/2106.11539)
* 对抗学习
  * [Low Curvature Activations Reduce Overfitting in Adversarial Training](https://arxiv.org/abs/2102.07861)
  * [Black-Box Detection of Backdoor Attacks With Limited Information and Data](https://arxiv.org/abs/2103.13127)
* 视频识别
  * [Adaptive Focus for Efficient Video Recognition](https://arxiv.org/abs/2105.03245)<br>:open_mouth:oral:star:[code](https://github.com/blackfeather-wang/AdaFocus)
  * [AdaMML: Adaptive Multi-Modal Learning for Efficient Video Recognition](https://arxiv.org/abs/2105.05165)<br>:star:[code](https://github.com/IBM/AdaMML):house:[project](https://rpand002.github.io/adamml.html)
* 音频-手势
  * [Audio2Gestures: Generating Diverse Gestures from Speech Audio with Conditional Variational Autoencoders](https://arxiv.org/abs/2108.06720)<br>:house:[project](https://openaccess.thecvf.com/content/ICCV2021/papers/Xiang_SnowflakeNet_Point_Cloud_Completion_by_Snowflake_Point_Deconvolution_With_Skip-Transformer_ICCV_2021_paper.pdf)
* 域适应
  * [Active Universal Domain Adaptation](https://openaccess.thecvf.com/content/ICCV2021/papers/Ma_Active_Universal_Domain_Adaptation_ICCV_2021_paper.pdf)
* NAS
  * [FairNAS: Rethinking Evaluation Fairness of Weight Sharing Neural Architecture Search](https://arxiv.org/abs/1907.01845)<br>:star:[code](https://github.com/xiaomi-automl/FairNAS)
* CMA  
  * [Towards Better Explanations of Class Activation Mapping](https://arxiv.org/abs/2102.05228)
  * [Keep CALM and Improve Visual Feature Attribution](https://arxiv.org/abs/2106.07861)<br>:star:[code](https://github.com/naver-ai/calm)
* 人群计数
  * [Exploiting Sample Correlation for Crowd Counting With Multi-Expert Network](https://openaccess.thecvf.com/content/ICCV2021/papers/Liu_Exploiting_Sample_Correlation_for_Crowd_Counting_With_Multi-Expert_Network_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/streamer-AP)
* 点云补全
  * [RFNet: Recurrent Forward Network for Dense Point Cloud Completion](https://arxiv.org/abs/2104.00820)
* 对比学习
  * [LatentCLR: A Contrastive Learning Approach for Unsupervised Discovery of Interpretable Directions](https://arxiv.org/abs/2104.00820)<br>:star:[code](https://github.com/catlab-team/latentclr)
* 自动驾驶
  * [Learn-to-Race: A Multimodal Control Environment for Autonomous Racing](https://openaccess.thecvf.com/content/ICCV2021/papers/Herman_Learn-To-Race_A_Multimodal_Control_Environment_for_Autonomous_Racing_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/learn-to-race/l2r)
* 量化
  * [Improving Low-Precision Network Quantization via Bin Regularization](https://openaccess.thecvf.com/content/ICCV2021/papers/Han_Improving_Low-Precision_Network_Quantization_via_Bin_Regularization_ICCV_2021_paper.pdf)
* 长尾识别
  * [Distilling Virtual Examples for Long-Tailed Recognition](https://arxiv.org/abs/2103.15042)
  * [Distributional Robustness Loss for Long-Tail Learning](https://arxiv.org/abs/2104.03066)
* 数据集
  * [Large Scale Multi-Illuminant (LSMI) Dataset for Developing White Balance Algorithm Under Mixed Illumination](https://openaccess.thecvf.com/content/ICCV2021/papers/Kim_Large_Scale_Multi-Illuminant_LSMI_Dataset_for_Developing_White_Balance_Algorithm_ICCV_2021_paper.pdf)<br>:sunflower:[dataset](https://github.com/DY112/LSMI-dataset)
* 行人行为预测
  * [Bifold and Semantic Reasoning for Pedestrian Behavior Prediction](https://arxiv.org/abs/2012.03298)
* 目标跟踪
  * [Learning Target Candidate Association To Keep Track of What Not To Track](https://arxiv.org/abs/2103.16556)<br>:star:[code](https://github.com/visionml/pytracking)
* 点云
  * [Superpoint Network for Point Cloud Oversegmentation](https://openaccess.thecvf.com/content/ICCV2021/papers/Hui_Superpoint_Network_for_Point_Cloud_Oversegmentation_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/fpthink/SPNet)
* 视线跟踪
  * [Looking Here or There? Gaze Following in 360-Degree Images](https://openaccess.thecvf.com/content/ICCV2021/papers/Li_Looking_Here_or_There_Gaze_Following_in_360-Degree_Images_ICCV_2021_paper.pdf)
* Flare Removal
  * [How to Train Neural Networks for Flare Removal](https://arxiv.org/abs/2011.12485)<br>:house:[project](https://yichengwu.github.io/flare-removal/):tv:[video](https://www.youtube.com/watch?v=eAXhcDjWoZ0)
* 其它
  * [A Lazy Approach to Long-Horizon Gradient-Based Meta-Learning](https://openaccess.thecvf.com/content/ICCV2021/papers/Zheng_Exploring_Temporal_Coherence_for_More_General_Video_Face_Forgery_Detection_ICCV_2021_paper.pdf)
  * [Viewing Graph Solvability via Cycle Consistency](https://openaccess.thecvf.com/content/ICCV2021/papers/Arrigoni_Viewing_Graph_Solvability_via_Cycle_Consistency_ICCV_2021_paper.pdf)<br>:open_mouth:oral:star:[code](https://github.com/federica-arrigoni/solvability)<br>:trophy:Best paper honorable mention
  * [SACoD: Sensor Algorithm Co-Design Towards Efficient CNN-Powered Intelligent PhlatCam](https://openaccess.thecvf.com/content/ICCV2021/papers/Fu_SACoD_Sensor_Algorithm_Co-Design_Towards_Efficient_CNN-Powered_Intelligent_PhlatCam_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/RICE-EIC/SACoD)
  * [Rethinking 360° Image Visual Attention Modelling with Unsupervised Learning](https://openaccess.thecvf.com/content/ICCV2021/papers/Djilali_Rethinking_360deg_Image_Visual_Attention_Modelling_With_Unsupervised_Learning._ICCV_2021_paper.pdf)

【2】
* 人体动作捕捉
  * [DeepMultiCap: Performance Capture of Multiple Characters Using Sparse Multiview Cameras](https://arxiv.org/abs/2105.00261)<br>:house:[project](http://liuyebin.com/dmc/dmc.html)
* transformer
  * [AI Choreographer: Music Conditioned 3D Dance Generation With AIST++](https://openaccess.thecvf.com/content/ICCV2021/papers/Li_AI_Choreographer_Music_Conditioned_3D_Dance_Generation_With_AIST_ICCV_2021_paper.pdf)<br>:house:[project](https://google.github.io/aichoreographer/):newspaper:简介:[Transformer又又来了，生成配有音乐的丝滑3D舞蹈，开放最大规模数据集AIST++](https://zhuanlan.zhihu.com/p/346151291)
* 点云
  * [PU-EVA: An Edge-Vector Based Approximation Solution for Flexible-Scale Point Cloud Upsampling](https://openaccess.thecvf.com/content/ICCV2021/papers/Luo_PU-EVA_An_Edge-Vector_Based_Approximation_Solution_for_Flexible-Scale_Point_Cloud_ICCV_2021_paper.pdf)
  * [A Backdoor Attack Against 3D Point Cloud Classifiers](http://arxiv.org/abs/2104.05808)<br>:star:[code](https://github.com/zhenxianglance/PCBA)
  * [A Closer Look at Rotation-Invariant Deep Point Cloud Analysis](https://openaccess.thecvf.com/content/ICCV2021/papers/Li_A_Closer_Look_at_Rotation-Invariant_Deep_Point_Cloud_Analysis_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/SILI1994/rotation-invariant-pointcloud-analysis)
* 三维
  * [SurfGen: Adversarial 3D Shape Synthesis With Explicit Surface Discriminators](https://openaccess.thecvf.com/content/ICCV2021/papers/Luo_SurfGen_Adversarial_3D_Shape_Synthesis_With_Explicit_Surface_Discriminators_ICCV_2021_paper.pdf)
  * [Deep Virtual Markers for Articulated 3D Shapes](https://arxiv.org/abs/2108.09000)<br>:star:[code](https://github.com/T2Kim/DeepVirtualMarkers):tv:[video](https://www.youtube.com/watch?v=Raq5axLdG6E)
* 物体重识别
  * [TransReID: Transformer-Based Object Re-Identification](https://arxiv.org/abs/2102.04378)<br>:star:[code](https://github.com/damo-cv/TransReID)
* 目标定位
  * [Foreground Activation Maps for Weakly Supervised Object Localization](https://openaccess.thecvf.com/content/ICCV2021/papers/Meng_Foreground_Activation_Maps_for_Weakly_Supervised_Object_Localization_ICCV_2021_paper.pdf)
* 手语识别
  * [Self-Mutual Distillation Learning for Continuous Sign Language Recognition](https://openaccess.thecvf.com/content/ICCV2021/papers/Hao_Self-Mutual_Distillation_Learning_for_Continuous_Sign_Language_Recognition_ICCV_2021_paper.pdf)
* NAS
  * [GLiT: Neural Architecture Search for Global and Local Image Transformer](https://arxiv.org/abs/2107.02960)<br>:star:[code](https://github.com/bychen515/GLiT)
* 分类
  * [Understanding Robustness of Transformers for Image Classification](http://arxiv.org/abs/2103.14586)
  * [Few-Shot Image Classification: Just Use a Library of Pre-Trained Feature Extractors and a Simple Classifier](https://arxiv.org/abs/2101.00562)
  * [Explanations for Occluded Images](https://arxiv.org/abs/2103.03622)<br>:star:[code](https://github.com/theyoucheng/deepcover):house:[project](https://www.cprover.org/deepcover/iccv2021/):tv:[video](https://www.cprover.org/deepcover/iccv2021/iccv2021-talk-compatible.mp4)
* 3D形状识别
  * [Learning Canonical View Representation for 3D Shape Recognition With Arbitrary Views](https://arxiv.org/abs/2108.07084)<br>:star:[code](https://github.com/weixmath/CVR)
* SR
  * [Fourier Space Losses for Efficient Perceptual Image Super-Resolution](https://arxiv.org/abs/2106.00783)<br>:star:[code](https://github.com/dariofuoli)
  * [COMISR: Compression-Informed Video Super-Resolution](https://arxiv.org/abs/2105.01237)<br>:star:[code](https://github.com/google-research/google-research/tree/master/comisr)
  * [Achieving On-Mobile Real-Time Super-Resolution With Neural Architecture and Pruning Search](https://arxiv.org/abs/2108.08910)
  * [Designing a Practical Degradation Model for Deep Blind Image Super-Resolution](https://arxiv.org/abs/2103.14006)<br>:star:[code](https://github.com/cszn/BSRGAN)
* 分割
  * [Guided Point Contrastive Learning for Semi-Supervised Point Cloud Semantic Segmentation](https://openaccess.thecvf.com/content/ICCV2021/papers/Jiang_Guided_Point_Contrastive_Learning_for_Semi-Supervised_Point_Cloud_Semantic_Segmentation_ICCV_2021_paper.pdf)
* 去反射
  * [Location-Aware Single Image Reflection Removal](https://arxiv.org/abs/2012.07131)<br>:star:[code](https://github.com/zdlarr/Location-aware-SIRR)
* SGG
  * [Segmentation-Grounded Scene Graph Generation](https://arxiv.org/abs/2104.14207)<br>:star:[code](https://github.com/ubc-vision/segmentation-sg)
* 自监督
  * [Geography-Aware Self-Supervised Learning](https://arxiv.org/abs/2011.09980)
* 对比学习
  * [CrossCLR: Cross-Modal Contrastive Learning for Multi-Modal Video Representations](https://arxiv.org/abs/2109.14910)
* 压缩成像
  * [Time-Multiplexed Coded Aperture Imaging: Learned Coded Aperture and Pixel Exposures for Compressive Imaging Systems](https://arxiv.org/abs/2104.02820)
* 视觉表征学习
  * [Contrasting Contrastive Self-Supervised Representation Learning Pipelines](https://arxiv.org/abs/2103.14005)<br>:star:[code](https://github.com/allenai/virb)
* 姿态
  * [Normalized Human Pose Features for Human Action Video Alignment](https://openaccess.thecvf.com/content/ICCV2021/papers/Liu_Normalized_Human_Pose_Features_for_Human_Action_Video_Alignment_ICCV_2021_paper.pdf)
* 图像聚类
  * [Learning Hierarchical Graph Neural Networks for Image Clustering](https://arxiv.org/abs/2107.01319)<br>:star:[code](https://github.com/dmlc/dgl/tree/master/examples/pytorch/hilander)
  * [One-Pass Multi-View Clustering for Large-Scale Data](https://openaccess.thecvf.com/content/ICCV2021/papers/Liu_One-Pass_Multi-View_Clustering_for_Large-Scale_Data_ICCV_2021_paper.pdf)
* 3D场景合成
  * [Indoor Scene Generation From a Collection of Semantic-Segmented Depth Images](https://arxiv.org/abs/2108.09022)<br>:star:[code](https://github.com/mingjiayang/SGSDI)
* 深度估计
  * [Can Scale-Consistent Monocular Depth Be Learned in a Self-Supervised Scale-Invariant Manner?](https://openaccess.thecvf.com/content/ICCV2021/papers/Wang_Can_Scale-Consistent_Monocular_Depth_Be_Learned_in_a_Self-Supervised_Scale-Invariant_ICCV_2021_paper.pdf)
* 视频帧插值
  * [Training Weakly Supervised Video Frame Interpolation With Events](https://openaccess.thecvf.com/content/ICCV2021/papers/Yu_Training_Weakly_Supervised_Video_Frame_Interpolation_With_Events_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/YU-Zhiyang/WEVI)
* 图像裁剪
  * [TransView: Inside, Outside, and Across the Cropping View Boundaries](https://openaccess.thecvf.com/content/ICCV2021/papers/Pan_TransView_Inside_Outside_and_Across_the_Cropping_View_Boundaries_ICCV_2021_paper.pdf)
* Deepfake 视频检测
  * [ID-Reveal: Identity-aware DeepFake Video Detection](https://openaccess.thecvf.com/content/ICCV2021/papers/Cozzolino_ID-Reveal_Identity-Aware_DeepFake_Video_Detection_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/grip-unina/id-reveal)
* GAN
  * [GAN-Control: Explicitly Controllable GANs](https://openaccess.thecvf.com/content/ICCV2021/papers/Shoshan_GAN-Control_Explicitly_Controllable_GANs_ICCV_2021_paper.pdf)(https://alonshoshan10.github.io/gan_control/)<br>:house:[project](https://alonshoshan10.github.io/gan_control/)
  * [Omni-GAN: On the Secrets of cGANs and Beyond](https://openaccess.thecvf.com/content/ICCV2021/papers/Zhou_Omni-GAN_On_the_Secrets_of_cGANs_and_Beyond_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/PeterouZh/Omni-GAN-PyTorch)
* 域适应
  * [Re-energizing Domain Discriminator with Sample Relabeling for Adversarial Domain Adaptation](https://arxiv.org/abs/2103.11661)
* SOD
* [Light Field Saliency Detection with Dual Local Graph Learning and Reciprocative Guidance](https://arxiv.org/abs/2108.06384)
* 成像
  * [An Asynchronous Kalman Filter for Hybrid Event Cameras](https://arxiv.org/abs/2012.05590)<br>:star:[code](https://github.com/ziweiWWANG/AKF)
* 小样本
  * [Z-Score Normalization, Hubness, and Few-Shot Learning](https://openaccess.thecvf.com/content/ICCV2021/papers/Fei_Z-Score_Normalization_Hubness_and_Few-Shot_Learning_ICCV_2021_paper.pdf)
* 人员重识别
  * [Dense Interaction Learning for Video-Based Person Re-Identification](https://arxiv.org/abs/2103.09013)<br>:open_mouth:oral
* VQA
  * [Unshuffling Data for Improved Generalization in Visual Question Answering](https://openaccess.thecvf.com/content/ICCV2021/papers/Teney_Unshuffling_Data_for_Improved_Generalization_in_Visual_Question_Answering_ICCV_2021_paper.pdf)
* DNN
  * [Architecture Disentanglement for Deep Neural Networks](https://arxiv.org/abs/2003.13268)<br>:star:[code](https://github.com/hujiecpp/NAD)
* 场景理解
  * [ACDC: The Adverse Conditions Dataset With Correspondences for Semantic Driving Scene Understanding](https://openaccess.thecvf.com/content/ICCV2021/papers/Sakaridis_ACDC_The_Adverse_Conditions_Dataset_With_Correspondences_for_Semantic_Driving_ICCV_2021_paper.pdf)<br>:house:[project](https://acdc.vision.ee.ethz.ch/)
* 其它
  * [Motion Basis Learning for Unsupervised Deep Homography Estimation with Subspace Projection](https://arxiv.org/abs/2103.15346)<br>:star:[code](https://github.com/megvii-research/BasesHomo)
  * [Batch Normalization Increases Adversarial Vulnerability and Decreases Adversarial Transferability: A Non-Robust Feature Perspective](https://openaccess.thecvf.com/content/ICCV2021/papers/Benz_Batch_Normalization_Increases_Adversarial_Vulnerability_and_Decreases_Adversarial_Transferability_A_ICCV_2021_paper.pdf)
  * [DeepCAD: A Deep Generative Network for Computer-Aided Design Models](https://arxiv.org/abs/2105.09492)<br>:house:[project](http://www.cs.columbia.edu/cg/deepcad/)
  * [Better Aggregation in Test-Time Augmentation](https://openaccess.thecvf.com/content/ICCV2021/papers/Shanmugam_Better_Aggregation_in_Test-Time_Augmentation_ICCV_2021_paper.pdf)
  * [Self-Born Wiring for Neural Trees](https://openaccess.thecvf.com/content/ICCV2021/papers/Chen_Self-Born_Wiring_for_Neural_Trees_ICCV_2021_paper.pdf)
  * [Detector-Free Weakly Supervised Grounding by Separation](https://arxiv.org/abs/2104.09829)
  * [Motion-Aware Dynamic Architecture for Efficient Frame Interpolation](https://openaccess.thecvf.com/content/ICCV2021/papers/Choi_Motion-Aware_Dynamic_Architecture_for_Efficient_Frame_Interpolation_ICCV_2021_paper.pdf)
  * [Relating Adversarially Robust Generalization to Flat Minima](https://arxiv.org/abs/2104.04448)
  * [Bit-Mixer: Mixed-Precision Networks With Runtime Bit-Width Selection](https://openaccess.thecvf.com/content/ICCV2021/papers/Bulat_Bit-Mixer_Mixed-Precision_Networks_With_Runtime_Bit-Width_Selection_ICCV_2021_paper.pdf)
  * [AINet: Association Implantation for Superpixel Segmentation](https://arxiv.org/abs/2101.10696)<br>:star:[code](https://github.com/wangyxxjtu/AINet-ICCV2021)
  * [Orthogonal Projection Loss](https://arxiv.org/abs/2103.14021)<br>:star:[code](https://github.com/kahnchana/opl)
  * [Knowledge-Enriched Distributional Model Inversion Attacks](https://arxiv.org/abs/2010.04092)<br>:star:[code](https://github.com/SCccc21/Knowledge-Enriched-DMI)
 

# 目录

|:dog:|:mouse:|:hamster:|:tiger:|
|------|------|------|------|
|[57.Image Matching(图像匹配)](#57)|[58.Computational Photography(光学、几何、光场成像、计算摄影)](#58)|[59.Graph Neural Networks(图神经网络)](#59)|
|[53.Vision Localization(视觉定位)](#53)|[54.Sketch recognition(草图)](#54)|[55.Activity Recognition(活动识别)](#55)|[56.Dataset(数据集)](#56)|
|[49.Human-Object Interaction(人物交互)](#49)|[50.Continual Learning(持续学习)](#50)|[51.View Synthesis(视图合成)](#51)|[52.Vision-and-Language(视觉语言)](#52)|
|[45.Image Caption(图像字幕)](#45)|[46.Defect Detection(缺陷检测)](#46)|[47.NAS](#47)|[48.6DoF](#48)|
|[41.Out-of-Distribution Detection(OOD)](#41)|[42.Visual Representations Learning(视觉表征学习)](#42)|[43.Dense Prediction(密集预测)](#43)|[44.Human motion prediction(人体运动预测)](#44)|
|[37.Multitask Learning(多任务学习)](#37)|[38.Weakly/Semi-Supervised/Self-supervised/Unsupervised Learning(自/半/弱监督学习)](#38)|[39.Incremental Learning(增量学习)](#39)|[40.Metric Learning(度量学习)](#40)|
|[33.Remote Sensing Images(遥感影像)](#33)|[34.Image Super-Resolution(图像超分辨率)](#34)|[35.Quantization/Pruning/Knowledge Distillation/Model Compression(量化、剪枝、蒸馏、模型压缩/扩展与优化)](#35)|[36.SLAM/AR/VR/机器人](#36)|
|[29.Image Retrieval(图像检索)](#29)|[30.Image Generation/synthesis(图像生成/合成)](#30)|[31.Style Transfer(风格迁移)](#31)|[32.语音](#32)|
|[25.Medical Image(医学影像)](#25)|[26.Image Processing(图像处理)](#26)|[27.Multi-label image recognition(多标签图像识别)](#27)|[28.Contrastive Learning(对比学习)](#28)]
|[21.Active Learning(主动学习)](#21)|[22.GAN](#22)|[23.Gaze Estimation(视线估计)](#23)|[24.Face(人脸)](#24)|
|[17.3D(三维视觉)](#17)|[18.Transformers](#18)|[19.Self-Driving Vehicles(自动驾驶)](#19)|[20.Adversarial Learning(对抗学习)](#20)|
|[13.Image Segmentation(图像分割)](#13)|[14.Object Detection(目标检测)](#13)|[15.Object Tracking(目标跟踪)](#15)|[16.Re-Identification(重识别)](#16)|
|[9.Video](#9)|[10.OCR](10)|[11.Visual Question Answering(视觉问答)](#11)|[12.Image/Fine-Grained Classification(图像/细粒度分类)](#12)|
|[5.Few-Shot/Zero-Shot Learning;Domain Generalization/Adaptation(小/零样本学习;域适应/泛化)](#5)|[6.Point Cloud(点云)](#6)|[7.Scene Graph Generation(场景图生成)](#7)|[8.Human Pose Estimation(人体姿态估计)](#8)|
|[1.Other(其它)](#1)|[2.Sign Language(手语识别)](#2)|[3.Image Clustering(图像聚类)](#3)|[4.Neural rendering(神经渲染)](#4)|



<a name="59"/>

## 59.Graph Neural Networks(图神经网络)
* [Meta-Aggregator: Learning to Aggregate for 1-bit Graph Neural Networks](https://arxiv.org/abs/2109.12872) 

<a name="58"/>

## 58.Computational Photography(光学、几何、光场成像、计算摄影)
* [An Asynchronous Kalman Filter for Hybrid Event Cameras](https://arxiv.org/abs/2012.05590)<br>:star:[code](https://github.com/ziweiWWANG/AKF)
* Snapshot compressive imaging(快照压缩成像)
  * [Dense Deep Unfolding Network with 3D-CNN Prior for Snapshot Compressive Imaging](https://arxiv.org/abs/2109.06548)<br>:star:[code](https://github.com/jianzhangcs/SCI3D)
* 光场
  * [Light Field Saliency Detection with Dual Local Graph Learning andReciprocative Guidance](https://arxiv.org/abs/2110.00698)
* 压缩成像
  * [Time-Multiplexed Coded Aperture Imaging: Learned Coded Aperture and Pixel Exposures for Compressive Imaging Systems](https://arxiv.org/abs/2104.02820)

<a name="57"/>

## 57.Image Matching(图像匹配)
* [Matching in the Dark: A Dataset for Matching Image Pairs of Low-light Scenes](https://arxiv.org/abs/2109.03585)
* 特征点匹配
  * [P2-Net: Joint Description and Detection of Local Features for Pixel and Point Matching](https://openaccess.thecvf.com/content/ICCV2021/papers/Wang_P2-Net_Joint_Description_and_Detection_of_Local_Features_for_Pixel_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/BingCS/P2-Net)
 
<a name="56"/>

## 56.Dataset(数据集)
* [Large Scale Multi-Illuminant (LSMI) Dataset for Developing White Balance Algorithm Under Mixed Illumination](https://openaccess.thecvf.com/content/ICCV2021/papers/Kim_Large_Scale_Multi-Illuminant_LSMI_Dataset_for_Developing_White_Balance_Algorithm_ICCV_2021_paper.pdf)<br>:sunflower:[dataset](https://github.com/DY112/LSMI-dataset)
* 生物医学图像
  * [BioFors: A Large Biomedical Image Forensics Dataset](https://arxiv.org/abs/2108.12961)
* 3D重建
  * [Common Objects in 3D: Large-Scale Learning and Evaluation of Real-life 3D Category Reconstruction](https://arxiv.org/abs/2109.00512)<br>:sunflower:[dataset](https://github.com/facebookresearch/co3d)
* 航空影像数据集
  * [Beyond Road Extraction: A Dataset for Map Update using Aerial Images](https://arxiv.org/abs/2110.04690)<br>:star:[code](https://github.com/favyen/muno21):house:[project](https://favyen.com/muno21/)<br>用于使用航拍图像更新地图的数据集
  
<a name="55"/>

## 55.Activity Recognition(活动识别)
* [Selective Feature Compression for Efficient Activity Recognition Inference](https://arxiv.org/abs/2104.00179)
* 小组活动识别
  * [Spatio-Temporal Dynamic Inference Network for Group Activity Recognition](https://arxiv.org/abs/2108.11743)<br>:star:[code](https://github.com/JacobYuan7/DIN_GAR)
  * [GroupFormer: Group Activity Recognition with Clustered Spatial-Temporal Transformer](https://arxiv.org/abs/2108.12630)<br>:star:[code](https://github.com/xueyee/GroupFormer) 
 

<a name="54"/>

## 54.Sketch recognition(草图)
* [SketchLattice: Latticed Representation for Sketch Manipulation](https://arxiv.org/abs/2108.11636)

<a name="53"/>

## 53.Vision Localization(视觉定位)
  * [Continual Learning for Image-Based Camera Localization](https://arxiv.org/abs/2108.09112)<br>:star:[code](https://github.com/AaltoVision/CL_HSCNet)

<a name="52"/>

## 52.Vision-and-Language(视觉语言)
* [YouRefIt: Embodied Reference Understanding with Language and Gesture](https://arxiv.org/abs/2109.03413)<br>:open_mouth:oral:house:[project](https://yixchen.github.io/YouRefIt/)
* 视觉语言导航
  * [Airbert: In-domain Pretraining for Vision-and-Language Navigation](https://arxiv.org/abs/2108.09105)<br>:house:[project](https://airbert-vln.github.io/)
  * [Waypoint Models for Instruction-guided Navigation in Continuous Environments](https://arxiv.org/abs/2110.02207)<br>:open_mouth:oral:star:[code](https://github.com/jacobkrantz/VLN-CE):house:[project](https://jacobkrantz.github.io/waypoint-vlnce/):tv:[video](https://youtu.be/hrHj9-1xoio)
  * [The Road To Know-Where: An Object-and-Room Informed Sequential BERT for Indoor Vision-Language Navigation](https://openaccess.thecvf.com/content/ICCV2021/papers/Qi_The_Road_To_Know-Where_An_Object-and-Room_Informed_Sequential_BERT_for_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/YuankaiQi/ORIST)

<a name="51"/>

## 51.View Synthesis(视图合成)
* [Out-of-boundary View Synthesis Towards Full-Frame Video Stabilization](https://arxiv.org/abs/2108.09041)
* [Deep 3D Mask Volume for View Synthesis of Dynamic Scenes](https://arxiv.org/abs/2108.13408)<br>:house:[project](https://cseweb.ucsd.edu//~viscomp/projects/ICCV21Deep/)
* [Embedding Novel Views in a Single JPEG Image](https://arxiv.org/abs/2108.13003)
* [Video Autoencoder: self-supervised disentanglement of static 3D structure and motion](https://arxiv.org/abs/2110.02951)<br>:open_mouth:oral:star:[code](https://github.com/zlai0/VideoAutoencoder/):house:[project](https://zlai0.github.io/VideoAutoencoder/#method_video):tv:[video](https://www.youtube.com/watch?v=UaJZd4FrM8E)
* [Geometry-Free View Synthesis: Transformers and No 3D Priors](https://arxiv.org/abs/2104.07652)<br>:star:[code](https://github.com/CompVis/geometry-free-view-synthesis)

<a name="50"/>

## 50.Continual Learning(持续学习)
* [Online Continual Learning with Natural Distribution Shifts: An Empirical Study with Visual Data](https://arxiv.org/abs/2108.09020)<br>:star:[code](https://github.com/IntelLabs/continuallearning)
* [Continual Learning on Noisy Data Streams via Self-Purified Replay](https://openaccess.thecvf.com/content/ICCV2021/papers/Kim_Continual_Learning_on_Noisy_Data_Streams_via_Self-Purified_Replay_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/ecrireme/SPR)

  
<a name="49"/>

## 49.Human-Object Interaction(人物交互)
* [Exploiting Scene Graphs for Human-Object Interaction Detection](https://arxiv.org/abs/2108.08584)
* [Spatially Conditioned Graphs for Detecting Human-Object Interactions](https://arxiv.org/abs/2012.06060)<br>:star:[code](https://github.com/fredzzhang/spatially-conditioned-graphs):tv:[video](https://www.youtube.com/watch?v=gkBWi_rWedU)
* [Virtual Multi-Modality Self-Supervised Foreground Matting for Human-Object Interaction](https://arxiv.org/abs/2110.03278)

<a name="48"/>

## 48.6DoF
* [SO-Pose: Exploiting Self-Occlusion for Direct 6D Pose Estimation](https://arxiv.org/abs/2108.08367)<br>:star:[code](https://github.com/shangbuhuan13/SO-Pose)
* [StereOBJ-1M: Large-scale Stereo Image Dataset for 6D Object Pose Estimation](https://arxiv.org/abs/2109.10115)

<a name="47"/>

## 47.NAS
* [BN-NAS: Neural Architecture Search with Batch Normalization](https://arxiv.org/abs/2108.07375)<br>:star:[code](https://github.com/bychen515/BNNAS)
* [RANK-NOSH: Efficient Predictor-Based Architecture Search via Non-Uniform Successive Halving](https://arxiv.org/abs/2108.08019)
* [Pi-NAS: Improving Neural Architecture Search by Reducing Supernet Training Consistency Shift](https://arxiv.org/abs/2108.09671)<br>:star:[code](https://github.com/Ernie1/Pi-NAS)
* [Evolving Search Space for Neural Architecture Search](https://arxiv.org/abs/2011.10904)<br>:star:[code](https://github.com/orashi/NSE_NAS):tv:[video](https://www.youtube.com/watch?v=fq21WBaumRc)
* [FairNAS: Rethinking Evaluation Fairness of Weight Sharing Neural Architecture Search](https://arxiv.org/abs/1907.01845)<br>:star:[code](https://github.com/xiaomi-automl/FairNAS)
* [GLiT: Neural Architecture Search for Global and Local Image Transformer](https://arxiv.org/abs/2107.02960)<br>:star:[code](https://github.com/bychen515/GLiT)

<a name="46"/>

## 46.Defect Detection(缺陷检测)
* [DRÆM -- A discriminatively trained reconstruction embedding for surface anomaly detection](https://arxiv.org/abs/2108.07610)

<a name="45"/>

## 45.Image Caption(图像字幕)
* [Who's Waldo? Linking People Across Text and Images](https://arxiv.org/abs/2108.07253)<br>:open_mouth:oral:house:[project](https://whoswaldo.github.io/)<br>:newspaper:解读:[ICCV2021 Oral-新任务！新数据集！康奈尔大学提出了类似VG但又不是VG的PVG任务](https://mp.weixin.qq.com/s/QC1UQRmZKgS0dctTXQ77Bg)
* art description generation(艺术描述生成)
  * [Explain Me the Painting: Multi-Topic Knowledgeable Art Description Generation](https://arxiv.org/abs/2109.05743)<br>:star:[code](https://github.com/noagarcia/explain-paintings)

<a name="44"/>

## 44.Human motion prediction(人体运动预测)
* [MSR-GCN: Multi-Scale Residual Graph Convolution Networks for Human Motion Prediction](https://arxiv.org/abs/2108.07152)<br>:star:[code](https://github.com/Droliven/MSRGCN)
* [Stochastic Scene-Aware Motion Prediction](https://arxiv.org/abs/2108.08284)<br>:star:[code](https://github.com/mohamedhassanmus/SAMP):house:[project](https://samp.is.tue.mpg.de/)  
* [Generating Smooth Pose Sequences for Diverse Human Motion Prediction](https://arxiv.org/abs/2108.08422)<br>:open_mouth:oral:star:[code](https://github.com/wei-mao-2019/gsps)

<a name="43"/>

## 43.Dense Prediction(密集预测)
* [FaPN: Feature-aligned Pyramid Network for Dense Image Prediction](https://arxiv.org/abs/2108.07058)<br>:star:[code](https://github.com/EMI-Group/FaPN)

<a name="42"/>

## 42.Representations Learning(表征学习)
* [Learning From Noisy Data With Robust Representation Learning](https://openaccess.thecvf.com/content/ICCV2021/papers/Li_Learning_From_Noisy_Data_With_Robust_Representation_Learning_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/salesforce/RRL/)
* 视觉表征学习
  * [Self-Supervised Visual Representations Learning by Contrastive Mask Prediction](https://arxiv.org/abs/2108.07954)
  * [Temporal Knowledge Consistency for Unsupervised Visual Representation Learning](https://arxiv.org/abs/2108.10668)
  * [Contrasting Contrastive Self-Supervised Representation Learning Pipelines](https://arxiv.org/abs/2103.14005)<br>:star:[code](https://github.com/allenai/virb)
* 视频表示学习
  * [Composable Augmentation Encoding for Video Representation Learning](https://arxiv.org/abs/2104.00616)

<a name="41"/>

## 41.Out-of-Distribution Detection(OOD)
* [CODEs: Chamfer Out-of-Distribution Examples against Overconfidence Issue](https://arxiv.org/abs/2108.06024)
* [Semantically Coherent Out-of-Distribution Detection](https://arxiv.org/abs/2108.11941)<br>:star:[code](https://github.com/jingkang50/ICCV21_SCOOD):house:[project](https://jingkang50.github.io/projects/scood)

<a name="40"/>

## 40.Metric Learning(度量学习)
* [Towards Interpretable Deep Metric Learning with Structural Matching](https://arxiv.org/abs/2108.05889)<br>:star:[code](https://github.com/wl-zhao/DIML)
* [Deep Relational Metric Learning](https://arxiv.org/abs/2108.10026)<br>:star:[code](https://github.com/zbr17/DRML)
* [LoOp: Looking for Optimal Hard Negative Embeddings for Deep Metric Learning](https://arxiv.org/abs/2108.09335)<br>:star:[code](https://github.com/puneesh00/LoOp)

<a name="39"/>

## 39.Incremental Learning(增量学习)
* 类增量学习
  * [Always Be Dreaming: A New Approach for Data-Free Class-Incremental Learning](https://arxiv.org/abs/2106.09701)<br>:newspaper:解读:[让模型实现“终生学习”，佐治亚理工学院提出Data-Free的增量学习](https://mp.weixin.qq.com/s/Fm9ufPD6rzL2VzaqpdFpjg)

<a name="38"/>

## 38.Weakly/Semi-Supervised/Self-supervised/Unsupervised Learning(自/半/弱监督学习)
* 半监督
  * [Trash to Treasure: Harvesting OOD Data with Cross-Modal Matching for Open-Set Semi-Supervised Learning](https://arxiv.org/abs/2108.05617)
* 自监督
  * [Focus on the Positives: Self-Supervised Learning for Biodiversity Monitoring](https://arxiv.org/abs/2108.06435)<br>:star:[code](https://github.com/omipan/camera_traps_self_supervised)
  * [Self-supervised Neural Networks for Spectral Snapshot Compressive Imaging](https://arxiv.org/abs/2108.12654)<br>:star:[code](https://github.com/mengziyi64/CASSI-Self-Supervised)
  * [ISD: Self-Supervised Learning by Iterative Similarity Distillation](https://arxiv.org/abs/2012.09259)<br>:star:[code](https://github.com/UMBCvision/ISD)
  * [Contrast and Order Representations for Video Self-Supervised Learning](https://openaccess.thecvf.com/content/ICCV2021/papers/Hu_Contrast_and_Order_Representations_for_Video_Self-Supervised_Learning_ICCV_2021_paper.pdf)
  * [On Feature Decorrelation in Self-Supervised Learning](https://arxiv.org/abs/2105.00470)<br>:open_mouth:oral
  * [Geography-Aware Self-Supervised Learning](https://arxiv.org/abs/2011.09980)

<a name="37"/>

## 37.Multitask Learning(多任务学习)
* [MultiTask-CenterNet (MCN): Efficient and Diverse Multitask Learning using an Anchor Free Approach](https://arxiv.org/abs/2108.05060)<br>:newspaper:解读:[ICCV2021《MultiTask CenterNet》CV多任务新进展！一节更比三节强](https://mp.weixin.qq.com/s/toAZS0OHdW4MG30P1wAAUA)
* [Multi-Task Self-Training for Learning General Representations](https://arxiv.org/abs/2108.11353)<br>:newspaper:解读:[ICCV2021 MuST：还在特定任务里为刷点而苦苦挣扎？谷歌的大佬们都已经开始玩多任务训练了](https://mp.weixin.qq.com/s/nhv1l9xBSaZceibIn5fhqw)

<a name="36"/>

## 36.SLAM/AR/VR/机器人
* 机器人
  * 室内导航
    * [The Surprising Effectiveness of Visual Odometry Techniques for Embodied PointGoal Navigation](https://arxiv.org/abs/2108.11550)<br>:star:[code](https://github.com/Xiaoming-Zhao/PointNav-VO):house:[project](https://xiaoming-zhao.github.io/projects/pointnav-vo/)
* VR/AR
  * [The Power of Points for Modeling Humans in Clothing](https://arxiv.org/abs/2109.01137)<br>:star:[code](https://github.com/qianlim/POP):house:[project](https://qianlim.github.io/POP):tv:[video](https://youtu.be/5M4F9zSWIEE)
  * 虚拟试穿  
    * [M3D-VTON: A Monocular-to-3D Virtual Try-On Network](https://arxiv.org/abs/2108.05126)<br>:star:[code](https://github.com/fyviezhao/M3D-VTON)
    * [ZFlow: Gated Appearance Flow-based Virtual Try-on with 3D Priors](https://arxiv.org/abs/2109.07001)
* SLAM
  * [On the Limits of Pseudo Ground Truth in Visual Camera Re-localisation](https://arxiv.org/abs/2109.00524)<br>:star:[code](https://github.com/tsattler/visloc_pseudo_gt_limitations/)

<a name="35"/>

## 35.Quantization/Pruning/Knowledge Distillation/Model Compression(量化、剪枝、蒸馏、模型压缩/扩展与优化)
* 知识蒸馏
  * [Distilling Holistic Knowledge with Graph Neural Networks](https://arxiv.org/abs/2108.05507)<br>:star:[code](https://github.com/wyc-ruiker/HKD)
  * [Lipschitz Continuity Guided Knowledge Distillation](https://arxiv.org/abs/2108.12905)<br>:star:[code](https://github.com/42Shawn/LONDON/tree/master)
  * [Densely Guided Knowledge Distillation Using Multiple Teacher Assistants](https://arxiv.org/abs/2009.08825)<br>:star:[code](https://github.com/wonchulSon/DGKD)
  * [Revisiting Adversarial Robustness Distillation: Robust Soft Labels Make Student Better](https://arxiv.org/abs/2108.07969)<br>:star:[code](https://github.com/zibojia/RSLAD)
* 量化
  * [Distance-aware Quantization](https://arxiv.org/abs/2108.06983)<br>:star:[code](https://github.com/cvlab-yonsei/DAQ):house:[project](https://cvlab.yonsei.ac.kr/projects/DAQ/) 
  * [Dynamic Network Quantization for Efficient Video Inference](https://arxiv.org/abs/2108.10394)<br>:star:[code](https://github.com/sunxm2357/VideoIQ):house:[project](https://cs-people.bu.edu/sunxm/VideoIQ/project.html)
  * [Cluster-Promoting Quantization with Bit-Drop for Minimizing Network Quantization Loss](https://arxiv.org/abs/2109.02100)
  * [Improving Low-Precision Network Quantization via Bin Regularization](https://openaccess.thecvf.com/content/ICCV2021/papers/Han_Improving_Low-Precision_Network_Quantization_via_Bin_Regularization_ICCV_2021_paper.pdf)
* 模型压缩
  * [GDP: Stabilized Neural Network Pruning via Gates with Differentiable Polarization](https://arxiv.org/abs/2109.02220)
* 剪枝
  * [ResRep: Lossless CNN Pruning via Decoupling Remembering and Forgetting](https://arxiv.org/abs/2007.03260)<br>:star:[code](https://github.com/DingXiaoH/ResRep)

<a name="34"/>

## 34.Super-Resolution(超分辨率)
* ISR
  * [Mutual Affine Network for Spatially Variant Kernel Estimation in Blind Image Super-Resolution](https://arxiv.org/abs/2108.05302)<br>:star:[code](https://github.com/JingyunLiang/MANet)
  * [Hierarchical Conditional Flow: A Unified Framework for Image Super-Resolution and Image Rescaling](https://arxiv.org/abs/2108.05301)<br>:star:[code](https://github.com/JingyunLiang/HCFlow)
  * [Deep Reparametrization of Multi-Frame Super-Resolution and Denoising](https://arxiv.org/abs/2108.08286)<br>:open_mouth:oral
  * [Dual-Camera Super-Resolution with Aligned Attention Modules](https://arxiv.org/abs/2109.01349)<br>:star:[code](https://github.com/Tengfei-Wang/DualCameraSR):house:[project](https://tengfei-wang.github.io/Dual-Camera-SR/index.html):tv:[video](https://www.youtube.com/watch?v=5TiUfAcNvuw)
  * [Attention-Based Multi-Reference Learning for Image Super-Resolution](https://arxiv.org/abs/2108.13697)<br>:star:[code](https://github.com/marcopesavento/AMRSR):house:[project](https://marcopesavento.github.io/AMRSR/)
  * [Learning a Single Network for Scale-Arbitrary Super-Resolution](https://arxiv.org/abs/2004.03791)
  * [Fourier Space Losses for Efficient Perceptual Image Super-Resolution](https://arxiv.org/abs/2106.00783)<br>:star:[code](https://github.com/dariofuoli)
  * [Achieving On-Mobile Real-Time Super-Resolution With Neural Architecture and Pruning Search](https://arxiv.org/abs/2108.08910)
  * [Designing a Practical Degradation Model for Deep Blind Image Super-Resolution](https://arxiv.org/abs/2103.14006)<br>:star:[code](https://github.com/cszn/BSRGAN)
* VSR
  * [Omniscient Video Super-Resolution](https://arxiv.org/abs/2103.15683)<br>:star:[code](https://github.com/psychopa4/OVSR)
  * [COMISR: Compression-Informed Video Super-Resolution](https://arxiv.org/abs/2105.01237)<br>:star:[code](https://github.com/google-research/google-research/tree/master/comisr)

<a name="33"/>

## 33.Remote Sensing Images(遥感影像)
* [SUNet: Symmetric Undistortion Network for Rolling Shutter Correction](https://arxiv.org/abs/2108.04775)
* [Change is Everywhere: Single-Temporal Supervised Object Change Detection in Remote Sensing Imagery](https://arxiv.org/abs/2108.07002)<br>:star:[code](https://github.com/Z-Zheng/ChangeStar)
* 卫星图像的全景分割
  * [Panoptic Segmentation of Satellite Image Time Series With Convolutional Temporal Attention Networks](https://arxiv.org/abs/2107.07933)<br>:star:[code](https://github.com/VSainteuf/utae-paps):sunflower:[PASTIS dataset](https://github.com/VSainteuf/pastis-benchmark)

<a name="32"/>

## 32.语音
* [The Right to Talk: An Audio-Visual Transformer Approach](https://arxiv.org/abs/2108.03256)<br>:star:[code](https://github.com/uark-cviu/Right2Talk)
* 音频分离
  * [Visual Scene Graphs for Audio Source Separation](https://arxiv.org/abs/2109.11955)
* 音频-手势
  * [Audio2Gestures: Generating Diverse Gestures from Speech Audio with Conditional Variational Autoencoders](https://arxiv.org/abs/2108.06720)<br>:house:[project](https://openaccess.thecvf.com/content/ICCV2021/papers/Xiang_SnowflakeNet_Point_Cloud_Completion_by_Snowflake_Point_Deconvolution_With_Skip-Transformer_ICCV_2021_paper.pdf)

<a name="31"/>

## 31.Style Transfer(风格迁移)
* [AdaAttN: Revisit Attention Mechanism in Arbitrary Neural Style Transfer](https://arxiv.org/abs/2108.03647)<br>:star:[code](https://github.com/Huage001/AdaAttN)
* [Domain-Aware Universal Style Transfer](https://arxiv.org/abs/2108.04441)<br>:star:[code](https://github.com/Kibeom-Hong/Domain-Aware-Style-Transfer)

<a name="30"/>

## 30.Image Generation/synthesis(图像生成/合成)
* [ILVR: Conditioning Method for Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2108.02938)<br>:open_mouth:oral
* [Image Synthesis via Semantic Composition](https://arxiv.org/abs/2109.07053)<br>:star:[code](https://github.com/dvlab-research/SCGAN):house:[project](https://shepnerd.github.io/scg/)

<a name="29"/>

## 29.Image Retrieval(图像检索)
  * [DOLG: Single-Stage Image Retrieval with Deep Orthogonal Fusion of Local and Global Features](https://arxiv.org/abs/2108.02927)<br>:star:[code](https://github.com/feymanpriv/DOLG)
  * [Image Retrieval on Real-life Images with Pre-trained Vision-and-Language Models](https://arxiv.org/abs/2108.04024)<br>:star:[code](https://github.com/Cuberick-Orion/CIRR):house:[project](https://cuberick-orion.github.io/CIRR/)
  * [Self-supervised Product Quantization for Deep Unsupervised Image Retrieval](https://arxiv.org/abs/2109.02244)
  * [Instance-Level Image Retrieval Using Reranking Transformers](https://arxiv.org/abs/2103.12236)<br>:star:[code](https://github.com/uvavision/RerankingTransformer)
* 跨域检索
  * [Universal Cross-Domain Retrieval: Generalizing Across Classes and Domains](https://arxiv.org/abs/2108.08356)
* Visual Geolocalization
  * [Viewpoint Invariant Dense Matching for Visual Geolocalization](https://arxiv.org/abs/2109.09827)<br>:star:[code](https://github.com/gmberton/geo_warp)
* 跨模态检索
  * [Ask&Confirm: Active Detail Enriching for Cross-Modal Retrieval With Partial Query](https://openaccess.thecvf.com/content/ICCV2021/papers/Cai_AskConfirm_Active_Detail_Enriching_for_Cross-Modal_Retrieval_With_Partial_Query_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/CuthbertCai/Ask-Confirm)
 

<a name="28"/>

## 28.Contrastive Learning(对比学习)
* [Improving Contrastive Learning by Visualizing Feature Transformation](https://arxiv.org/abs/2108.02982)<br>:open_mouth:oral:star:[code](https://github.com/DTennant/CL-Visualizing-Feature-Transformation)
* [TACo: Token-aware Cascade Contrastive Learning for Video-Text Alignment](https://arxiv.org/abs/2108.09980)<br>:newspaper:解读:[ICCV2021-TOCo-微软&CMU提出Token感知的级联对比学习方法，在视频文本对齐任务上“吊打”其他SOTA方法](https://mp.weixin.qq.com/s/sNwvYL1qsgyVrRe3-QmzhA)
* [A Broad Study on the Transferability of Visual Representations With Contrastive Learning](https://arxiv.org/abs/2103.13517)<br>:star:[code](https://github.com/asrafulashiq/transfer_broad)
* [Vi2CLR: Video and Image for Visual Contrastive Learning of Representation](https://openaccess.thecvf.com/content/ICCV2021/papers/Diba_Vi2CLR_Video_and_Image_for_Visual_Contrastive_Learning_of_Representation_ICCV_2021_paper.pdf)
* [LatentCLR: A Contrastive Learning Approach for Unsupervised Discovery of Interpretable Directions](https://arxiv.org/abs/2104.00820)<br>:star:[code](https://github.com/catlab-team/latentclr)
* [CrossCLR: Cross-Modal Contrastive Learning for Multi-Modal Video Representations](https://arxiv.org/abs/2109.14910)

<a name="27"/>

## 27.Multi-label image recognition(多标签图像识别)
* [Residual Attention: A Simple but Effective Method for Multi-Label Recognition](https://arxiv.org/abs/2108.02456)
* [Transformer-based Dual Relation Graph for Multi-label Image Recognition](https://arxiv.org/abs/2110.04722)



<a name="26"/>

## 26.Image Processing(图像处理)
* 图像形状操纵
  * [Image Shape Manipulation from a Single Augmented Training Sample](https://arxiv.org/abs/2109.06151)<br>:open_mouth:oral:star:[code](https://github.com/eliahuhorwitz/DeepSIM):house:[project](http://www.vision.huji.ac.il/deepsim/)
* 边缘检测
  * [RINDNet: Edge Detection for Discontinuity in Reflectance, Illumination, Normal and Depth](https://arxiv.org/abs/2108.00616)<br>:open_mouth:oral:star:[code](https://github.com/MengyangPu/RINDNet)
  * [Pixel Difference Networks for Efficient Edge Detection](https://arxiv.org/abs/2108.07009)<br>:star:[code](https://github.com/zhuoinoulu/pidinet)
* 图像识别
  * [MicroNet: Improving Image Recognition with Extremely Low FLOPs](https://arxiv.org/abs/2108.05894)<br>:star:[code](https://github.com/liyunsheng13/micronet)
* 图像去模糊
  * [Rethinking Coarse-to-Fine Approach in Single Image Deblurring](https://arxiv.org/abs/2108.05054)<br>:star:[code](https://github.com/chosj95/MIMO-UNet)
  * [Single Image Defocus Deblurring Using Kernel-Sharing Parallel Atrous Convolutions](https://arxiv.org/abs/2108.09108)
* Image quality assessment(图像质量评估IQA)
  * [MUSIQ: Multi-scale Image Quality Transformer](https://arxiv.org/abs/2108.05997)<br>:star:[code](https://github.com/google-research/google-research/tree/master/musiq)
* Image Harmonization
  * [SSH: A Self-Supervised Framework for Image Harmonization](https://arxiv.org/abs/2108.06805)<br>:star:[code](https://github.com/VITA-Group/SSHarmonization)
* 去阴影
  * [CANet: A Context-Aware Network for Shadow Removal](https://arxiv.org/abs/2108.09894)
* 去噪
  * [Rethinking Deep Image Prior for Denoising](https://arxiv.org/abs/2108.12841)<br>:star:[code](https://github.com/gistvision/DIP-denosing)
  * [Rethinking Noise Synthesis and Modeling in Raw Denoising](https://arxiv.org/abs/2110.04756)<br>:star:[code](https://github.com/zhangyi-3/noise-synthesis)
  * [C2N: Practical Generative Noise Modeling for Real-World Denoising](https://openaccess.thecvf.com/content/ICCV2021/papers/Jang_C2N_Practical_Generative_Noise_Modeling_for_Real-World_Denoising_ICCV_2021_paper.pdf)
  * 视频去噪
    * [Patch Craft: Video Denoising by Deep Modeling and Patch Matching](http://arxiv.org/abs/2103.13767)
* 图像着色
  * [Towards Vivid and Diverse Image Colorization with Generative Color Prior](https://arxiv.org/abs/2108.08826)
* 图像增强
  * [Real-time Image Enhancer via Learnable Spatial-aware 3D Lookup Tables](https://arxiv.org/abs/2108.08697)
  * [Adaptive Unfolding Total Variation Network for Low-Light Image Enhancement](https://arxiv.org/abs/2110.00984)<br>:star:[code](https://github.com/CharlieZCJ/UTVNet)
  * [Representative Color Transform for Image Enhancement](https://openaccess.thecvf.com/content/ICCV2021/papers/Kim_Representative_Color_Transform_for_Image_Enhancement_ICCV_2021_paper.pdf)
* 图像恢复
  * [Spatially-Adaptive Image Restoration using Distortion-Guided Networks](https://arxiv.org/abs/2108.08617)<br>:star:[code](https://github.com/human-analysis/spatially-adaptive-image-restoration)
  * [Dynamic Attentive Graph Learning for Image Restoration](https://arxiv.org/abs/2109.06620)<br>:star:[code](https://github.com/jianzhangcs/DAGL)
* 图像压缩
  * [Variable-Rate Deep Image Compression through Spatially-Adaptive Feature Transform](https://arxiv.org/abs/2108.09551)<br>:star:[code](https://github.com/micmic123/QmapCompression)
* 图像修复
  * [Image Inpainting via Conditional Texture and Structure Dual Generation](https://arxiv.org/abs/2108.09760)<br>:star:[code](https://github.com/Xiefan-Guo/CTSDG)
 * Image extrapolation
  * [SemIE: Semantically-aware Image Extrapolation](https://arxiv.org/abs/2108.13702)<br>:house:[project](https://semie-iccv.github.io/)
 * Reversible Image Conversion
  * [IICNet: A Generic Framework for Reversible Image Conversion](https://arxiv.org/abs/2109.04242)<br>:star:[code](https://github.com/felixcheng97/IICNet)
* 伪影去除
  * [Towards Flexible Blind JPEG Artifacts Removal](https://arxiv.org/abs/2109.14573)<br>:star:[code](https://github.com/jiaxi-jiang/FBCNN)
* De-rendering
  * [De-rendering Stylized Texts](https://arxiv.org/abs/2110.01890)<br>:star:[code](https://github.com/CyberAgentAILab/derendering-text):house:[project](https://cyberagentailab.github.io/derendering-text/)
* 去除光晕
  * [Light Source Guided Single-Image Flare Removal From Unpaired Data](https://openaccess.thecvf.com/content/ICCV2021/papers/Qiao_Light_Source_Guided_Single-Image_Flare_Removal_From_Unpaired_Data_ICCV_2021_paper.pdf)
* 全景图拼接
  * [Minimal Solutions for Panoramic Stitching Given Gravity Prior](https://arxiv.org/abs/2012.00465)
* Flare Removal
  * [How to Train Neural Networks for Flare Removal](https://arxiv.org/abs/2011.12485)<br>:house:[project](https://yichengwu.github.io/flare-removal/):tv:[video](https://www.youtube.com/watch?v=eAXhcDjWoZ0)
* 图像裁剪
  * [TransView: Inside, Outside, and Across the Cropping View Boundaries](https://openaccess.thecvf.com/content/ICCV2021/papers/Pan_TransView_Inside_Outside_and_Across_the_Cropping_View_Boundaries_ICCV_2021_paper.pdf)
* 去反射
  * [Location-Aware Single Image Reflection Removal](https://arxiv.org/abs/2012.07131)<br>:star:[code](https://github.com/zdlarr/Location-aware-SIRR)

<a name="25"/>

## 25.Medical Image(医学影像)
* 医学图像分割
  * [Recurrent Mask Refinement for Few-Shot Medical Image Segmentation](https://arxiv.org/abs/2108.00622)<br>:star:[code](https://github.com/uci-cbcl/RP-Net)
* 病理学图像表示
  * [A QuadTree Image Representation for Computational Pathology](https://arxiv.org/abs/2108.10873)
* 医学图像分析
  * [Preservational Learning Improves Self-supervised Medical Image Models by Reconstructing Diverse Contexts](https://arxiv.org/abs/2109.04379)<br>:star:[code](https://github.com/Luchixiang/PCRL)
* 医学图像去噪
  * [Eformer: Edge Enhancement based Transformer for Medical Image Denoising](https://arxiv.org/abs/2109.08044)
* 视频翻译
  * [Long-Term Temporally Consistent Unpaired Video Translation From Simulated Surgical 3D Data](https://arxiv.org/abs/2103.17204)<br>:star:[code](https://gitlab.com/nct_tso_public/surgical-video-sim2real):house:[project](http://opencas.dkfz.de/video-sim2real/)
* 病理学图像核检测分割
  * [Mutual-Complementing Framework for Nuclei Detection and Segmentation in Pathology Image](https://openaccess.thecvf.com/content/ICCV2021/papers/Feng_Mutual-Complementing_Framework_for_Nuclei_Detection_and_Segmentation_in_Pathology_Image_ICCV_2021_paper.pdf)

 
<a name="24"/>

## 24.Face(人脸)
* [VariTex: Variational Neural Face Textures](https://arxiv.org/abs/2104.05988)<br>:star:[code](https://github.com/mcbuehler/VariTex):house:[project](https://mcbuehler.github.io/VariTex/):tv:[video](https://www.youtube.com/watch?v=6-GFHcLkbik)
* 人脸造假检测
  * [OpenForensics: Large-Scale Challenging Dataset For Multi-Face Forgery Detection And Segmentation In-The-Wild](https://arxiv.org/abs/2107.14480)<br>:house:[project](https://sites.google.com/view/ltnghia/research/openforensics)
  * [Exploring Temporal Coherence for More General Video Face Forgery Detection](https://arxiv.org/abs/2108.06693)
* 人脸合成
  * [Disentangled Lifespan Face Synthesis](https://arxiv.org/abs/2108.02874)<br>:star:[code](https://github.com/SenHe/DLFS):house:[project](https://senhe.github.io/projects/iccv_2021_lifespan_face/):tv:[video](https://youtu.be/uklX03ns0m0)
* 人脸识别                                                                                      
  * [PASS: Protected Attribute Suppression System for Mitigating Bias in Face Recognition](https://arxiv.org/abs/2108.03764)
  * [SynFace: Face Recognition with Synthetic Data](https://arxiv.org/abs/2108.07960)
* Face perception面部感知
 * [Learning Facial Representations from the Cycle-consistency of Face](https://arxiv.org/abs/2108.03427)<br>:star:[code](https://github.com/JiaRenChang/FaceCycle)
* 说话人脸生成
  * [FACIAL: Synthesizing Dynamic Talking Face with Implicit Attribute Learning](https://arxiv.org/abs/2108.07938)
* 人脸表情识别
  * [Understanding and Mitigating Annotation Bias in Facial Expression Recognition](https://arxiv.org/abs/2108.08504)
  * [TransFER: Learning Relation-aware Facial Expression Representations with Transformers](https://arxiv.org/abs/2108.11116)
* 人脸呈现攻击检测
  * [Detection and Continual Learning of Novel Face Presentation Attacks](https://arxiv.org/abs/2108.12081)<br>:star:[code](https://github.com/mrostami1366)
* 人脸编辑
  * [Talk-to-Edit: Fine-Grained Facial Editing via Dialog](https://arxiv.org/abs/2109.04425)<br>:star:[code](https://github.com/yumingj/Talk-to-Edit):house:[project](https://www.mmlab-ntu.com/project/talkedit/)<br>:newspaper:解读:[ICCV2021 | 南洋理工大学、港中大提出Talk-to-Edit，对话实现高细粒度人脸编辑](https://mp.weixin.qq.com/s/48FsUqsppXaXUu-QMUIhCQ)
* 人脸对齐
  * [ADNet: Leveraging Error-Bias Towards Normal Direction in Face Alignment](https://arxiv.org/abs/2109.05721) 
* 人脸图像重建
  * [Focal Frequency Loss for Image Reconstruction and Synthesis](https://arxiv.org/abs/2012.12821)<br>:star:[code](https://github.com/EndlessSora/focal-frequency-loss):house:[project](https://www.mmlab-ntu.com/project/ffl/index.html):tv:[video](https://www.youtube.com/watch?v=RNTnDtKvcpc)
* 3D人脸重建
  * [Topologically Consistent Multi-View Face Inference Using Volumetric Sampling](https://arxiv.org/abs/2110.02948)<br>:star:[code](https://tianyeli.github.io/tofu) 
  * [Self-Supervised 3D Face Reconstruction via Conditional Estimation](https://arxiv.org/abs/2110.04800)
* 三维人脸动画
  * [MeshTalk: 3D Face Animation From Speech Using Cross-Modality Disentanglement](https://openaccess.thecvf.com/content/ICCV2021/papers/Richard_MeshTalk_3D_Face_Animation_From_Speech_Using_Cross-Modality_Disentanglement_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/facebookresearch/meshtalk):tv:[video](https://research.fb.com/wp-content/uploads/2021/04/mesh_talk.mp4)



<a name="23"/>

## 23.Gaze Estimation(视线估计)
* [Generalizing Gaze Estimation with Outlier-guided Collaborative Adaptation](https://arxiv.org/abs/2107.13780)<br>:star:[code](https://github.com/DreamtaleCore/PnP-GA)
* 视线跟踪
  * [Looking Here or There? Gaze Following in 360-Degree Images](https://openaccess.thecvf.com/content/ICCV2021/papers/Li_Looking_Here_or_There_Gaze_Following_in_360-Degree_Images_ICCV_2021_paper.pdf)

<a name="22"/>

## 22.GAN
* [Sketch Your Own GAN](https://arxiv.org/abs/2108.02774)<br>:star:[code](https://github.com/peterwang512/GANSketching):house:[project](https://peterwang512.github.io/GANSketching/)
* [Online Multi-Granularity Distillation for GAN Compression](https://arxiv.org/abs/2108.06908)<br>:star:[code](https://github.com/bytedance/OMGD)
* [Dual Projection Generative Adversarial Networks for Conditional Image Generation](https://arxiv.org/abs/2108.09016)<br>:star:[code](https://github.com/phymhan/P2GAN)
* [InSeGAN: A Generative Approach to Segmenting Identical Instances in Depth Images](https://arxiv.org/abs/2108.13865)
* [ReStyle: A Residual-Based StyleGAN Encoder via Iterative Refinement](https://arxiv.org/abs/2104.02699)<br>:star:[code](https://github.com/yuval-alaluf/restyle-encoder):house:[project](https://yuval-alaluf.github.io/restyle-encoder/):tv:[video](https://youtu.be/6pGzLECSIWM)
* [WarpedGANSpace: Finding non-linear RBF paths in GAN latent space](https://arxiv.org/abs/2109.13357)<br>:star:[code](https://github.com/chi0tzp/WarpedGANSpace)
* [Toward a Visual Concept Vocabulary for GAN Latent Space](https://arxiv.org/abs/2110.04292)
* [Collaging Class-specific GANs for Semantic Image Synthesis](https://arxiv.org/abs/2110.04281)<br>:house:[project](https://yuheng-li.github.io/CollageGAN/)
* [Latent Transformations via NeuralODEs for GAN-Based Image Editing](https://openaccess.thecvf.com/content/ICCV2021/papers/Khrulkov_Latent_Transformations_via_NeuralODEs_for_GAN-Based_Image_Editing_ICCV_2021_paper.pdf)
* [Reality Transform Adversarial Generators for Image Splicing Forgery Detection and Localization](https://openaccess.thecvf.com/content/ICCV2021/papers/Bi_Reality_Transform_Adversarial_Generators_for_Image_Splicing_Forgery_Detection_and_ICCV_2021_paper.pdf)
* [GAN-Control: Explicitly Controllable GANs](https://openaccess.thecvf.com/content/ICCV2021/papers/Shoshan_GAN-Control_Explicitly_Controllable_GANs_ICCV_2021_paper.pdf)(https://alonshoshan10.github.io/gan_control/)<br>:house:[project](https://alonshoshan10.github.io/gan_control/)
* [Omni-GAN: On the Secrets of cGANs and Beyond](https://openaccess.thecvf.com/content/ICCV2021/papers/Zhou_Omni-GAN_On_the_Secrets_of_cGANs_and_Beyond_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/PeterouZh/Omni-GAN-PyTorch)
* GAN inversion(GAN逆映射)
  * [From Continuity to Editability: Inverting GANs with Consecutive Images](https://arxiv.org/abs/2107.13812)<br>:star:[code](https://github.com/cnnlstm/InvertingGANs_with_ConsecutiveImgs)
  * [GAN Inversion for Out-of-Range Images with Geometric Transformations](https://arxiv.org/abs/2108.08998)<br>:house:[project](https://kkang831.github.io/publication/ICCV_2021_BDInvert/)
* 图像到图像翻译
  * [Unaligned Image-to-Image Translation by Learning to Reweight](https://arxiv.org/abs/2109.11736)<br>:star:[code](https://github.com/Mid-Push/IrwGAN)
  * [Bridging the Gap between Label- and Reference-based Synthesis in Multi-attribute Image-to-Image Translation](https://arxiv.org/abs/2110.05055)<br>:star:[code](https://github.com/huangqiusheng/BridgeGAN)
* Image translation
  * [Scaling-up Disentanglement for Image Translation](https://arxiv.org/abs/2103.14017)<br>:star:[code](https://github.com/avivga/overlord):house:[project](http://www.vision.huji.ac.il/overlord/)


<a name="21"/>

## 21.Active Learning(主动学习)
* [Semi-Supervised Active Learning with Temporal Output Discrepancy](https://arxiv.org/abs/2107.14153)<br>:star:[code](https://github.com/siyuhuang/TOD)
* [Influence Selection for Active Learning](https://arxiv.org/abs/2108.09331)
  

<a name="20"/>

## 20.Adversarial Learning(对抗学习)
* [Feature Importance-aware Transferable Adversarial Attacks](https://arxiv.org/abs/2107.14185)<br>:star:[code](https://github.com/hcguoO0/FIA)
* [TkML-AP: Adversarial Attacks to Top-k Multi-Label Learning](https://arxiv.org/abs/2108.00146)
* [Meta Gradient Adversarial Attack](https://arxiv.org/abs/2108.04204)
* [AGKD-BML: Defense Against Adversarial Attack by Attention Guided Knowledge Distillation and Bi-directional Metric Learning](https://arxiv.org/abs/2108.06017)<br>:star:[code](https://github.com/hongw579/AGKD-BML)
* [Exploiting Multi-Object Relationships for Detecting Adversarial Attacks in Complex Scenes](https://arxiv.org/abs/2108.08421)
* [AdvDrop: Adversarial Attack to DNNs by Dropping Information](https://arxiv.org/abs/2108.09034)<br>:star:[code](https://github.com/RjDuan/AdvDrop)
* [Low Curvature Activations Reduce Overfitting in Adversarial Training](https://arxiv.org/abs/2102.07861)
* [Black-Box Detection of Backdoor Attacks With Limited Information and Data](https://arxiv.org/abs/2103.13127)

<a name="19"/>

## 19.Self-Driving Vehicles(自动驾驶)
* [End-to-End Urban Driving by Imitating a Reinforcement Learning Coach](https://arxiv.org/abs/2108.08265)<br>:star:[code](https://github.com/zhejz/carla-roach)  
* [MultiSiam: Self-supervised Multi-instance Siamese Representation Learning for Autonomous Driving](https://arxiv.org/abs/2108.12178)<br>:star:[code](https://github.com/KaiChen1998/MultiSiam)
* [NEAT: Neural Attention Fields for End-to-End Autonomous Driving](https://arxiv.org/abs/2109.04456)<br>:star:[code](https://github.com/autonomousvision/neat)
* [Safety-aware Motion Prediction with Unseen Vehicles for Autonomous Driving](https://arxiv.org/abs/2109.01510)<br>:star:[code](https://github.com/xrenaa/Safety-Aware-Motion-Prediction)
* [Social-NCE: Contrastive Learning of Socially-aware Motion Representations](https://arxiv.org/abs/2012.11717)<br>:star:[code](https://github.com/vita-epfl/social-nce-crowdnav):tv:[video](https://www.youtube.com/watch?v=s1khZWWiQfA)
* Human trajectory prediction(人体轨迹预测)
  * [Personalized Trajectory Prediction via Distribution Discrimination](https://arxiv.org/abs/2107.14204)<br>:star:[code](https://github.com/CHENGY12/DisDis)
  * [Human Trajectory Prediction via Counterfactual Analysis](https://arxiv.org/abs/2107.14202)(https://github.com/CHENGY12/CausalHTP)
* 轨迹预测
  * [Unlimited Neighborhood Interaction for Heterogeneous Trajectory Prediction](https://arxiv.org/abs/2108.00238)
  * [LOKI: Long Term and Key Intentions for Trajectory Prediction](https://arxiv.org/abs/2108.08236)
  * [MG-GAN: A Multi-Generator Model Preventing Out-of-Distribution Samples in Pedestrian Trajectory Prediction](https://arxiv.org/abs/2108.09274)<br>:star:[code](https://github.com/selflein/MG-GAN)
  * [DenseTNT: End-to-end Trajectory Prediction from Dense Goal Sets](https://arxiv.org/abs/2108.09640)
* 运动预测
  * [RAIN: Reinforced Hybrid Attention Inference Network for Motion Forecasting](https://arxiv.org/abs/2108.01316)<br>:house:[project](https://jiachenli94.github.io/publications/RAIN/)
  * [SLAMP: Stochastic Latent Appearance and Motion Prediction](https://arxiv.org/abs/2108.02760)<br>:house:[project](https://kuis-ai.github.io/slamp/)
  * [SlowFast Rolling-Unrolling LSTMs for Action Anticipation in Egocentric Videos](https://arxiv.org/abs/2109.00829)
* 自动导航
  * [FOVEA: Foveated Image Magnification for Autonomous Navigation](https://arxiv.org/abs/2108.12102)<br>:house:[project](https://www.cs.cmu.edu/~mengtial/proj/fovea/)  
* 交通场景理解
  * [Structured Bird's-Eye-View Traffic Scene Understanding from Onboard Images](https://arxiv.org/abs/2110.01997)<br>:star:[code](https://github.com/ybarancan/STSU)
* 车辆车牌识别
  * 车辆重识别
    * [Heterogeneous Relational Complement for Vehicle Re-identification](https://arxiv.org/abs/2109.07894)
 * 自主赛车
   * [Learn-to-Race: A Multimodal Control Environment for Autonomous Racing](https://openaccess.thecvf.com/content/ICCV2021/papers/Herman_Learn-To-Race_A_Multimodal_Control_Environment_for_Autonomous_Racing_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/learn-to-race/l2r)
 
<a name="18"/>

## 18.Transformers
* [Generic Attention-model Explainability for Interpreting Bi-Modal and Encoder-Decoder Transformers](https://arxiv.org/abs/2103.15679)<br>:open_mouth:oral:star:[code](https://github.com/hila-chefer/Transformer-MM-Explainability)<br>:newspaper:解读:[ICCV2021 Oral-TAU&Facebook提出了通用的Attention模型可解释性](https://mp.weixin.qq.com/s/3skb8XMiwM3QtdH7ZazoYg)
* [PlaneTR: Structure-Guided Transformers for 3D Plane Recovery](https://arxiv.org/abs/2107.13108)<br>:star:[code](https://github.com/IceTTTb/PlaneTR3D)
* [Rethinking and Improving Relative Position Encoding for Vision Transformer](https://arxiv.org/abs/2107.14222)<br>:star:[code](https://github.com/microsoft/AutoML/tree/main/iRPE)
* [Vision Transformer with Progressive Sampling](https://arxiv.org/abs/2108.01684)
* [Paint Transformer: Feed Forward Neural Painting with Stroke Prediction](https://arxiv.org/abs/2108.03798)<br>:open_mouth:oral:star:[code](https://github.com/Huage001/PaintTransformer)
* [Rethinking Spatial Dimensions of Vision Transformers](https://arxiv.org/abs/2103.16302)<br>:star:[code](https://github.com/naver-ai/pit)<br>:newspaper:解读:[ICCV2021-PiT-池化操作不是CNN的专属，ViT说：“我也可以”；南大提出池化视觉Transformer（PiT）](https://mp.weixin.qq.com/s/b051uw8SSu6x-5R27e8AMg)
* [PnP-DETR: Towards Efficient Visual Analysis with Transformers](https://arxiv.org/abs/2109.07036)<br>:star:[code](https://github.com/twangnh/pnp-detr)
* [Describing and Localizing Multiple Changes With Transformers](https://arxiv.org/abs/2103.14146)<br>:star:[code](https://github.com/cvpaperchallenge/Describing-and-Localizing-Multiple-Change-with-Transformers):house:[project](https://cvpaperchallenge.github.io/Describing-and-Localizing-Multiple-Change-with-Transformers/)
* [LeViT: A Vision Transformer in ConvNet's Clothing for Faster Inference](https://openaccess.thecvf.com/content/ICCV2021/papers/Graham_LeViT_A_Vision_Transformer_in_ConvNets_Clothing_for_Faster_Inference_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/facebookresearch/LeViT)
* 密集预测
  * [Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions](https://arxiv.org/abs/2102.12122)<br>:open_mouth:oral:star:[code](https://github.com/whai362/PVT)<br>:newspaper:解读:[大白话Pyramid Vision Transformer](https://mp.weixin.qq.com/s/oJHZWmStQYYzEEOveiuQrQ)
* 3D人体纹理估计
  * [3D Human Texture Estimation from a Single Image with Transformers](https://arxiv.org/abs/2109.02563)<br>:open_mouth:oral:house:[project](https://www.mmlab-ntu.com/project/texformer/)
* 图像编辑  
  * [Multi-Scale Vision Longformer: A New Vision Transformer for High-Resolution Image Encoding](https://arxiv.org/abs/2103.15358)<br>:star:[code](https://github.com/microsoft/vision-longformer)
* OCR  
  * [DocFormer: End-to-End Transformer for Document Understanding](https://arxiv.org/abs/2106.11539)
* 根据音乐生成舞蹈
  * [AI Choreographer: Music Conditioned 3D Dance Generation With AIST++](https://openaccess.thecvf.com/content/ICCV2021/papers/Li_AI_Choreographer_Music_Conditioned_3D_Dance_Generation_With_AIST_ICCV_2021_paper.pdf)<br>:house:[project](https://google.github.io/aichoreographer/):newspaper:简介:[Transformer又又来了，生成配有音乐的丝滑3D舞蹈，开放最大规模数据集AIST++](https://zhuanlan.zhihu.com/p/346151291)


<a name="17"/>

## 17.3D(三维视觉)
* [Discovering 3D Parts from Image Collections](https://arxiv.org/abs/2107.13629)<br>:open_mouth:oral:star:[code](https://github.com/chhankyao/lpd):house:[project](https://chhankyao.github.io/lpd/):tv:[video](https://youtu.be/erNQANNwK6k)
* [PixelSynth: Generating a 3D-Consistent Experience from a Single Image](https://arxiv.org/abs/2108.05892)<br>:star:[code](https://github.com/crockwell/pixelsynth):house:[project](https://crockwell.github.io/pixelsynth/)
* [Towers of Babel: Combining Images, Language, and 3D Geometry for Learning Multimodal Vision](https://arxiv.org/abs/2108.05863)<br>:star:[code](https://github.com/tgxs002/wikiscenes):house:[project](https://www.cs.cornell.edu/projects/babel/)
* [Pixel-Perfect Structure-from-Motion with Featuremetric Refinement](https://arxiv.org/abs/2108.08291)<br>:star:[code](https://github.com/cvg/pixel-perfect-sfm)
* [Learning Anchored Unsigned Distance Functions with Gradient Direction Alignment for Single-view Garment Reconstruction](https://arxiv.org/abs/2108.08478)
* [LSD-StructureNet: Modeling Levels of Structural Detail in 3D Part Hierarchies](https://arxiv.org/abs/2108.13459)
* [Rational Polynomial Camera Model Warping for Deep Learning Based Satellite Multi-View Stereo Matching](https://arxiv.org/abs/2109.11121)<br>:star:[code](https://github.com/WHU-GPCV/SatMVS)
* [Where2Act: From Pixels to Actions for Articulated 3D Objects](https://arxiv.org/abs/2101.02692)<br>:tv:[video](https://www.youtube.com/watch?v=cdMSZru3Aa8)
* [BuildingNet: Learning to Label 3D Buildings](https://arxiv.org/abs/2110.04955)<br>:open_mouth:oral:star:[code](https://github.com/buildingnet/buildingnet_dataset):house:[project](https://buildingnet.org/)
* [SurfGen: Adversarial 3D Shape Synthesis With Explicit Surface Discriminators](https://openaccess.thecvf.com/content/ICCV2021/papers/Luo_SurfGen_Adversarial_3D_Shape_Synthesis_With_Explicit_Surface_Discriminators_ICCV_2021_paper.pdf)
* [Deep Virtual Markers for Articulated 3D Shapes](https://arxiv.org/abs/2108.09000)<br>:star:[code](https://github.com/T2Kim/DeepVirtualMarkers):tv:[video](https://www.youtube.com/watch?v=Raq5axLdG6E)
* 深度估计
  * [StructDepth: Leveraging the structural regularities for self-supervised indoor depth estimation](https://arxiv.org/abs/2108.08574)<br>:star:[code](https://github.com/SJTU-ViSYS/StructDepth)
  * [Bridging Unsupervised and Supervised Depth from Focus via All-in-Focus Supervision](https://arxiv.org/abs/2108.10843)<br>:star:[code](https://github.com/albert100121/AiFDepthNet):house:[project](https://albert100121.github.io/AiFDepthNet/)
  * [Augmenting Depth Estimation with Geospatial Context](https://arxiv.org/abs/2109.09879)
  * [Can Scale-Consistent Monocular Depth Be Learned in a Self-Supervised Scale-Invariant Manner?](https://openaccess.thecvf.com/content/ICCV2021/papers/Wang_Can_Scale-Consistent_Monocular_Depth_Be_Learned_in_a_Self-Supervised_Scale-Invariant_ICCV_2021_paper.pdf)
  * Monocular Depth Estimation(单目深度估计)
    * [MonoIndoor: Towards Good Practice of Self-Supervised Monocular Depth Estimation for Indoor Environments](https://arxiv.org/abs/2107.12429)
    * [Regularizing Nighttime Weirdness: Efficient Self-supervised Monocular Depth Estimation in the Dark](https://arxiv.org/abs/2108.03830)<br>:star:[code](https://github.com/w2kun/RNW)
    * [Towards Interpretable Deep Networks for Monocular Depth Estimation](https://arxiv.org/abs/2108.05312)<br>:star:[code](https://github.com/youzunzhi/InterpretableMDE):tv:[video](https://www.youtube.com/watch?v=er7TF2CusWo)
    * [Self-supervised Monocular Depth Estimation for All Day Images using Domain Separation](https://arxiv.org/abs/2108.07628)<br>:star:[code](https://github.com/LINA-lln/ADDS-DepthNet)
    * [Fine-grained Semantics-aware Representation Enhancement for Self-supervised Monocular Depth Estimation](https://arxiv.org/abs/2108.08829)<br>:open_mouth:oral:star:[code](https://github.com/hyBlue/FSRE-Depth)
    * [Excavating the Potential Capacity of Self-Supervised Monocular Depth Estimation](https://arxiv.org/abs/2109.12484)<br>:star:[code](https://github.com/prstrive/EPCDepth)
    * [R-MSFM: Recurrent Multi-Scale Feature Modulation for Monocular Depth Estimating](https://openaccess.thecvf.com/content/ICCV2021/papers/Zhou_R-MSFM_Recurrent_Multi-Scale_Feature_Modulation_for_Monocular_Depth_Estimating_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/jsczzzk/R-MSFM)
* 深度补全
  * [Bayesian Deep Basis Fitting for Depth Completion With Uncertainty](https://arxiv.org/abs/2103.15254)
* Omnidirectional Localization
  * [PICCOLO: Point Cloud-Centric Omnidirectional Localization](https://arxiv.org/abs/2108.06545)
* 三维重建
  * [Learning Signed Distance Field for Multi-view Surface Reconstruction](https://arxiv.org/abs/2108.09964)<br>:open_mouth:oral
  * [3DStyleNet: Creating 3D Shapes with Geometric and Texture Style Variations](https://arxiv.org/abs/2108.12958)<br>:open_mouth:oral:house:[project](https://nv-tlabs.github.io/3DStyleNet/)
  * [DensePose 3D: Lifting Canonical Surface Maps of Articulated Objects to the Third Dimension](https://arxiv.org/abs/2109.00033)
  * [In-the-Wild Single Camera 3D Reconstruction Through Moving Water Surfaces](https://vccimaging.org/Publications/Xiong2021MovingWater/Xiong2021MovingWater.pdf)<br>:open_mouth:oral:star:[code](https://github.com/vccimaging/Reconstrution_Through_Moving_Water):tv:[video](https://www.youtube.com/watch?v=F6R52hfAs6s)
  * 三维场景重建
    * [Graph-to-3D: End-to-End Generation and Manipulation of 3D Scenes Using Scene Graphs](https://arxiv.org/abs/2108.08841)
    * [VolumeFusion: Deep Depth Fusion for 3D Scene Reconstruction](https://arxiv.org/abs/2108.08623)
  * 三维形状重建  
    * [3DIAS: 3D Shape Reconstruction with Implicit Algebraic Surfaces](https://arxiv.org/abs/2108.08653)<br>:house:[project](https://myavartanoo.github.io/3dias/)
    * [Multiresolution Deep Implicit Functions for 3D Shape Representation](https://arxiv.org/abs/2109.05591)
    * [MVTN: Multi-View Transformation Network for 3D Shape Recognition](https://arxiv.org/abs/2011.13244)<br>:star:[code](https://github.com/ajhamdi/MVTN):tv:[video](https://www.youtube.com/watch?v=1zaHx8ztlhk)
  * 三维网格重建    
    * [Vis2Mesh: Efficient Mesh Reconstruction from Unstructured Point Clouds of Large Scenes with Learned Virtual View Visibility](https://arxiv.org/abs/2108.08378)<br>:star:[code](https://github.com/GDAOSU/vis2mesh) 
* 三维场景
  * [Scene Synthesis via Uncertainty-Driven Attribute Synchronization](https://arxiv.org/abs/2108.13499)<br>:star:[code](https://github.com/yanghtr/Sync2Gen)
* 相机校准
  * [CTRL-C: Camera calibration TRansformer with Line-Classification](https://arxiv.org/abs/2109.02259) 
* 表面重建
  * [Temporally-Coherent Surface Reconstruction via Metric-Consistent Atlases](https://arxiv.org/abs/2104.06950)
* 3D场景合成
  * [Indoor Scene Generation From a Collection of Semantic-Segmented Depth Images](https://arxiv.org/abs/2108.09022)<br>:star:[code](https://github.com/mingjiayang/SGSDI)
* 3D形状识别
  * [Learning Canonical View Representation for 3D Shape Recognition With Arbitrary Views](https://arxiv.org/abs/2108.07084)<br>:star:[code](https://github.com/weixmath/CVR)
 
<a name="16"/>

## 16.Re-Identification(重识别)
##### Object Re-Identification目标(物体)重识别
* [TransReID: Transformer-Based Object Re-Identification](https://arxiv.org/abs/2102.04378)<br>:star:[code](https://github.com/damo-cv/TransReID)
#### Person Re-Identification(人员重识别)
* [Spatio-Temporal Representation Factorization for Video-based Person Re-Identification](https://arxiv.org/abs/2107.11878)
* [Learning Instance-level Spatial-Temporal Patterns for Person Re-identification](https://arxiv.org/abs/2108.00171)<br>:star:[code](https://github.com/RenMin1991/cleaned-DukeMTMC-reID/)
* [Towards Discriminative Representation Learning for Unsupervised Person Re-identification](https://arxiv.org/abs/2108.03439)
* [Learning by Aligning: Visible-Infrared Person Re-identification using Cross-Modal Correspondences](https://arxiv.org/abs/2108.07422)<br>:star:[code](https://github.com/cvlab-yonsei/LbA):house:[project](https://cvlab.yonsei.ac.kr/projects/LbA/)
* [Video-based Person Re-identification with Spatial and Temporal Memory Networks](https://arxiv.org/abs/2108.09039)<br>:house:[project](https://cvlab.yonsei.ac.kr/projects/STMN/)
* [Multi-Expert Adversarial Attack Detection in Person Re-identification Using Context Inconsistency](https://arxiv.org/abs/2108.09891)
* [Clothing Status Awareness for Long-Term Person Re-Identification](https://openaccess.thecvf.com/content/ICCV2021/papers/Huang_Clothing_Status_Awareness_for_Long-Term_Person_Re-Identification_ICCV_2021_paper.pdf)
* [Dense Interaction Learning for Video-Based Person Re-Identification](https://arxiv.org/abs/2103.09013)<br>:open_mouth:oral
* 域适应人员重识别
  * [IDM: An Intermediate Domain Module for Domain Adaptive Person Re-ID](https://arxiv.org/abs/2108.02413)<br>:open_mouth:oral:star:[code](https://github.com/SikaStar/IDM)
* Crowd Counting(拥挤人群计数)
  * [Rethinking Counting and Localization in Crowds:A Purely Point-Based Framework](https://arxiv.org/abs/2107.12746)<br>:open_mouth:oral:star:[code](https://github.com/TencentYoutuResearch)
  * [Uniformity in Heterogeneity:Diving Deep into Count Interval Partition for Crowd Counting](https://arxiv.org/abs/2107.12619)<br>:star:[code](https://github.com/TencentYoutuResearch)
  * [Spatial Uncertainty-Aware Semi-Supervised Crowd Counting](https://arxiv.org/abs/2107.13271)<br>:star:[code](https://github.com/smallmax00/SUA_crowd_counting)
  * [Variational Attention: Propagating Domain-Specific Knowledge for Multi-Domain Learning in Crowd Counting](https://arxiv.org/abs/2108.08023)<br>:star:[code](https://github.com/Zhaoyi-Yan/DKPNet)
  * [Exploiting Sample Correlation for Crowd Counting With Multi-Expert Network](https://openaccess.thecvf.com/content/ICCV2021/papers/Liu_Exploiting_Sample_Correlation_for_Crowd_Counting_With_Multi-Expert_Network_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/streamer-AP)
* 跨模态人员重识别
  * [Cross-Modality Person Re-Identification via Modality Confusion and Center Aggregation](https://openaccess.thecvf.com/content/ICCV2021/papers/Hao_Cross-Modality_Person_Re-Identification_via_Modality_Confusion_and_Center_Aggregation_ICCV_2021_paper.pdf)
* Person Search
  * [ASMR: Learning Attribute-Based Person Search with Adaptive Semantic Margin Regularizer](https://arxiv.org/abs/2108.04533)<br>:house:[project](http://cvlab.postech.ac.kr/research/ASMR/)
* 行人检测
  * [MOTSynth: How Can Synthetic Data Help Pedestrian Detection and Tracking?](https://arxiv.org/abs/2108.09518)
  * [Stacked Homography Transformations for Multi-View Pedestrian Detection](https://openaccess.thecvf.com/content/ICCV2021/papers/Song_Stacked_Homography_Transformations_for_Multi-View_Pedestrian_Detection_ICCV_2021_paper.pdf)
* 行人属性识别
  * [Spatial and Semantic Consistency Regularizations for Pedestrian Attribute Recognition](https://arxiv.org/abs/2109.05686)
* Person Search(行人搜索)
  * [Weakly Supervised Person Search with Region Siamese Networks](https://arxiv.org/abs/2109.06109)
* 行人行为预测
  * [Bifold and Semantic Reasoning for Pedestrian Behavior Prediction](https://arxiv.org/abs/2012.03298)

<a name="15"/>

## 15.Object Tracking(目标跟踪)
* [Saliency-Associated Object Tracking](https://arxiv.org/abs/2108.03637)
* [Box-Aware Feature Enhancement for Single Object Tracking on Point Clouds](https://arxiv.org/abs/2108.04728)<br>:star:[code](https://github.com/Ghostish/BAT)
* [Learning to Track Objects from Unlabeled Videos](https://arxiv.org/abs/2108.12711)<br>:star:[code](https://github.com/VISION-SJTU/USOT)
* [DepthTrack : Unveiling the Power of RGBD Tracking](https://arxiv.org/abs/2108.13962)<br>:star:[code](https://github.com/xiaozai/DeT)
* [Learning Target Candidate Association To Keep Track of What Not To Track](https://arxiv.org/abs/2103.16556)<br>:star:[code](https://github.com/visionml/pytracking)
* 视觉目标跟踪
  * [Learning to Adversarially Blur Visual Object Tracking](https://arxiv.org/abs/2107.12085)<br>:star:[code](https://github.com/tsingqguo/ABA)
  * [Learn to Match: Automatic Matching Network Design for Visual Tracking](https://arxiv.org/abs/2108.00803)<br>:star:[code](https://github.com/JudasDie/SOTS)
  * [Video Annotation for Visual Tracking via Selection and Refinement](https://arxiv.org/abs/2108.03821)<br>:star:[code](https://github.com/Daikenan/VASR)
* 卫星图像跟踪 
  * [HiFT: Hierarchical Feature Transformer for Aerial Tracking](https://arxiv.org/abs/2108.00202)<br>:star:[code](https://github.com/vision4robotics/HiFT)
* 3D多目标跟踪
  * [Exploring Simple 3D Multi-Object Tracking for Autonomous Driving](https://arxiv.org/abs/2108.10312)<br>:star:[code](https://github.com/qcraftai/simtrack)<br>:newspaper:解读:[ICCV 2021丨轻舟智航提出SimTrack: 3D多目标一体化检测与跟踪，简单又精确](https://mp.weixin.qq.com/s/7vCckbjGd65NMgW9evR4Ag)
* 多目标跟踪与分割
  * [Assignment-Space-Based Multi-Object Tracking and Segmentation](https://openaccess.thecvf.com/content/ICCV2021/papers/Choudhuri_Assignment-Space-Based_Multi-Object_Tracking_and_Segmentation_ICCV_2021_paper.pdf)

<a name="14"/>

## 14.Object Detection(目标检测)
* [Rank & Sort Loss for Object Detection and Instance Segmentation](https://arxiv.org/abs/2107.11669)<br>:open_mouth:oral:star:[code](https://github.com/kemaloksuz/RankSortLoss)
* [MDETR : Modulated Detection for End-to-End Multi-Modal Understanding](https://arxiv.org/abs/2104.12763)<br>:open_mouth:oral:star:[code](https://github.com/ashkamath/mdetr)
* [SimROD: A Simple Adaptation Method for Robust Object Detection](https://arxiv.org/abs/2107.13389)<br>:open_mouth:oral
* [GraphFPN: Graph Feature Pyramid Network for Object Detection](https://arxiv.org/abs/2108.00580)
* [Fast Convergence of DETR with Spatially Modulated Co-Attention](https://arxiv.org/abs/2108.02404)<br>:star:[code](https://github.com/gaopengcuhk/SMCA-DETR)
* [Oriented R-CNN for Object Detection](https://arxiv.org/abs/2108.05699)<br>:star:[code](https://github.com/jbwang1997/OBBDetection) 
* [Conditional DETR for Fast Training Convergence](https://arxiv.org/abs/2108.06152)<br>:newspaper:解读:[通过显式寻找物体的 extremity 区域加快 DETR 的收敛：Conditional DETR](https://mp.weixin.qq.com/s/RbtdfFDczrSxi0F4apbx1w)
* [Vector-Decomposed Disentanglement for Domain-Invariant Object Detection](https://arxiv.org/abs/2108.06685)<br>:star:[code](https://github.com/AmingWu/VDD-DAOD)
* [G-DetKD: Towards General Distillation Framework for Object Detectors via Contrastive and Semantic-guided Feature Imitation](https://arxiv.org/abs/2108.07482)
* [ODAM: Object Detection, Association, and Mapping using Posed RGB Video](https://arxiv.org/abs/2108.10165)<br>:open_mouth:oral
* [Reconcile Prediction Consistency for Balanced Object Detection](https://arxiv.org/abs/2108.10809)
* [Deep Structured Instance Graph for Distilling Object Detectors](https://arxiv.org/abs/2109.12862)<br>:star:[code](https://github.com/dvlab-research/Dsig)
* [Towards Rotation Invariance in Object Detection](https://arxiv.org/abs/2109.13488)<br>:star:[code](https://github.com/akasha-imaging/ICCV2021)
* [Morphable Detector for Object Detection on Demand](https://arxiv.org/abs/2110.04917)<br>:star:[code](https://github.com/Zhaoxiangyun/Morphable-Detector)
* [DetCo: Unsupervised Contrastive Learning for Object Detection](https://arxiv.org/abs/2102.04803)<br>:star:[code](https://github.com/xieenze/DetCo)
* 3D目标检测
  * [Geometry Uncertainty Projection Network for Monocular 3D Object Detection](https://arxiv.org/abs/2107.13774)
  * [Fog Simulation on Real LiDAR Point Clouds for 3D Object Detection in Adverse Weather](https://arxiv.org/abs/2108.05249)<br>:star:[code](https://github.com/MartinHahner/LiDAR_fog_sim):house:[project](https://www.trace.ethz.ch/publications/2021/lidar_fog_simulation/)
  * [Is Pseudo-Lidar needed for Monocular 3D Object detection?](https://arxiv.org/abs/2108.06417)<br>:star:[code](https://github.com/TRI-ML/dd3d)
  * [RandomRooms: Unsupervised Pre-training from Synthetic Shapes and Randomized Layouts for 3D Object Detection](https://arxiv.org/abs/2108.07794)
  * [LIGA-Stereo: Learning LiDAR Geometry Aware Representations for Stereo-based 3D Detector](https://arxiv.org/abs/2108.08258)<br>:star:[code](https://github.com/xy-guo/LIGA-Stereo):house:[project](https://xy-guo.github.io/liga/)  
  * [Improving 3D Object Detection with Channel-wise Transformer](https://arxiv.org/abs/2108.10723)
  * [4D-Net for Learned Multi-Modal Alignment](https://arxiv.org/abs/2109.01066)
  * [Pyramid R-CNN: Towards Better Performance and Adaptability for 3D Object Detection](https://arxiv.org/abs/2109.02499)
  * [Voxel Transformer for 3D Object Detection](https://arxiv.org/abs/2109.02497)
  * [An End-to-End Transformer Model for 3D Object Detection](https://arxiv.org/abs/2109.08141)<br>:open_mouth:oral:star:[code](https://github.com/facebookresearch/3detr):house:[project](https://facebookresearch.github.io/3detr/)
* 目标定位
  * 弱监督目标定位
    * [Normalization Matters in Weakly Supervised Object Localization](https://arxiv.org/abs/2107.13221)
    * [Online Refinement of Low-Level Feature Based Activation Map for Weakly Supervised Object Localization](https://openaccess.thecvf.com/content/ICCV2021/papers/Xie_Online_Refinement_of_Low-Level_Feature_Based_Activation_Map_for_Weakly_ICCV_2021_paper.pdf)
    * [Foreground Activation Maps for Weakly Supervised Object Localization](https://openaccess.thecvf.com/content/ICCV2021/papers/Meng_Foreground_Activation_Maps_for_Weakly_Supervised_Object_Localization_ICCV_2021_paper.pdf)
* Anomaly Detection(图像异常检测)
  * [Divide-and-Assemble: Learning Block-wise Memory for Unsupervised Anomaly Detection](https://arxiv.org/abs/2107.13118)
* 弱监督目标检测
  * [Boosting Weakly Supervised Object Detection via Learning Bounding Box Adjusters](https://arxiv.org/abs/2108.01499)<br>:star:[code](https://github.com/DongSky/lbba_boosted_wsod)
* OOD 检测
  * [Triggering Failures: Out-Of-Distribution detection by learning from local adversarial attacks in Semantic Segmentation](https://arxiv.org/abs/2108.01634)<br>:star:[code](https://github.com/valeoai/obsnet)
* 显著目标检测
  * [Disentangled High Quality Salient Object Detection](https://arxiv.org/abs/2108.03551)
  * [Specificity-preserving RGB-D Saliency Detection](https://arxiv.org/abs/2108.08162)<br>:star:[code](https://github.com/taozh2017/SPNet)
  * [Light Field Saliency Detection with Dual Local Graph Learning and Reciprocative Guidance](https://arxiv.org/abs/2108.06384)
  * RGB-D显著目标检测
    * [RGB-D Saliency Detection via Cascaded Mutual Information Minimization](https://arxiv.org/abs/2109.07246)<br>:star:[code](https://github.com/JingZhang617/cascaded_rgbd_sod)
  * co-saliency detection
    * [Summarize and Search: Learning Consensus-aware Dynamic Convolution for Co-Saliency Detection](https://arxiv.org/abs/2110.00338)<br>:star:[code](https://github.com/nnizhang/CADC)
* 违禁物品检测
  * [Towards Real-World Prohibited Item Detection: A Large-Scale X-ray Benchmark](https://arxiv.org/abs/2108.07020)<br>:sunflower:[dataset](https://github.com/bywang2018/security-dataset)
* 小样本目标检测
  * [DeFRCN: Decoupled Faster R-CNN for Few-Shot Object Detection](https://arxiv.org/abs/2108.09017)<br>:star:[code](https://github.com/er-muyue/DeFRCN)
  * [Query Adaptive Few-Shot Object Detection With Heterogeneous Graph Convolutional Networks](https://openaccess.thecvf.com/content/ICCV2021/papers/Han_Query_Adaptive_Few-Shot_Object_Detection_With_Heterogeneous_Graph_Convolutional_Networks_ICCV_2021_paper.pdf)
* 视觉关系协同定位
  * [Few-shot Visual Relationship Co-localization](https://arxiv.org/abs/2108.11618)<br>:star:[code](https://github.com/vl2g/VRC):house:[project](https://vl2g.github.io/projects/vrc/)
* 密集目标检测
  * [Mutual Supervision for Dense Object Detection](https://arxiv.org/abs/2109.05986)<br>:star:[code](https://github.com/MCG-NJU)
* 域适应目标检测
  * [Seeking Similarities over Differences: Similarity-based Domain Alignment for Adaptive Object Detection](https://arxiv.org/abs/2110.01428)
* 图像篡改检测
  * [Image Manipulation Detection by Multi-View Multi-Scale Supervision](https://arxiv.org/abs/2104.06832)<br>:star:[code](https://github.com/dong03/MVSS-Net)
* Visual Relationship Detection(VRD视觉关系检测)
  * [Grounding Consistency: Distilling Spatial Common Sense for Precise Visual Relationship Detection](https://openaccess.thecvf.com/content/ICCV2021/papers/Diomataris_Grounding_Consistency_Distilling_Spatial_Common_Sense_for_Precise_Visual_Relationship_ICCV_2021_paper.pdf)
* 长尾目标检测
  * [Exploring Classification Equilibrium in Long-Tailed Object Detection](http://arxiv.org/abs/2108.07507)<br>:star:[code](https://github.com/fcjian/LOCE)
* Salient Object Ranking
  * [Salient Object Ranking with Position-Preserved Attention](https://arxiv.org/abs/2106.05047)<br>:star:[code](https://github.com/EricFH/SOR)


<a name="13"/>

## 13.Image Segmentation(图像分割)
* [Standardized Max Logits: A Simple yet Effective Approach for Identifying Unexpected Road Obstacles in Urban-Scene Segmentation](https://arxiv.org/abs/2107.11264)<br>:open_mouth:oral:star:[code](https://github.com/shjung13/Standardized-max-logits):tv:[video](https://www.youtube.com/watch?v=leBJZHzX6xM)
* [TransForensics: Image Forgery Localization with Dense Self-Attention](https://arxiv.org/abs/2108.03871)
* [From Contexts to Locality: Ultra-high Resolution Image Segmentation via Locality-aware Contextual Correlation](https://arxiv.org/abs/2109.02580)<br>:star:[code](https://github.com/liqiokkk/FCtL)
* [Labels4Free: Unsupervised Segmentation using StyleGAN](https://arxiv.org/abs/2103.14968)<br>:house:[project](https://rameenabdal.github.io/Labels4Free/):tv:[video](https://www.youtube.com/watch?v=_pHunGpvLVk)
* [Warp-Refine Propagation: Semi-Supervised Auto-labeling via Cycle-consistency](https://arxiv.org/abs/2109.13432)
* [Scaling up instance annotation via label propagation](https://arxiv.org/abs/2110.02277)<br>:star:[code](https://github.com/ethanweber/scaling-anno):house:[project](http://scaling-anno.csail.mit.edu/)
* 全景分割
  * [Panoptic Narrative Grounding](https://arxiv.org/abs/2109.04988)
* 语义分割
  * [Re-distributing Biased Pseudo Labels for Semi-supervised Semantic Segmentation: A Baseline Investigation](https://arxiv.org/abs/2107.11279)<br>:open_mouth:oral:star:[code](https://github.com/CVMI-Lab/DARS)
  * [Personalized Image Semantic Segmentation](https://arxiv.org/abs/2107.13978)<br>:star:[code](https://github.com/zhangyuygss/PIS)
  * [RECALL: Replay-based Continual Learning in Semantic Segmentation](https://arxiv.org/abs/2108.03673)
  * [Deep Metric Learning for Open World Semantic Segmentation](https://arxiv.org/abs/2108.04562)
  * [LabOR: Labeling Only if Required for Domain Adaptive Semantic Segmentation](https://arxiv.org/abs/2108.05570)<br>:open_mouth:oral
  * [Dual Path Learning for Domain Adaptation of Semantic Segmentation](https://arxiv.org/abs/2108.06337)<br>:star:[code](https://github.com/royee182/DPL)
  * [Multi-Target Adversarial Frameworks for Domain Adaptation in Semantic Segmentation](https://arxiv.org/abs/2108.06962)
  * [Exploiting a Joint Embedding Space for Generalized Zero-Shot Semantic Segmentation](https://arxiv.org/abs/2108.06536)<br>:star:[code](https://github.com/cvlab-yonsei/JoEm):house:[project](https://cvlab.yonsei.ac.kr/projects/JoEm/)
  * [Multi-Anchor Active Domain Adaptation for Semantic Segmentation](https://arxiv.org/abs/2108.08012)<br>:open_mouth:oral:star:[code](https://github.com/munanning/MADA)
  * [Pixel Contrastive-Consistent Semi-Supervised Semantic Segmentation](https://arxiv.org/abs/2108.09025)
  * [Self-Regulation for Semantic Segmentation](https://arxiv.org/abs/2108.09702)<br>:star:[code](https://github.com/dongzhang89/SR-SS)
  * [ShapeConv: Shape-aware Convolutional Layer for Indoor RGB-D Semantic Segmentation](https://arxiv.org/abs/2108.10528)
  * [Generalize then Adapt: Source-Free Domain Adaptive Semantic Segmentation](https://arxiv.org/abs/2108.11249)<br>:house:[project](https://sites.google.com/view/sfdaseg)
  * [Mining Contextual Information Beyond Image for Semantic Segmentation](https://arxiv.org/abs/2108.11819)<br>:star:[code](https://github.com/CharlesPikachu/mcibi)
  * [ISNet: Integrate Image-Level and Semantic-Level Context for Semantic Segmentation](https://arxiv.org/abs/2108.12382)<br>:star:[code](https://github.com/SegmentationBLWX/sssegmentation)
  * [Pseudo-mask Matters in Weakly-supervised Semantic Segmentation](https://arxiv.org/abs/2108.12995)
  * [SIGN: Spatial-information Incorporated Generative Network for Generalized Zero-shot Semantic Segmentation](https://arxiv.org/abs/2108.12517)
  * [Region-Aware Contrastive Learning for Semantic Segmentation](https://openaccess.thecvf.com/content/ICCV2021/papers/Hu_Region-Aware_Contrastive_Learning_for_Semantic_Segmentation_ICCV_2021_paper.pdf)
  * [GP-S3Net: Graph-Based Panoptic Sparse Semantic Segmentation Network](https://openaccess.thecvf.com/content/ICCV2021/papers/Razani_GP-S3Net_Graph-Based_Panoptic_Sparse_Semantic_Segmentation_Network_ICCV_2021_paper.pdf)
  * 小样本语义分割
    * [Simpler is Better: Few-shot Semantic Segmentation with Classifier Weight Transformer](https://arxiv.org/abs/2108.03032)<br>:star:[code](https://github.com/zhiheLu/CWT-for-FSS)
    * [Learning Meta-class Memory for Few-Shot Semantic Segmentation](https://arxiv.org/abs/2108.02958)
  * 3D语义分割
    * [VMNet: Voxel-Mesh Network for Geodesic-Aware 3D Semantic Segmentation](https://arxiv.org/abs/2107.13824)<br>:open_mouth:oral:star:[code](https://github.com/hzykent/VMNet)
  * 视频语义分割
    * [Domain Adaptive Video Segmentation via Temporal Consistency Regularization](https://arxiv.org/abs/2107.11004)<br>:star:[code](https://github.com/Dayan-Guan/DA-VSN)
  * 弱监督语义分割
    * [Leveraging Auxiliary Tasks with Affinity Learning for Weakly Supervised Semantic Segmentation](https://arxiv.org/abs/2107.11787)<br>:star:[code](https://github.com/xulianuwa/AuxSegNet)
    * [Complementary Patch for Weakly Supervised Semantic Segmentation](https://arxiv.org/abs/2108.03852)
  * 点云语义分割
    * [ReDAL: Region-based and Diversity-aware Active Learning for Point Cloud Semantic Segmentation](https://arxiv.org/abs/2107.11769)<br>:tv:[video](https://www.youtube.com/watch?v=XJeb9kMxs5E)
    * [Perturbed Self-Distillation: Weakly Supervised Large-Scale Point Cloud Semantic Segmentation](https://openaccess.thecvf.com/content/ICCV2021/papers/Zhang_Perturbed_Self-Distillation_Weakly_Supervised_Large-Scale_Point_Cloud_Semantic_Segmentation_ICCV_2021_paper.pdf)
    * [TempNet: Online Semantic Segmentation on Large-Scale Point Cloud Series](https://openaccess.thecvf.com/content/ICCV2021/papers/Zhou_TempNet_Online_Semantic_Segmentation_on_Large-Scale_Point_Cloud_Series_ICCV_2021_paper.pdf)
    * [Guided Point Contrastive Learning for Semi-Supervised Point Cloud Semantic Segmentation](https://openaccess.thecvf.com/content/ICCV2021/papers/Jiang_Guided_Point_Contrastive_Learning_for_Semi-Supervised_Point_Cloud_Semantic_Segmentation_ICCV_2021_paper.pdf)
  * OOD
    * [Entropy Maximization and Meta Classification for Out-of-Distribution Detection in Semantic Segmentation](https://arxiv.org/abs/2012.06575)<br>:star:[code](https://github.com/robin-chan/meta-ood)
* 实例分割
  * [Rank & Sort Loss for Object Detection and Instance Segmentation](https://arxiv.org/abs/2107.11669)<br>:open_mouth:oral:star:[code](https://github.com/kemaloksuz/RankSortLoss)
  * [SOTR: Segmenting Objects with Transformers](https://arxiv.org/abs/2108.06747)<br>:star:[code](https://github.com/easton-cau/SOTR)
  * [A Weakly Supervised Amodal Segmenter with Boundary Uncertainty Estimation](https://arxiv.org/abs/2108.09897)
  * [Instances as Queries](https://arxiv.org/abs/2105.01928)<br>:star:[code](https://github.com/hustvl/QueryInst):tv:[video](https://www.youtube.com/watch?v=3Fqwvn6_oUQ)
  * [CrossVIS: Crossover Learning for Fast Online Video Instance Segmentation](https://arxiv.org/abs/2104.05970)<br>:star:[code](https://github.com/hustvl/CrossVIS):tv:[video](https://www.youtube.com/watch?v=tPvYYjTgaNs)
  * 视频实例分割
    * [Video Instance Segmentation with a Propose-Reduce Paradigm](https://arxiv.org/abs/2103.13746)<br>:star:[code](https://github.com/dvlab-research/ProposeReduce)
  * 3D实例分割
    * [Hierarchical Aggregation for 3D Instance Segmentation](https://arxiv.org/abs/2108.02350)<br>:star:[code](https://github.com/hustvl/HAIS)
* 小样本分割
  * [Mining Latent Classes for Few-shot Segmentation](https://arxiv.org/abs/2103.15402)<br>:open_mouth:oral:star:[code](https://github.com/LiheYoung/MiningFSS)
* Human Motion Segmentation(人体运动分割)
  * [Graph Constrained Data Representation Learning for Human Motion Segmentation](https://arxiv.org/abs/2107.13362)<br>:star:[code](https://github.com/mdimiccoli/GCRL-for-HMS/)
* 点云分割
  * [Learning with Noisy Labels for Robust Point Cloud Segmentation](https://arxiv.org/abs/2107.14230)<br>:open_mouth:oral:star:[code](https://github.com/pleaseconnectwifi/PNAL):house:[project](https://shuquanye.com/PNAL_website/)
  * [DRINet: A Dual-Representation Iterative Learning Network for Point Cloud Segmentation](https://arxiv.org/abs/2108.04023)
* 视频目标分割(VOS)
  * [Full-Duplex Strategy for Video Object Segmentation](https://arxiv.org/abs/2108.03151)<br>:house:[project](http://dpfan.net/FSNet/)
  * [Joint Inductive and Transductive Learning for Video Object Segmentation](https://arxiv.org/abs/2108.03679)<br>:star:[code](https://github.com/maoyunyao/JOINT)
  * [Hierarchical Memory Matching Network for Video Object Segmentation](https://arxiv.org/abs/2109.11404)<br>:star:[code](https://github.com/Hongje/HMMN)
  * [Self-supervised Video Object Segmentation by Motion Grouping](https://arxiv.org/abs/2104.07658)<br>:star:[code](https://github.com/charigyang/motiongrouping):house:[project](https://charigyang.github.io/motiongroup/):tv:[video](https://www.youtube.com/watch?v=Q0dLExLXZIw)
  * [Deep Transport Network for Unsupervised Video Object Segmentation](https://openaccess.thecvf.com/content/ICCV2021/papers/Zhang_Deep_Transport_Network_for_Unsupervised_Video_Object_Segmentation_ICCV_2021_paper.pdf)
* 语义场景分割 
  * [BiMaL: Bijective Maximum Likelihood Approach to Domain Adaptation in Semantic Scene Segmentation](https://arxiv.org/abs/2108.03267)<br>:star:[code](https://github.com/uark-cviu/BiMaL)
* Referring Segmentation(基于文本的分割) 
  * [Vision-Language Transformer and Query Generation for Referring Segmentation](https://arxiv.org/abs/2108.05565)<br>:star:[code](https://github.com/henghuiding/Vision-Language-Transformer)
* 场景理解
  * [DeepPanoContext: Panoramic 3D Scene Understanding with Holistic Scene Context Graph and Relation-based Optimization](https://arxiv.org/abs/2108.10743)
  * [ACDC: The Adverse Conditions Dataset With Correspondences for Semantic Driving Scene Understanding](https://openaccess.thecvf.com/content/ICCV2021/papers/Sakaridis_ACDC_The_Adverse_Conditions_Dataset_With_Correspondences_for_Semantic_Driving_ICCV_2021_paper.pdf)<br>:house:[project](https://acdc.vision.ee.ethz.ch/)
* CMA  
  * [Towards Better Explanations of Class Activation Mapping](https://arxiv.org/abs/2102.05228)
  * [Keep CALM and Improve Visual Feature Attribution](https://arxiv.org/abs/2106.07861)<br>:star:[code](https://github.com/naver-ai/calm)

<a name="12"/>

## 12.Image/Fine-Grained Classification(图像/细粒度分类) 
* [DiagViB-6: A Diagnostic Benchmark Suite for Vision Models in the Presence of Shortcut and Generalization Opportunities](https://arxiv.org/abs/2108.05779)
* [Online Continual Learning For Visual Food Classification](https://arxiv.org/abs/2108.06781)
* [A Unified Objective for Novel Class Discovery](https://arxiv.org/abs/2108.08536)<br>:open_mouth:oral:star:[code](https://github.com/DonkeyShot21/UNO):house:[project](https://ncd-uno.github.io/)<br>:newspaper:解读:[ICCV2021 Oral | UNO：用于“新类发现”的统一目标函数，简化训练流程！已开源！](https://mp.weixin.qq.com/s/3aQ5AUKOAnO7kDtsxhRJ3Q)
* [Improving Generalization of Batch Whitening by Convolutional Unit Optimization](https://arxiv.org/abs/2108.10629)<br>:star:[code](https://github.com/YooshinCho/pytorch_ConvUnitOptimization)
* [Towards Learning Spatially Discriminative Feature Representations](https://arxiv.org/abs/2109.01359)
* [CrossViT: Cross-Attention Multi-Scale Vision Transformer for Image Classification](https://arxiv.org/abs/2103.14899)<br>:star:[code](https://github.com/IBM/CrossViT)<br>:newspaper:解读:[ICCV2021 MIT-IBM沃森开源CrossViT：Transformer走向多分支、多尺度](https://mp.weixin.qq.com/s/aqDaF4Iy96Nx1pvX__6vHg)
* [SCOUTER: Slot Attention-based Classifier for Explainable Image Recognition](https://arxiv.org/abs/2009.06138)<br>:star:[code](https://github.com/wbw520/scouter)
* [Influence-Balanced Loss for Imbalanced Visual Classification](https://arxiv.org/abs/2110.02444)<br>:star:[code](https://github.com/pseulki/IB-Loss) 
* [Explanations for Occluded Images](https://arxiv.org/abs/2103.03622)<br>:star:[code](https://github.com/theyoucheng/deepcover):house:[project](https://www.cprover.org/deepcover/iccv2021/):tv:[video](https://www.cprover.org/deepcover/iccv2021/iccv2021-talk-compatible.mp4)
* [Understanding Robustness of Transformers for Image Classification](http://arxiv.org/abs/2103.14586)
* 长尾识别
  * [Parametric Contrastive Learning](https://arxiv.org/abs/2107.12028)<br>:star:[code](https://github.com/jiequancui/Parametric-Contrastive-Learning)
  * [ACE: Ally Complementary Experts for Solving Long-Tailed Recognition in One-Shot](https://arxiv.org/abs/2108.02385)<br>:open_mouth:oral:star:[code](https://github.com/jrcai/ACE)
  * [Self Supervision to Distillation for Long-Tailed Visual Recognition](https://arxiv.org/abs/2109.04075)<br>:star:[code](https://github.com/MCG-NJU)
  * [Distilling Virtual Examples for Long-Tailed Recognition](https://arxiv.org/abs/2103.15042)
  * [Distributional Robustness Loss for Long-Tail Learning](https://arxiv.org/abs/2104.03066)
* 细粒度
  * [Webly Supervised Fine-Grained Recognition: Benchmark Datasets and An Approach](https://arxiv.org/abs/2108.02399)<br>:star:[code](https://github.com/NUST-Machine-Intelligence-Laboratory/weblyFG-dataset)
  * [Learning Canonical 3D Object Representation for Fine-Grained Recognition](https://arxiv.org/abs/2108.04628)
  * [Counterfactual Attention Learning for Fine-Grained Visual Categorization and Re-identification](https://arxiv.org/abs/2108.08728)<br>:star:[code](https://github.com/raoyongming/CAL)
* 小样本分类
  * [Transductive Few-Shot Classification on the Oblique Manifold](https://arxiv.org/abs/2108.04009)
  * [Relational Embedding for Few-Shot Classification](https://arxiv.org/abs/2108.09666)
  * [Binocular Mutual Learning for Improving Few-shot Classification](https://arxiv.org/abs/2108.12104)<br>:star:[code](https://github.com/ZZQzzq/BML)
  * [Partner-Assisted Learning for Few-Shot Image Classification](https://arxiv.org/abs/2109.07607)
  * [On the Importance of Distractors for Few-Shot Classification](https://arxiv.org/abs/2109.09883)<br>:star:[code](https://github.com/quantacode/Contrastive-Finetuning)
  * [Few-Shot Image Classification: Just Use a Library of Pre-Trained Feature Extractors and a Simple Classifier](https://arxiv.org/abs/2101.00562)
* 多标签分类
  * [Asymmetric Loss For Multi-Label Classification](https://arxiv.org/abs/2009.14119)<br>:star:[code](https://github.com/Alibaba-MIIL/ASL)
  * [Semantic Diversity Learning for Zero-Shot Multi-label Classification](https://arxiv.org/abs/2105.05926)<br>:star:[code](https://github.com/Alibaba-MIIL/ZS_SDL)

  
<a name="11"/>

## 11.Visual Question Answering(视觉问答)
* [Greedy Gradient Ensemble for Robust Visual Question Answering](https://arxiv.org/abs/2107.12651)<br>:star:[code](https://github.com/GeraldHan/GGE)
* [Weakly Supervised Relative Spatial Reasoning for Visual Question Answering](https://arxiv.org/abs/2109.01934)<br>:star:[code](https://github.com/pratyay-banerjee/weak_sup_vqa)
* [Calibrating Concepts and Operations: Towards Symbolic Reasoning on Real Images](https://arxiv.org/abs/2110.00519)<br>:star:[code](https://github.com/Lizw14/CaliCO)
* [Unshuffling Data for Improved Generalization in Visual Question Answering](https://openaccess.thecvf.com/content/ICCV2021/papers/Teney_Unshuffling_Data_for_Improved_Generalization_in_Visual_Question_Answering_ICCV_2021_paper.pdf)
* video question answering
  * [Just Ask: Learning to Answer Questions from Millions of Narrated Videos](https://arxiv.org/abs/2012.00451)<br>:open_mouth:oral:star:[code](https://github.com/antoyang/just-ask):house:[project](https://antoyang.github.io/just-ask.html)
  * [Adversarial VQA: A New Benchmark for Evaluating the Robustness of VQA Models](https://arxiv.org/abs/2106.00245)<br>:house:[project](https://adversarialvqa.github.io/)
  * [Env-QA: A Video Question Answering Benchmark for Comprehensive Understanding of Dynamic Environments](https://openaccess.thecvf.com/content/ICCV2021/papers/Gao_Env-QA_A_Video_Question_Answering_Benchmark_for_Comprehensive_Understanding_of_ICCV_2021_paper.pdf)<br>:sunflower:[dataset](http://vipl.ict.ac.cn/resources/envqa)
* A-VQA
  * [Pano-AVQA: Grounded Audio-Visual Question Answering on 360∘ Videos](https://arxiv.org/abs/2110.05122)


<a name="10"/>

## 10.OCR
* [Joint Visual Semantic Reasoning: Multi-Stage Decoder for Text Recognition](https://arxiv.org/abs/2107.12090)<br>:tv:[video](https://www.youtube.com/watch?v=GPk-O3ZqIoI)
* [Text is Text, No Matter What: Unifying Text Recognition using Knowledge Distillation](https://arxiv.org/abs/2107.12087)<br>:tv:[video](https://www.youtube.com/watch?v=8VLkaf_hGdQ)
* [Towards the Unseen: Iterative Text Recognition by Distilling from Errors](https://arxiv.org/abs/2107.12081)<br>:tv:[video](https://www.youtube.com/watch?v=ywaGXFZIiDI)
* 任意形状文本检测
  * [Adaptive Boundary Proposal Network for Arbitrary Shape Text Detection](https://arxiv.org/abs/2107.12664)<br>:star:[code](https://github.com/GXYM/TextBPN)
* 场景文本识别
  * [From Two to One: A New Scene Text Recognizer with Visual Language Modeling Network](https://arxiv.org/abs/2108.09661)<br>:star:[code](https://github.com/wangyuxin87/VisionLAN)
* 场景文本替换
  * [STRIVE: Scene Text Replacement In Videos](https://arxiv.org/abs/2109.02762)<br>:house:[project](https://striveiccv2021.github.io/STRIVE-ICCV2021/)  
  
<a name="9"/>

## 9.Video
* Action Detection and Recognition(人体动作检测与识别)
  * [Channel-wise Topology Refinement Graph Convolution for Skeleton-Based Action Recognition](https://arxiv.org/abs/2107.12213)<br>:star:[code](https://github.com/Uason-Chen/CTR-GCN)
  * [MGSampler: An Explainable Sampling Strategy for Video Action Recognition](https://arxiv.org/abs/2104.09952)
  * [Skeleton Cloud Colorization for Unsupervised 3D Action Representation Learning](https://arxiv.org/abs/2108.01959)
  * [Video Pose Distillation for Few-Shot, Fine-Grained Sports Action Recognition](https://arxiv.org/abs/2109.01305)
  * [Class Semantics-based Attention for Action Detection](https://arxiv.org/abs/2109.02613)
  * [MultiSports: A Multi-Person Video Dataset of Spatio-Temporally Localized Sports Actions](https://arxiv.org/pdf/2105.07404.pdf)<br>:star:[code](https://github.com/MCG-NJU/MultiSports)
  * [AdaSGN: Adapting Joint Number and Model Size for Efficient Skeleton-Based Action Recognition](https://arxiv.org/abs/2103.11770)<br>:star:[code](https://github.com/lshiwjx/AdaSGN)
  * [OadTR: Online Action Detection With Transformers](https://arxiv.org/abs/2106.11149)<br>:star:[code](https://github.com/wangxiang1230/OadTR)
  * [Self-Supervised 3D Skeleton Action Representation Learning With Motion Consistency and Continuity](https://openaccess.thecvf.com/content/ICCV2021/papers/Su_Self-Supervised_3D_Skeleton_Action_Representation_Learning_With_Motion_Consistency_and_ICCV_2021_paper.pdf)
  * 零样本动作识别
    * [Elaborative Rehearsal for Zero-shot Action Recognition](https://arxiv.org/abs/2108.02833)<br>:star:[code](https://github.com/DeLightCMU/ElaborativeRehearsal)
  * Temporal Action Localization(时序动作定位)
    * [Enriching Local and Global Contexts for Temporal Action Localization](https://arxiv.org/abs/2107.12960)
    * [Learning Action Completeness from Points for Weakly-supervised Temporal Action Localization](https://arxiv.org/abs/2108.05029)<br>:open_mouth:oral:star:[code](https://github.com/Pilhyeon)
    * [Foreground-Action Consistency Network for Weakly Supervised Temporal Action Localization](https://arxiv.org/abs/2108.06524)<br>:star:[code](https://github.com/LeonHLJ/FAC-Net)
  * Temporal Action Proposal Generation(时序动作提案生成)
    * [Relaxed Transformer Decoders for Direct Action Proposal Generation](https://arxiv.org/abs/2102.01894)<br>:star:[code](https://github.com/MCG-NJU/RTD-Action)
* Action Quality Assessment(行动质量评估)
  * [Group-aware Contrastive Regression for Action Quality Assessment](https://arxiv.org/abs/2108.07797)
* Video Rescaling
  * [Self-Conditioned Probabilistic Learning of Video Rescaling](https://arxiv.org/abs/2107.11639)
* Video activity localisation
  * [Cross-Sentence Temporal and Semantic Relations in Video Activity Localisation](https://arxiv.org/abs/2107.11443)<br>:newspaper:解读:[ICCV2021 | 如何高效视频定位？QMUL&北大&Adobe强强联手提出弱监督CRM，性能SOTA](https://mp.weixin.qq.com/s/tlGzpUU56HWjVqDtVOkdWg)
* 视频修复
  * [Internal Video Inpainting by Implicit Long-range Propagation](https://arxiv.org/abs/2108.01912)<br>:star:[code](https://github.com/Tengfei-Wang/Implicit-Internal-Video-Inpainting):house:[project](https://tengfei-wang.github.io/Implicit-Internal-Video-Inpainting/)
  * [Occlusion-Aware Video Object Inpainting](https://arxiv.org/abs/2108.06765)<br>:house:[project](http://www.kelei.site/voin/)
  * [FuseFormer: Fusing Fine-Grained Information in Transformers for Video Inpainting](https://arxiv.org/abs/2109.02974)<br>:star:[code](https://github.com/ruiliu-ai/FuseFormer)
* 视频分析
  * 视频表征学习
    * [Enhancing Self-supervised Video Representation Learning via Multi-level Feature Optimization](https://arxiv.org/abs/2108.02183)
    * [Self-Supervised Video Representation Learning with Meta-Contrastive Network](https://arxiv.org/abs/2108.08426)
    * [Long Short View Feature Decomposition via Contrastive Video Representation Learning](https://arxiv.org/abs/2109.11593)
* 视频剪辑
  * [Learning to Cut by Watching Movies](https://arxiv.org/abs/2108.04294)<br>:star:[code](https://github.com/PardoAlejo/LearningToCut):house:[project](https://www.alejandropardo.net/publication/learning-to-cut/) 
* 视频字幕
  * Dense Video Captioning
    * [End-to-End Dense Video Captioning with Parallel Decoding](https://arxiv.org/abs/2108.07781)<br>:star:[code](https://github.com/ttengwang/PDVC)
* 视频编码
  * [Overfitting the Data: Compact Neural Video Delivery via Content-aware Feature Modulation](https://arxiv.org/abs/2108.08202)<br>:star:[code](https://github.com/Neural-video-delivery/CaFM-Pytorch-ICCV2021)<br>:newspaper:解读:[ICCV2021—工业界中的神经网络视频传输超分算法](https://mp.weixin.qq.com/s/dQCZaZNz0oCMHQQEdV79aA)
* 视频生成
  * [Click to Move: Controlling Video Generation with Sparse Motion](https://arxiv.org/abs/2108.08815)<br>:star:[code](https://github.com/PierfrancescoArdino/C2M)
* Video Relation Detection(视频关系检测)
  * [Social Fabric: Tubelet Compositions for Video Relation Detection](https://arxiv.org/abs/2108.08363)<br>:star:[code](https://github.com/shanshuo/Social-Fabric)
* Video Grounding
  * [Support-Set Based Cross-Supervision for Video Grounding](https://arxiv.org/abs/2108.10576)
* 视频精彩片段检测
  * [Cross-category Video Highlight Detection via Set-based Learning](https://arxiv.org/abs/2108.11770)<br>:star:[code](https://github.com/ChrisAllenMing/Cross_Category_Video_Highlight)
  * [PR-Net: Preference Reasoning for Personalized Video Highlight Detection](https://arxiv.org/abs/2109.01799)
  * [HighlightMe: Detecting Highlights from Human-Centric Videos](https://arxiv.org/abs/2110.01774)
  * [Temporal Cue Guided Video Highlight Detection With Low-Rank Audio-Visual Fusion](https://openaccess.thecvf.com/content/ICCV2021/papers/Ye_Temporal_Cue_Guided_Video_Highlight_Detection_With_Low-Rank_Audio-Visual_Fusion_ICCV_2021_paper.pdf)
* 视频识别
  * [Searching for Two-Stream Models in Multivariate Space for Video Recognition](https://arxiv.org/abs/2108.12957)
  * [Adaptive Focus for Efficient Video Recognition](https://arxiv.org/abs/2105.03245)<br>:open_mouth:oral:star:[code](https://github.com/blackfeather-wang/AdaFocus)
  * [AdaMML: Adaptive Multi-Modal Learning for Efficient Video Recognition](https://arxiv.org/abs/2105.05165)<br>:star:[code](https://github.com/IBM/AdaMML):house:[project](https://rpand002.github.io/adamml.html)
* Motion Retargeting(运动重定位)
  * [Contact-Aware Retargeting of Skinned Motion](https://arxiv.org/abs/2109.07431)<br>:tv:[video](https://www.youtube.com/watch?v=qQ4HO2Hibsk)
* 视频预测
  * [A Hierarchical Variational Neural Uncertainty Model for Stochastic Video Prediction](https://arxiv.org/abs/2110.03446)<br>:open_mouth:oral
* 视频合成
  * [iPOKE: Poking a Still Image for Controlled Stochastic Video Synthesis](https://arxiv.org/abs/2107.02790)<br>:star:[code](https://github.com/CompVis/ipoke):house:[project](https://bit.ly/3dJN4Lf)
* 视频帧插值
  * [Training Weakly Supervised Video Frame Interpolation With Events](https://openaccess.thecvf.com/content/ICCV2021/papers/Yu_Training_Weakly_Supervised_Video_Frame_Interpolation_With_Events_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/YU-Zhiyang/WEVI)
* Deepfake 视频检测
  * [ID-Reveal: Identity-aware DeepFake Video Detection](https://openaccess.thecvf.com/content/ICCV2021/papers/Cozzolino_ID-Reveal_Identity-Aware_DeepFake_Video_Detection_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/grip-unina/id-reveal)

<a name="8"/>

## 8.Human Pose Estimation(人体姿态估计)
* [Human Pose Regression with Residual Log-likelihood Estimation](https://arxiv.org/abs/2107.11291)<br>:open_mouth:oral:star:[code](https://github.com/Jeff-sjtu/res-loglikelihood-regression)
* [Online Knowledge Distillation for Efficient Pose Estimation](https://arxiv.org/abs/2108.02092)
* [DECA: Deep viewpoint-Equivariant human pose estimation using Capsule Autoencoders](https://arxiv.org/abs/2108.08557)<br>:open_mouth:oral:star:[code](https://github.com/mmlab-cv/DECA)
* [Estimating and Exploiting the Aleatoric Uncertainty in Surface Normal Estimation](https://arxiv.org/abs/2109.09881)<br>:open_mouth:oral:star:[code](https://github.com/baegwangbin/surface_normal_uncertainty)
* [Dynamical Pose Estimation](https://arxiv.org/abs/2103.06182)<br>:star:[code](https://github.com/hankyang94/DAMP):tv:[video](https://www.youtube.com/watch?v=S6L0h-d0IYM)
* 3D 人体姿态估计
  * [PyMAF: 3D Human Pose and Shape Regression with Pyramidal Mesh Alignment Feedback Loop](https://arxiv.org/abs/2103.16507)<br>:open_mouth:oral:star:[code](https://github.com/HongwenZhang/PyMAF):house:[project](https://hongwenzhang.github.io/pymaf/)
  * [HuMoR: 3D Human Motion Model for Robust Pose Estimation](https://arxiv.org/abs/2105.04668)<br>:open_mouth:oral:house:[project](https://geometry.stanford.edu/projects/humor/):tv:[video](https://youtu.be/5VWirxUHG0Y)
  * [Probabilistic Monocular 3D Human Pose Estimation with Normalizing Flows](https://arxiv.org/abs/2107.13788)<br>:star:[code](https://github.com/twehrbein/Probabilistic-Monocular-3D-Human-Pose-Estimation-with-Normalizing-Flows):tv:[video](https://www.youtube.com/watch?v=gaNX5CIl1L8)
  * [Learning Skeletal Graph Neural Networks for Hard 3D Pose Estimation](https://arxiv.org/abs/2108.07181)<br>:star:[code](https://github.com/ailingzengzzz/Skeletal-GNN)
  * [EventHPE: Event-based 3D Human Pose and Shape Estimation](https://arxiv.org/abs/2108.06819)<br>:star:[code](https://github.com/JimmyZou/EventHPE)
  * [imGHUM: Implicit Generative Models of 3D Human Shape and Articulated Pose](https://arxiv.org/abs/2108.10842)
  * [Graph-Based 3D Multi-Person Pose Estimation Using Multi-View Images](https://arxiv.org/abs/2109.05885)
  * [Unsupervised 3D Pose Estimation for Hierarchical Dance Video Recognition](https://arxiv.org/abs/2109.09166)
  * [Learning to Regress Bodies from Images using Differentiable Semantic Rendering](https://arxiv.org/abs/2110.03480)<br>:house:[project](https://dsr.is.tue.mpg.de/)
  * [Hierarchical Kinematic Probability Distributions for 3D Human Shape and Pose Estimation from Images in the Wild](https://arxiv.org/abs/2110.00990)<br>:star:[code](https://github.com/akashsengupta1997/HierarchicalProbabilistic3DHuman)
* 3D姿势迁移
  * [Unsupervised Geodesic-preserved Generative Adversarial Networks for Unconstrained 3D Pose Transfer](https://arxiv.org/abs/2108.07520)<br>:star:[code](https://github.com/mikecheninoulu/Unsupervised_IEPGAN)
* 手部姿势
  * 手势合成
    * [Speech Drives Templates: Co-Speech Gesture Synthesis with Learned Templates](https://arxiv.org/abs/2108.08020)(https://github.com/ShenhanQian/SpeechDrivesTemplates)
  * 手势识别
    * [Hand Image Understanding via Deep Multi-Task Learning](https://arxiv.org/abs/2107.11646)
  * 3D 手部姿态
    * [HandFoldingNet: A 3D Hand Pose Estimation Network Using Multiscale-Feature Guided Folding of a 2D Hand Skeleton](https://arxiv.org/abs/2108.05545)<br>:star:[code](https://github.com/cwc1260/HandFold)
    * [EventHands: Real-Time Neural 3D Hand Pose Estimation From an Event Stream](https://openaccess.thecvf.com/content/ICCV2021/papers/Rudnev_EventHands_Real-Time_Neural_3D_Hand_Pose_Estimation_From_an_Event_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/r00tman/EventHands):house:[project](https://4dqv.mpi-inf.mpg.de/EventHands/):tv:[video](https://youtu.be/jB1nkSYtblU)
  * 手部交互姿势估计
    * [End-to-End Detection and Pose Estimation of Two Interacting Hands](https://openaccess.thecvf.com/content/ICCV2021/papers/Kim_End-to-End_Detection_and_Pose_Estimation_of_Two_Interacting_Hands_ICCV_2021_paper.pdf)
* 三维网格合成
  * [Deep Hybrid Self-Prior for Full 3D Mesh Generation](https://arxiv.org/abs/2108.08017)<br>:house:[project](https://yqdch.github.io/DHSP3D/)
* 人体重建
  * [ARCH++: Animation-Ready Clothed Human Reconstruction Revisited](https://arxiv.org/abs/2108.07845)<br>:tv:[video](https://www.youtube.com/watch?v=kNtlheGLSR8)
  * 3D 人体重建
    * [Probabilistic Modeling for Human Mesh Recovery](https://arxiv.org/abs/2108.11944)<br>:star:[code](https://github.com/nkolot/ProHMR):house:[project](https://www.seas.upenn.edu/~nkolot/projects/prohmr/)
    * [Gravity-Aware Monocular 3D Human-Object Reconstruction](https://arxiv.org/abs/2108.08844)<br>:house:[project](http://4dqv.mpi-inf.mpg.de/GraviCap/)
    * [THUNDR: Transformer-Based 3D Human Reconstruction With Markers](https://arxiv.org/abs/2106.09336)
* 4D人体捕捉
  * [Learning Motion Priors for 4D Human Body Capture in 3D Scenes](https://arxiv.org/abs/2108.10399)<br>:star:[code](https://github.com/sanweiliti/LEMO):house:[project](https://sanweiliti.github.io/LEMO/LEMO.html):tv:[video](https://youtu.be/ly8UaeFqFhw) 
* 人体姿态估计与合成
  * [Physics-based Human Motion Estimation and Synthesis from Videos](https://arxiv.org/abs/2109.09913)
* 多人姿态估计 
  * [Shape-aware Multi-Person Pose Estimation from Multi-View Images](https://arxiv.org/abs/2110.02330)<br>:star:[code](https://github.com/zj-dong/Multi-Person-Pose-Estimation):house:[project](https://ait.ethz.ch/projects/2021/multi-human-pose/):tv:[video](https://www.youtube.com/watch?v=KE5Jpnyqmh4)<br>论文公开
  * [The Center of Attention: Center-Keypoint Grouping via Attention for Multi-Person Pose Estimation](https://arxiv.org/abs/2110.05132)<br>:star:[code](https://github.com/dvl-tum)
* 人/物体姿态关键点检测
  * [Keypoint Communities](https://arxiv.org/abs/2110.00988)<br>:star:[code](https://github.com/DuncanZauss/Keypoint_Communities)
* 人体运动捕捉
  * [SOMA: Solving Optical Marker-Based MoCap Automatically](https://arxiv.org/abs/2110.04431)<br>:star:[code](https://github.com/nghorbani/soma):house:[project](https://soma.is.tue.mpg.de/):tv:[video](https://youtu.be/BEFCqIefLA8)
  * [DeepMultiCap: Performance Capture of Multiple Characters Using Sparse Multiview Cameras](https://arxiv.org/abs/2105.00261)<br>:house:[project](http://liuyebin.com/dmc/dmc.html)
* 2D人体姿势估计
  * [An Empirical Study of the Collapsing Problem in Semi-Supervised 2D Human Pose Estimation](https://arxiv.org/abs/2011.12498)<br>:star:[code](https://github.com/xierc/Semi_Human_Pose)
* Human Action Video Alignment
  * [Normalized Human Pose Features for Human Action Video Alignment](https://openaccess.thecvf.com/content/ICCV2021/papers/Liu_Normalized_Human_Pose_Features_for_Human_Action_Video_Alignment_ICCV_2021_paper.pdf)

<a name="7"/>

## 7.Scene Graph Generation(场景图生成)
* [Spatial-Temporal Transformer for Dynamic Scene Graph Generation](https://arxiv.org/abs/2107.12309)<br>:star:[code](https://github.com/yrcong/STTran):tv:[video](https://www.youtube.com/watch?v=6D3ExjQpbjQ&feature=youtu.be)
* [Unconditional Scene Graph Generation](https://arxiv.org/abs/2108.05884)<br>:house:[project](https://scenegraphgen.github.io/)
* [Target Adaptive Context Aggregation for Video Scene Graph Generation](https://arxiv.org/abs/2108.08121)<br>:star:[code](https://github.com/MCG-NJU/TRACE)
* [Learning to Generate Scene Graph from Natural Language Supervision](https://arxiv.org/pdf/2109.02227)<br>:star:[code](https://github.com/YiwuZhong/SGG_from_NLS)
* [Segmentation-Grounded Scene Graph Generation](https://arxiv.org/abs/2104.14207)<br>:star:[code](https://github.com/ubc-vision/segmentation-sg)
* 场景合成
  * [Self-Supervised Real-to-Sim Scene Generation](https://arxiv.org/abs/2011.14488)<br>:house:[project](https://research.nvidia.com/publication/2021-08_Sim2SG)

<a name="6"/>

## 6.Point Cloud(点云)
* [AdaFit: Rethinking Learning-based Normal Estimation on Point Clouds](https://arxiv.org/abs/2108.05836)<br>:star:[code](https://github.com/Runsong123/AdaFit):house:[project](https://runsong123.github.io/AdaFit/)
* [Adaptive Graph Convolution for Point Cloud Analysis](https://arxiv.org/abs/2108.08035)<br>:star:[code](https://github.com/hrzhou2/AdaptConv-master)
* [Learning Inner-Group Relations on Point Clouds](https://arxiv.org/abs/2108.12468)
* [CPFN: Cascaded Primitive Fitting Networks for High-Resolution Point Clouds](https://arxiv.org/abs/2109.00113)<br>:star:[code](https://github.com/erictuanle/CPFN)
* [Cloud Transformers: A Universal Approach To Point Cloud Processing Tasks](https://arxiv.org/abs/2007.11679)<br>:star:[code](https://github.com/saic-vul/cloud_transformers):tv:[video](https://www.youtube.com/watch?v=lYTzLhy-ybw) 
* [PCAM: Product of Cross-Attention Matrices for Rigid Registration of Point Clouds](https://arxiv.org/abs/2110.01269)<br>:star:[code](https://github.com/valeoai/PCAM)
* [3DVG-Transformer: Relation Modeling for Visual Grounding on Point Clouds](https://openaccess.thecvf.com/content/ICCV2021/papers/Zhao_3DVG-Transformer_Relation_Modeling_for_Visual_Grounding_on_Point_Clouds_ICCV_2021_paper.pdf)
* [Differentiable Convolution Search for Point Cloud Processing](http://arxiv.org/abs/2108.12856)
* [Superpoint Network for Point Cloud Oversegmentation](https://openaccess.thecvf.com/content/ICCV2021/papers/Hui_Superpoint_Network_for_Point_Cloud_Oversegmentation_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/fpthink/SPNet)
* [PU-EVA: An Edge-Vector Based Approximation Solution for Flexible-Scale Point Cloud Upsampling](https://openaccess.thecvf.com/content/ICCV2021/papers/Luo_PU-EVA_An_Edge-Vector_Based_Approximation_Solution_for_Flexible-Scale_Point_Cloud_ICCV_2021_paper.pdf)
* 点云去噪
  * [Score-Based Point Cloud Denoising](https://arxiv.org/abs/2107.10981)<br>:star:[code](https://github.com/luost26/score-denoise)
* 点云配准
  * [HRegNet: A Hierarchical Network for Large-scale Outdoor LiDAR Point Cloud Registration](https://arxiv.org/abs/2107.11992)<br>:star:[code](https://github.com/ispc-lab/HRegNet):house:[project](https://ispc-group.github.io/hregnet)
  * [(Just) A Spoonful of Refinements Helps the Registration Error Go Down](https://arxiv.org/abs/2108.03257https://arxiv.org/abs/2108.03257)<br>:open_mouth:oral:star:[code](https://github.com/SergioRAgostinho/just-a-spoonful)
  * [A Robust Loss for Point Cloud Registration](https://arxiv.org/abs/2108.11682)
  * [Deep Hough Voting for Robust Global Registration](https://arxiv.org/abs/2109.04310)
  * [Sampling Network Guided Cross-Entropy Method for Unsupervised Point Cloud Registration](https://arxiv.org/abs/2109.06619)<br>:star:[code](https://github.com/Jiang-HB/CEMNet)
  * [Feature Interactive Representation for Point Cloud Registration](https://openaccess.thecvf.com/content/ICCV2021/papers/Wu_Feature_Interactive_Representation_for_Point_Cloud_Registration_ICCV_2021_paper.pdf)
  * [LSG-CPD: Coherent Point Drift With Local Surface Geometry for Point Cloud Registration](https://openaccess.thecvf.com/content/ICCV2021/papers/Liu_LSG-CPD_Coherent_Point_Drift_With_Local_Surface_Geometry_for_Point_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/ChirikjianLab/LSG-CPD):tv:[video](https://www.youtube.com/watch?v=1lxz9Uu-GXI)
* 3D点云
  * [Unsupervised Learning of Fine Structure Generation for 3D Point Clouds by 2D Projection Matching](https://arxiv.org/abs/2108.03746)<br>:star:[code](https://github.com/chenchao15/2D_projection_matching)
  * [Spatio-temporal Self-Supervised Representation Learning for 3D Point Clouds](https://arxiv.org/abs/2109.00179)<br>:star:[code](https://github.com/yichen928/STRL):house:[project](https://siyuanhuang.com/STRL/)
* 点云补全
  * [SnowflakeNet: Point Cloud Completion by Snowflake Point Deconvolution with Skip-Transformer](https://arxiv.org/abs/2108.04444)<br>:open_mouth:oral:star:[code](https://github.com/AllenXiangX/SnowflakeNet)
  * [ME-PCN: Point Completion Conditioned on Mask Emptiness](https://arxiv.org/abs/2108.08187)
  * [PoinTr: Diverse Point Cloud Completion with Geometry-Aware Transformers](https://arxiv.org/abs/2108.08839)<br>:open_mouth:oral:star:[code](https://github.com/yuxumin/PoinTr)
  * [Voxel-based Network for Shape Completion by Leveraging Edge Generation](https://arxiv.org/abs/2108.09936)<br>:star:[code](https://github.com/xiaogangw/VE-PCN)
  * [RFNet: Recurrent Forward Network for Dense Point Cloud Completion](https://arxiv.org/abs/2104.00820)
* 点云增强
  * [Point Cloud Augmentation with Weighted Local Transformations](https://arxiv.org/abs/2110.05379)<br>:star:[code](https://github.com/mlvlab/PointWOLF)
* 点云形状分析
  * [Walk in the Cloud: Learning Curves for Point Clouds Shape Analysis](https://arxiv.org/abs/2105.01288)<br>:star:[code](https://github.com/tiangexiang/CurveNet):house:[project](https://curvenet.github.io/)
* 点云分析
  * [A Closer Look at Rotation-Invariant Deep Point Cloud Analysis](https://openaccess.thecvf.com/content/ICCV2021/papers/Li_A_Closer_Look_at_Rotation-Invariant_Deep_Point_Cloud_Analysis_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/SILI1994/rotation-invariant-pointcloud-analysis)
* 3D点云分类
  * [A Backdoor Attack Against 3D Point Cloud Classifiers](http://arxiv.org/abs/2104.05808)<br>:star:[code](https://github.com/zhenxianglance/PCBA)


<a name="5"/>

## 5.Few-Shot/Zero-Shot Learning;Domain Generalization/Adaptation(小/零样本学习;域适应/泛化)
* 域适应
  * [Transporting Causal Mechanisms for Unsupervised Domain Adaptation](https://arxiv.org/abs/2107.11055)<br>:open_mouth:oral
  * [Generalized Source-free Domain Adaptation](https://arxiv.org/abs/2108.01614)<br>:star:[code](https://github.com/Albert0147/G-SFDA)
  * [Semantic Concentration for Domain Adaptation](https://arxiv.org/abs/2108.05720)<br>:star:[code](https://github.com/BIT-DA)
  * [PIT: Position-Invariant Transform for Cross-FoV Domain Adaptation](https://arxiv.org/abs/2108.07142)<br>:star:[code](https://github.com/sheepooo/PIT-Position-Invariant-Transform)
  * [Learning Cross-modal Contrastive Features for Video Domain Adaptation](https://arxiv.org/abs/2108.11974)
  * [Zero-Shot Day-Night Domain Adaptation With a Physics Prior](https://openaccess.thecvf.com/content/ICCV2021/papers/Lengyel_Zero-Shot_Day-Night_Domain_Adaptation_With_a_Physics_Prior_ICCV_2021_paper.pdf)<br>:open_mouth:oral:star:[code](https://github.com/Attila94/CIConv)
  * [Active Universal Domain Adaptation](https://openaccess.thecvf.com/content/ICCV2021/papers/Ma_Active_Universal_Domain_Adaptation_ICCV_2021_paper.pdf)
  * [Re-energizing Domain Discriminator with Sample Relabeling for Adversarial Domain Adaptation](https://arxiv.org/abs/2103.11661)
  * 无监督域适应
    * [Adversarial Unsupervised Domain Adaptation with Conditional and Label Shift: Infer, Align and Iterate](https://arxiv.org/abs/2107.13469)
    * [Recursively Conditional Gaussian for Ordinal Unsupervised Domain Adaptation](https://arxiv.org/abs/2107.13467)<br>:open_mouth:oral
    * [Tune it the Right Way: Unsupervised Validation of Domain Adaptation via Soft Neighborhood Density](https://arxiv.org/abs/2108.10860)
    * [Adversarial Robustness for Unsupervised Domain Adaptation](https://arxiv.org/abs/2109.00946)<br>:house:[project](http://deepawais.com/robust_uda/)
  * 零样本域适应
    * [Zero-Shot Domain Adaptation with a Physics Prior](https://arxiv.org/abs/2108.05137)<br>:open_mouth:oral:star:[code](https://github.com/Attila94/CIConv)
* 域泛化
  * [Domain Generalization via Gradient Surgery](https://arxiv.org/abs/2108.01621)<br>:star:[code](https://github.com/lucasmansilla/DGvGS)
  * [Learning to Diversify for Single Domain Generalization](https://arxiv.org/abs/2108.11726)<br>:star:[code](https://github.com/BUserName/Learning_to_diversify)
  * [Shape-Biased Domain Generalization via Shock Graph Embeddings](https://arxiv.org/abs/2109.05671)
* 小样本
  * [Boosting the Generalization Capability in Cross-Domain Few-shot Learning via Noise-enhanced Supervised Autoencoder](https://arxiv.org/abs/2108.05028)
  * [Generalized and Incremental Few-Shot Learning by Explicit Learning and Calibration without Forgetting](https://arxiv.org/abs/2108.08165)<br>:star:[code](https://github.com/annusha/LCwoF)
  * [Meta Navigator: Search for a Good Adaptation Policy for Few-shot Learning](https://arxiv.org/abs/2109.05749)
  * [Meta-Learning with Task-Adaptive Loss Function for Few-Shot Learning](https://arxiv.org/abs/2110.03909)<br>:open_mouth:oral:star:[code](https://github.com/baiksung)
  * [Z-Score Normalization, Hubness, and Few-Shot Learning](https://openaccess.thecvf.com/content/ICCV2021/papers/Fei_Z-Score_Normalization_Hubness_and_Few-Shot_Learning_ICCV_2021_paper.pdf)
* Zero-Shot Learning(零样本学习)
  * [Discriminative Region-based Multi-Label Zero-Shot Learning](https://arxiv.org/abs/2108.09301)<br>:star:[code](https://github.com/akshitac8/BiAM)
  * [Field-Guide-Inspired Zero-Shot Learning](https://arxiv.org/abs/2108.10967)
  * Generalized Zero-Shot Learning(广义零样本学习)
    * [FREE: Feature Refinement for Generalized Zero-Shot Learning](https://arxiv.org/abs/2107.13807)<br>:star:[code](https://github.com/shiming-chen/FREE)
 
<a name="4"/>

## 4.Neural rendering(神经渲染)
* [In-Place Scene Labelling and Understanding with Implicit Scene Representation](https://arxiv.org/abs/2103.15875)<br>:open_mouth:oral:house:[project](https://shuaifengzhi.com/Semantic-NeRF/):tv:[video](https://www.youtube.com/watch?v=FpShWO7LVbM)
* [Differentiable Surface Rendering via Non-Differentiable Sampling](https://arxiv.org/abs/2108.04886)
* [Self-Calibrating Neural Radiance Fields](https://arxiv.org/abs/2108.13826)<br>:star:[code](https://github.com/POSTECH-CVLab/SCNeRF)
* [NerfingMVS: Guided Optimization of Neural Radiance Fields for Indoor Multi-view Stereo](https://arxiv.org/abs/2109.01129)<br>:open_mouth:oral:star:[code](https://github.com/weiyithu/NerfingMVS):house:[project](https://weiyithu.github.io/NerfingMVS/)
* [Learning Object-Compositional Neural Radiance Field for Editable Scene Rendering](https://arxiv.org/abs/2109.01847)<br>:star:[code](https://github.com/zju3dv/object_nerf):house:[project](https://zju3dv.github.io/object_nerf/)
* [CodeNeRF: Disentangled Neural Radiance Fields for Object Categories](https://arxiv.org/abs/2109.01750)<br>:star:[code](https://github.com/wayne1123/code-nerf)
* [MVSNeRF: Fast Generalizable Radiance Field Reconstruction from Multi-View Stereo](https://arxiv.org/abs/2103.15595)<br>:star:[code](https://github.com/apchenstu/mvsnerf):house:[project](https://apchenstu.github.io/mvsnerf/):tv:[video](https://youtu.be/3M3edNiaGsA)
* [PlenOctrees for Real-Time Rendering of Neural Radiance Fields](https://arxiv.org/abs/2103.14024)<br>:open_mouth:oral:star:[Conversion Code](https://github.com/sxyu/plenoctree):star:[Viewer Code](https://github.com/sxyu/volrend):house:[project](https://alexyu.net/plenoctrees/):tv:[video](https://youtu.be/obrmH1T5mfI)
* [Neural Radiance Flow for 4D View Synthesis and Video Processing](https://arxiv.org/abs/2012.09790)<br>:star:[code](https://github.com/yilundu/nerflow):house:[project](https://yilundu.github.io/nerflow/)
* 3D photography(3D 相片)
  * [SLIDE: Single Image 3D Photography with Soft Layering and Depth-aware Inpainting](https://arxiv.org/abs/2109.01068)<br>:open_mouth:oral:house:[project](https://varunjampani.github.io/slide/):tv:[video](https://www.youtube.com/watch?v=RQio7q-ueY8)

<a name="3"/>

## 3.Image Clustering(图像聚类)
* [Clustering by Maximizing Mutual Information Across Views](https://arxiv.org/abs/2107.11635)
* [Learning Hierarchical Graph Neural Networks for Image Clustering](https://arxiv.org/abs/2107.01319)<br>:star:[code](https://github.com/dmlc/dgl/tree/master/examples/pytorch/hilander)
* [One-Pass Multi-View Clustering for Large-Scale Data](https://openaccess.thecvf.com/content/ICCV2021/papers/Liu_One-Pass_Multi-View_Clustering_for_Large-Scale_Data_ICCV_2021_paper.pdf)

<a name="2"/>

## 2.Sign Language(手语识别)
* [Mixed SIGNals: Sign Language Production via a Mixture of Motion Primitives](https://arxiv.org/abs/2107.11317)
* [SignBERT: Pre-Training of Hand-Model-Aware Representation for Sign Language Recognition](https://arxiv.org/abs/2110.05382)
* [Self-Mutual Distillation Learning for Continuous Sign Language Recognition](https://openaccess.thecvf.com/content/ICCV2021/papers/Hao_Self-Mutual_Distillation_Learning_for_Continuous_Sign_Language_Recognition_ICCV_2021_paper.pdf)

<a name="1"/>

## 1.Other(其它)
* [Bias Loss for Mobile Neural Networks](https://arxiv.org/abs/2107.11170)
* [Improve Unsupervised Pretraining for Few-label Transfer](https://arxiv.org/abs/2107.12369)
* [Temporal-wise Attention Spiking Neural Networks for Event Streams Classification](https://arxiv.org/abs/2107.11711)
* [Accelerating Atmospheric Turbulence Simulation via Learned Phase-to-Space Transform](https://arxiv.org/abs/2107.11627)
* [Energy-Based Open-World Uncertainty Modeling for Confidence Calibration](https://arxiv.org/abs/2107.12628)
* [Robustness via Cross-Domain Ensembles](https://arxiv.org/abs/2103.10919)<br>:open_mouth:oral:star:[code](https://github.com/EPFL-VILAB/XDEnsembles):house:[project](https://crossdomain-ensembles.epfl.ch/):tv:[video](https://youtu.be/h0FI5Sp7y7g)
* [Warp Consistency for Unsupervised Learning of Dense Correspondences](https://arxiv.org/abs/2104.03308)<br>:open_mouth:oral:star:[code](https://github.com/PruneTruong/DenseMatching)
* [Few-Shot and Continual Learning with Attentive Independent Mechanisms](https://arxiv.org/abs/2107.14053)<br>:star:[code](https://github.com/huang50213/AIM-Fewshot-Continual)
* [Out-of-Core Surface Reconstruction via Global TGV Minimization](https://arxiv.org/abs/2107.14790)
* [ELLIPSDF: Joint Object Pose and Shape Optimization with a Bi-level Ellipsoid and Signed Distance Function Description](https://arxiv.org/abs/2108.00355)
* [Multi-scale Matching Networks for Semantic Correspondence](https://arxiv.org/abs/2108.00211)<br>:star:[code](https://github.com/wintersun661/MMNet)
* [Learning with Noisy Labels via Sparse Regularization](https://arxiv.org/abs/2108.00192)<br>:star:[code](https://github.com/hitcszx/lnl_sr)
* [CanvasVAE: Learning to Generate Vector Graphic Documents](https://arxiv.org/abs/2108.01249)
* [Toward Spatially Unbiased Generative Models](https://arxiv.org/abs/2108.01285)<br>:star:[code](https://github.com/jychoi118/toward_spatial_unbiased)
* [Learning Compatible Embeddings](https://arxiv.org/abs/2108.01958)<br>:star:[code](https://github.com/IrvingMeng/LCE)
* [Instance Similarity Learning for Unsupervised Feature Representation](https://arxiv.org/abs/2108.02721)<br>:star:[code](https://github.com/ZiweiWangTHU/ISL)
* [Generalizable Mixed-Precision Quantization via Attribution Rank Preservation](https://arxiv.org/abs/2108.02720)<br>:star:[code](https://github.com/ZiweiWangTHU/GMPQ)
* [Unifying Nonlocal Blocks for Neural Networks](https://arxiv.org/abs/2108.02451)<br>:star:[code](https://github.com/zh460045050/SNL_ICCV2021)
* [Impact of Aliasing on Generalization in Deep Convolutional Networks](https://arxiv.org/abs/2108.03489)
* [NASOA: Towards Faster Task-oriented Online Fine-tuning with a Zoo of Models](https://arxiv.org/abs/2108.03434)<br>:star:[code](https://github.com/NAS-OA/NASOA)
* [ProAI: An Efficient Embedded AI Hardware for Automotive Applications - a Benchmark Study](https://arxiv.org/abs/2108.05170)
* [m-RevNet: Deep Reversible Neural Networks with Momentum](https://arxiv.org/abs/2108.05862) [涉嫌学术不端，已申请撤稿](https://www.zhihu.com/question/480075870/answer/2064860328)
* [Continual Neural Mapping: Learning An Implicit Scene Representation from Sequential Observations](https://arxiv.org/abs/2108.05851)
* [MT-ORL: Multi-Task Occlusion Relationship Learning](https://arxiv.org/abs/2108.05722)<br>:star:[code](https://github.com/fengpanhe/MT-ORL) 
* [Finding Representative Interpretations on Convolutional Neural Networks](https://arxiv.org/abs/2108.06384)
* [Collaborative Unsupervised Visual Representation Learning from Decentralized Data](https://arxiv.org/abs/2108.06492)
* 异常检测
  * [Weakly Supervised Temporal Anomaly Segmentation with Dynamic Time Warping](https://arxiv.org/abs/2108.06816)
* [Orthogonal Jacobian Regularization for Unsupervised Disentanglement in Image Generation](https://arxiv.org/abs/2108.07668)<br>:star:[code](https://github.com/csyxwei/OroJaR)
* [PR-RRN: Pairwise-Regularized Residual-Recursive Networks for Non-rigid Structure-from-Motion](https://arxiv.org/abs/2108.07506)
* [Instance Segmentation in 3D Scenes using Semantic Superpoint Tree Networks](https://arxiv.org/abs/2108.07478)<br>:star:[code](https://github.com/Gorilla-Lab-SCUT)
* [Learning RAW-to-sRGB Mappings with Inaccurately Aligned Supervision](https://arxiv.org/abs/2108.08119)<br>:star:[code](https://github.com/cszhilu1998/RAW-to-sRGB)
* [Structured Outdoor Architecture Reconstruction by Exploration and Classification](https://arxiv.org/abs/2108.07990)
* [Global Pooling, More than Meets the Eye: Position Information is Encoded Channel-Wise in CNNs](https://arxiv.org/abs/2108.07884)<br>:star:[code](https://github.com/islamamirul/PermuteNet)
* [A New Journey from SDRTV to HDRTV](https://arxiv.org/abs/2108.07978)<br>:star:[code](https://github.com/chxy95/HDRTVNet)
* [A Simple Framework for 3D Lensless Imaging with Programmable Masks](https://arxiv.org/abs/2108.07966)<br>:star:[code](https://github.com/CSIPlab/Programmable3Dcam)
* [Causal Attention for Unbiased Visual Recognition](https://arxiv.org/abs/2108.08782)<br>:star:[code](https://github.com/Wangt-CN/CaaM)
* [Learning to Match Features with Seeded Graph Matching Network](https://arxiv.org/abs/2108.08771)<br>:star:[code](https://github.com/vdvchen/SGMNet)
* [Amplitude-Phase Recombination: Rethinking Robustness of Convolutional Neural Networks in Frequency Domain](https://arxiv.org/abs/2108.08487)
* [PatchMatch-RL: Deep MVS with Pixelwise Depth, Normal, and Visibility](https://arxiv.org/abs/2108.08943)<br>:open_mouth:oral
* [Towards Understanding the Generative Capability of Adversarially Robust Classifiers](https://arxiv.org/abs/2108.09093)<br>:open_mouth:oral
* [Ranking Models in Unlabeled New Environments](https://arxiv.org/abs/2108.10310)<br>:star:[code](https://github.com/sxzrt/Proxy-Set)
* [Learning of Visual Relations: The Devil is in the Tails](https://arxiv.org/abs/2108.09668)
* [BlockCopy: High-Resolution Video Processing with Block-Sparse Feature Propagation and Online Policies](https://arxiv.org/abs/2108.09376)
* [Patch2CAD: Patchwise Embedding Learning for In-the-Wild Shape Retrieval from a Single Image](https://arxiv.org/abs/2108.09368)
* 去偏差
  * [BiaSwap: Removing dataset bias with bias-tailored swapping augmentation](https://arxiv.org/abs/2108.10008)
* [Full-Velocity Radar Returns by Radar-Camera Fusion](https://arxiv.org/abs/2108.10637)
* [CSG-Stump: A Learning Friendly CSG-Like Representation for Interpretable Shape Parsing](https://arxiv.org/abs/2108.11305)<br>:star:[code](https://github.com/kimren227/CSGStumpNet):house:[project](https://kimren227.github.io/projects/CSGStump/)
* [NGC: A Unified Framework for Learning with Open-World Noisy Data](https://arxiv.org/abs/2108.11035)
* [LocTex: Learning Data-Efficient Visual Representations from Localized Textual Supervision](https://arxiv.org/abs/2108.11950)<br>:house:[project](https://loctex.mit.edu/)
* [Unsupervised Dense Deformation Embedding Network for Template-Free Shape Correspondence](https://arxiv.org/abs/2108.11609)
* [Lifelong Infinite Mixture Model Based on Knowledge-Driven Dirichlet Process](https://arxiv.org/abs/2108.12278)<br>:star:[code](https://github.com/dtuzi123/Lifelong-infinite-mixture-model)
* [Digging into Uncertainty in Self-supervised Multi-view Stereo](https://arxiv.org/abs/2108.12966)
* [Learning to Discover Reflection Symmetry via Polar Matching Convolution](https://arxiv.org/abs/2108.12952)
* [A Dual Adversarial Calibration Framework for Automatic Fetal Brain Biometry](https://arxiv.org/abs/2108.12719)
* [The Functional Correspondence Problem](https://arxiv.org/abs/2109.01097)
* [The Animation Transformer: Visual Correspondence via Segment Matching](https://arxiv.org/abs/2109.02614)
* [Parsing Table Structures in the Wild](https://arxiv.org/abs/2109.02199)
* [Square Root Marginalization for Sliding-Window Bundle Adjustment](https://arxiv.org/abs/2109.02182)
* [Hierarchical Object-to-Zone Graph for Object Navigation](https://arxiv.org/abs/2109.02066)
* [Dual Transfer Learning for Event-based End-task Prediction via Pluggable Event to Image Translation](https://arxiv.org/abs/2109.01801)
* [Robustness and Generalization via Generative Adversarial Training](https://arxiv.org/abs/2109.02765)
* [Learning Fast Sample Re-weighting Without Reward Data](https://arxiv.org/abs/2109.03216)<br>:star:[code](https://github.com/google-research/google-research/tree/master/ieg)
* [ReconfigISP: Reconfigurable Camera Image Processing Pipeline](https://arxiv.org/abs/2109.04760)<br>:house:[project](https://www.mmlab-ntu.com/project/reconfigisp/)
* [Learning Indoor Inverse Rendering with 3D Spatially-Varying Lighting](https://arxiv.org/abs/2109.06061)<br>:open_mouth:oral
* [Low-Shot Validation: Active Importance Sampling for Estimating Classifier Performance on Rare Categories](https://arxiv.org/abs/2109.05720)
* [DisUnknown: Distilling Unknown Factors for Disentanglement Learning](https://arxiv.org/abs/2109.08090)<br>:star:[code](https://github.com/stormraiser/disunknown):house:[project](https://stormraiser.github.io/disunknown/)
* [S3VAADA: Submodular Subset Selection for Virtual Adversarial Active Domain Adaptation](https://arxiv.org/abs/2109.08901)<br>:house:[project](https://sites.google.com/iisc.ac.in/s3vaada-iccv2021)
* [ALADIN: All Layer Adaptive Instance Normalization for Fine-grained Style Similarity](https://arxiv.org/abs/2103.09776)<br>:tv:[video](https://www.youtube.com/watch?v=TxE1_juIHqY)
* [Photon-Starved Scene Inference using Single Photon Cameras](https://arxiv.org/abs/2107.11001)<br>:tv:[video](https://www.youtube.com/watch?v=r1YvHnGbi6k)
* [OSCAR-Net: Object-centric Scene Graph Attention for Image Attribution](https://arxiv.org/abs/2108.03541)<br>:star:[code](https://github.com/exnx/oscar):house:[project](https://exnx.github.io/oscar/)
* [Learning to Estimate Hidden Motions with Global Motion Aggregation](https://arxiv.org/abs/2104.02409)<br>:star:[code](https://github.com/zacjiang/GMA):tv:[video](https://www.youtube.com/watch?v=cBNSQ8ZFKSE)
* [Modelling Neighbor Relation in Joint Space-Time Graph for Video Correspondence Learning](https://arxiv.org/abs/2109.13499)
* [Meta Learning on a Sequence of Imbalanced Domains with Difficulty Awareness](https://arxiv.org/abs/2109.14120)<br>:star:[code](https://github.com/joey-wang123/Imbalancemeta) 
* [Procedure Planning in Instructional Videosvia Contextual Modeling and Model-based Policy Learning](https://arxiv.org/abs/2110.01770)<br>:open_mouth:oral
* [Extensions of Karger's Algorithm: Why They Fail in Theory and How They Are Useful in Practice](https://arxiv.org/abs/2110.02750)<br>:open_mouth:oral:star:[code](https://github.com/ejnnr)
* [Neural Strokes: Stylized Line Drawing of 3D Shapes](https://arxiv.org/abs/2110.03900)<br>:star:[code](https://github.com/DifanLiu/NeuralStrokes)
* [Learning Realistic Human Reposing using Cyclic Self-Supervision with 3D Shape, Pose, and Appearance Consistency](https://arxiv.org/abs/2110.05458)
* [Omnidata: A Scalable Pipeline for Making Multi-Task Mid-Level Vision Datasets from 3D Scans](https://arxiv.org/abs/2110.04994)<br>:house:[project](https://omnidata.vision/)
* [Cherry-Picking Gradients: Learning Low-Rank Embeddings of Visual Data via Differentiable Cross-Approximation](https://openaccess.thecvf.com/content/ICCV2021/papers/Usvyatsov_Cherry-Picking_Gradients_Learning_Low-Rank_Embeddings_of_Visual_Data_via_Differentiable_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/aelphy/c-pic)
* [Exploiting Explanations for Model Inversion Attacks](https://arxiv.org/abs/2104.12669)
* [Learning Bias-Invariant Representation by Cross-Sample Mutual Information Minimization](https://arxiv.org/abs/2108.05449)
* [RDI-Net: Relational Dynamic Inference Networks](https://openaccess.thecvf.com/content/ICCV2021/papers/Wang_RDI-Net_Relational_Dynamic_Inference_Networks_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/huanyuhello/RDI-Net)
* [ARAPReg: An As-Rigid-As Possible Regularization Loss for Learning Deformable Shape Generators](https://arxiv.org/abs/2108.09432)<br>:star:[code](https://github.com/GitBoSun/ARAPReg)
* [T-Net: Effective Permutation-Equivariant Network for Two-View Correspondence Learning](https://openaccess.thecvf.com/content/ICCV2021/papers/Zhong_T-Net_Effective_Permutation-Equivariant_Network_for_Two-View_Correspondence_Learning_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/x-gb/T-Net)
* [Learning To Stylize Novel Views](http://arxiv.org/abs/2105.13509)<br>:star:[code](https://github.com/hhsinping/stylescene):house:[project](https://hhsinping.github.io/3d_scene_stylization/)
* [A Lazy Approach to Long-Horizon Gradient-Based Meta-Learning](https://openaccess.thecvf.com/content/ICCV2021/papers/Zheng_Exploring_Temporal_Coherence_for_More_General_Video_Face_Forgery_Detection_ICCV_2021_paper.pdf)
* [Viewing Graph Solvability via Cycle Consistency](https://openaccess.thecvf.com/content/ICCV2021/papers/Arrigoni_Viewing_Graph_Solvability_via_Cycle_Consistency_ICCV_2021_paper.pdf)<br>:open_mouth:oral:star:[code](https://github.com/federica-arrigoni/solvability)<br>:trophy:Best paper honorable mention
* [SACoD: Sensor Algorithm Co-Design Towards Efficient CNN-Powered Intelligent PhlatCam](https://openaccess.thecvf.com/content/ICCV2021/papers/Fu_SACoD_Sensor_Algorithm_Co-Design_Towards_Efficient_CNN-Powered_Intelligent_PhlatCam_ICCV_2021_paper.pdf)<br>:star:[code](https://github.com/RICE-EIC/SACoD)
* [Rethinking 360° Image Visual Attention Modelling with Unsupervised Learning](https://openaccess.thecvf.com/content/ICCV2021/papers/Djilali_Rethinking_360deg_Image_Visual_Attention_Modelling_With_Unsupervised_Learning._ICCV_2021_paper.pdf)
* [Motion Basis Learning for Unsupervised Deep Homography Estimation with Subspace Projection](https://arxiv.org/abs/2103.15346)<br>:star:[code](https://github.com/megvii-research/BasesHomo)
* [Batch Normalization Increases Adversarial Vulnerability and Decreases Adversarial Transferability: A Non-Robust Feature Perspective](https://openaccess.thecvf.com/content/ICCV2021/papers/Benz_Batch_Normalization_Increases_Adversarial_Vulnerability_and_Decreases_Adversarial_Transferability_A_ICCV_2021_paper.pdf)
* [DeepCAD: A Deep Generative Network for Computer-Aided Design Models](https://arxiv.org/abs/2105.09492)<br>:house:[project](http://www.cs.columbia.edu/cg/deepcad/)
* [Better Aggregation in Test-Time Augmentation](https://openaccess.thecvf.com/content/ICCV2021/papers/Shanmugam_Better_Aggregation_in_Test-Time_Augmentation_ICCV_2021_paper.pdf)
* [Self-Born Wiring for Neural Trees](https://openaccess.thecvf.com/content/ICCV2021/papers/Chen_Self-Born_Wiring_for_Neural_Trees_ICCV_2021_paper.pdf)
* [Detector-Free Weakly Supervised Grounding by Separation](https://arxiv.org/abs/2104.09829)
* [Motion-Aware Dynamic Architecture for Efficient Frame Interpolation](https://openaccess.thecvf.com/content/ICCV2021/papers/Choi_Motion-Aware_Dynamic_Architecture_for_Efficient_Frame_Interpolation_ICCV_2021_paper.pdf)
* [Relating Adversarially Robust Generalization to Flat Minima](https://arxiv.org/abs/2104.04448)
* [Bit-Mixer: Mixed-Precision Networks With Runtime Bit-Width Selection](https://openaccess.thecvf.com/content/ICCV2021/papers/Bulat_Bit-Mixer_Mixed-Precision_Networks_With_Runtime_Bit-Width_Selection_ICCV_2021_paper.pdf)
* [AINet: Association Implantation for Superpixel Segmentation](https://arxiv.org/abs/2101.10696)<br>:star:[code](https://github.com/wangyxxjtu/AINet-ICCV2021)
* [Orthogonal Projection Loss](https://arxiv.org/abs/2103.14021)<br>:star:[code](https://github.com/kahnchana/opl)
* [Knowledge-Enriched Distributional Model Inversion Attacks](https://arxiv.org/abs/2010.04092)<br>:star:[code](https://github.com/SCccc21/Knowledge-Enriched-DMI)
* [Architecture Disentanglement for Deep Neural Networks](https://arxiv.org/abs/2003.13268)<br>:star:[code](https://github.com/hujiecpp/NAD)

